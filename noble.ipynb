{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import functions.core.core_functions as core_functions\n",
    "import functions.pyarrow_functions as pyarrow_functions\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe.utils import assert_eq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import json\n",
    "import gc\n",
    "import cudf\n",
    "import os\n",
    "import yaml\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "from google.cloud import bigquery, storage, bigquery_storage\n",
    "import ace_tools_open as tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "importlib.reload(core_functions)\n",
    "importlib.reload(pyarrow_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "resp = core_functions.initialize_clients(service_account_secret_name='SA_N90_CORE_APPS')\n",
    "\n",
    "config = resp.get('config')\n",
    "n90_bigquery_client = resp.get('clients').get('bigquery_client')\n",
    "storage_client = resp.get('clients').get('storage_client')\n",
    "\n",
    "unknown_dma_overrides = config.get('national_dma_overrides_to_us_national')\n",
    "national_media_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_media_only = True\n",
    "\n",
    "detections_sql = f\"\"\"\n",
    "    SELECT * FROM `next90-core-applications.n90_data_lake.p_avs_detections`\n",
    "    WHERE sfdc_advertiser_id = 'a1mUV000000FiYbYAK'\n",
    "    AND detection_timestamp >= '2024-09-01' and detection_timestamp < '2024-12-01'\n",
    "    AND origin = 'L'\n",
    "    AND name not in ('DIRECTV', 'DISH')\n",
    "    \n",
    "    \"\"\"\n",
    "if national_media_only:\n",
    "    detections_sql += f\"\"\"\n",
    "AND geo_neustar_id > 800000\n",
    "ORDER BY detection_timestamp ASC\n",
    "\"\"\"\n",
    "else:\n",
    "    detections_sql += f\"\"\"\n",
    "    ORDER BY detection_timestamp ASC\n",
    "    \"\"\"\n",
    "    \n",
    "detections_df = bigquery.Client().query(detections_sql).result().to_dataframe()\n",
    "\n",
    "activity_session_sql = f\"\"\"\n",
    "    SELECT activity_session_id, uuid, created_time, neustar_dma_id, neustar_country, neustar_state, url, domain, zip_code, lat, lon, calculated_source, is_bot, device_type, device_model, hour, city, session_id, numPvs, gclid, brand_id, year, month, day, sfdc_account_id, sfdc_account_name, sfdc_advertiser_id, activity_session_timestamp, process_month_group, bcw_monday_match, week_within_month, activity_date_time, click_view_gclid, bcw_id, bcw_index, bcm_index, geo_location, geo_type, geo_neustar_id, segments_date, segments_day_of_week, segments_media, segments_month_label, segments_quarter_label, segments_week_label, segments_month, segments_quarter, segments_week, segments_year, segments_broadcast_year, segments_broadcast_month_index, segments_broadcast_week_index, session_timestamp, activity_session_last_updated, activity_session_last_audit_id\n",
    "    FROM `next90-core-applications.n90_data_lake.p_activity_sessions`\n",
    "    WHERE sfdc_advertiser_id = 'a1mUV000000FiYbYAK'\n",
    "    AND activity_date_time >= '2024-09-01' and activity_date_time < '2024-12-01'\n",
    "    ORDER BY activity_date_time ASC\n",
    "    \"\"\"\n",
    "    \n",
    "activity_session_df = bigquery.Client().query(activity_session_sql).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df['length_in_seconds'] = detections_df['length_in_seconds'].astype('Int64').fillna(30)\n",
    "detections_df.loc[detections_df['length_in_seconds'] == 0, 'length_in_seconds'] = 30\n",
    "detections_df['attribution_window'] = (detections_df['length_in_seconds'] + 300).astype('Int64')\n",
    "detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp'])\n",
    "detections_df['attribution_end_time'] = detections_df['detection_timestamp'] + pd.to_timedelta(detections_df['attribution_window'], unit='s')\n",
    "try:\n",
    "    activity_session_df['session_timestamp'] = pd.to_datetime(activity_session_df['activity_date_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    activity_session_df['session_timestamp'] = pd.to_datetime(activity_session_df['activity_date_time']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['attribution_end_time'] = pd.to_datetime(detections_df['attribution_end_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['attribution_end_time'] = pd.to_datetime(detections_df['attribution_end_time']).dt.tz_localize('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activity_session_df = activity_session_df[0:10000].copy()\n",
    "test_activity_session_df = core_functions.extract_url_data(test_activity_session_df, 'url')\n",
    "test_activity_session_df['url_gclid'].dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activity_session_df[['tracking_id', 'tracking_type']] = test_activity_session_df.apply(core_functions.set_tracking_info, axis=1)\n",
    "test_activity_session_df['activity_source'] = test_activity_session_df.apply(lambda row: core_functions.calculate_source(row['referrer'], row['tracking_type']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_activity_session_df['referrer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df['geo_neustar_id'].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.loc[detections_df['geo_neustar_id'].isin([602,504]),['affiliate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.sort_values(by=['detection_timestamp'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlaps\n",
    "overlap_flags = []\n",
    "for i in range(len(detections_df)):\n",
    "    current_end = detections_df.loc[i, 'attribution_end_time']\n",
    "    overlaps = ((detections_df['detection_timestamp'] < current_end) & (detections_df['detection_timestamp'] >= detections_df.loc[i, 'detection_timestamp'])).sum() > 1\n",
    "    overlap_flags.append(overlaps)\n",
    "\n",
    "detections_df['overlaps'] = overlap_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add overlap details\n",
    "overlap_details = []\n",
    "for i in range(len(detections_df)):\n",
    "    parent_id = detections_df.loc[i, 'occurrence_id']\n",
    "    parent_start = detections_df.loc[i, 'detection_timestamp']\n",
    "    parent_end = detections_df.loc[i, 'attribution_end_time']\n",
    "    \n",
    "    # Find overlapping airings\n",
    "    overlaps = detections_df[\n",
    "        (detections_df['detection_timestamp'] < parent_end) & \n",
    "        (detections_df['detection_timestamp'] >= parent_start) & \n",
    "        (detections_df['occurrence_id'] != parent_id)\n",
    "    ]\n",
    "    \n",
    "    # If overlaps exist, calculate the overlap gap and store details\n",
    "    if len(overlaps) > 0:\n",
    "        overlap_data = []\n",
    "        for _, row in overlaps.iterrows():\n",
    "            overlap_start = row['detection_timestamp']\n",
    "            gap = (overlap_start - parent_start).total_seconds() / 60  # Gap in minutes\n",
    "            overlap_data.append({\n",
    "                'overlap_id': row['occurrence_id'],\n",
    "                'overlap_gap': gap\n",
    "            })\n",
    "        overlap_details.append(overlap_data)\n",
    "    else:\n",
    "        overlap_details.append([])\n",
    "\n",
    "# Add overlap details to the DataFrame\n",
    "detections_df['overlap_details'] = overlap_details\n",
    "\n",
    "# Display the DataFrame with overlap details\n",
    "tools.display_dataframe_to_user(name=\"Detections with Overlap Details\", dataframe=detections_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_session_na_df = activity_session_df[activity_session_df['neustar_country'].isin(['us', 'ca'])].copy().reset_index(drop=True)\n",
    "# activity_session_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_session_na_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure timestamps are consistent in their time zones\n",
    "try:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_localize('UTC')\n",
    "\n",
    "# Match sessions with airings\n",
    "time_window = pd.Timedelta(minutes=10)  # 10 minutes before and after\n",
    "session_curves = []\n",
    "\n",
    "# Convert detections_df to a dictionary for faster access\n",
    "detections_dict = detections_df.set_index('occurrence_id').to_dict(orient='index')\n",
    "for i, airing in detections_df.iterrows():\n",
    "    airing_id = airing['occurrence_id']\n",
    "    airing_time = airing['detection_timestamp']\n",
    "    \n",
    "    # Filter sessions within 10 minutes of airing\n",
    "    relevant_sessions = activity_session_na_df[\n",
    "        (activity_session_na_df['session_timestamp'] >= (airing_time - time_window)) &\n",
    "        (activity_session_na_df['session_timestamp'] <= (airing_time + time_window))\n",
    "    ]\n",
    "    relevant_sessions['time_diff'] = (relevant_sessions['session_timestamp'] - airing_time).dt.total_seconds()  # Time difference in seconds\n",
    "    relevant_sessions['airing_id'] = airing_id\n",
    "    session_curves.append(relevant_sessions)\n",
    "\n",
    "# Combine all session curves\n",
    "all_sessions_df = pd.concat(session_curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot bell curve for each source\n",
    "sources = ['google-ppc', 'direct', 'other', 'bing', 'google-organic', 'yahoo']\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for source in sources:\n",
    "    source_data = all_sessions_df[all_sessions_df['calculated_source'] == source]\n",
    "    plt.hist(\n",
    "        source_data['time_diff'], \n",
    "        bins=range(-600, 601, 10),  # Binning every 10 seconds\n",
    "        alpha=0.5, \n",
    "        label=source, \n",
    "        density=True\n",
    "    )\n",
    "plt.ylim(0, 0.00225)\n",
    "plt.title('Web Session Response Curves (10 Min Before and After Airing)')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Statistics for each source\n",
    "statistics = all_sessions_df.loc[all_sessions_df['calculated_source'].isin(sources)].groupby('calculated_source')['time_diff'].agg(['mean', 'std', 'count'])\n",
    "print(statistics)\n",
    "\n",
    "sources2 = all_sessions_df['calculated_source'].unique()\n",
    "\n",
    "for source in sources2:\n",
    "    if source in sources:\n",
    "        continue\n",
    "    source_data = all_sessions_df[all_sessions_df['calculated_source'] == source]\n",
    "    plt.hist(\n",
    "        source_data['time_diff'], \n",
    "        bins=range(-600, 601, 10),  # Binning every 10 seconds\n",
    "        alpha=0.5, \n",
    "        density=True\n",
    "    )\n",
    "plt.ylim(0, 0.0225)\n",
    "plt.title('Web Session Response Curves (10 Min Before and After Airing)')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Statistics for each source\n",
    "statistics = all_sessions_df.loc[all_sessions_df['calculated_source'].isin(sources2)].groupby('calculated_source')['time_diff'].agg(['mean', 'std', 'count'])\n",
    "print(statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure timestamps are consistent in their time zones\n",
    "try:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_localize('UTC')\n",
    "\n",
    "try:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_localize('UTC')\n",
    "\n",
    "# Identify airings with overlaps\n",
    "time_window_overlap = pd.Timedelta(minutes=5)\n",
    "overlap_flags = []\n",
    "\n",
    "for i, airing in detections_df.iterrows():\n",
    "    current_end = airing['detection_timestamp'] + time_window_overlap\n",
    "    overlaps = (\n",
    "        detections_df[\n",
    "            (detections_df['detection_timestamp'] < current_end) &\n",
    "            (detections_df['detection_timestamp'] > airing['detection_timestamp'])\n",
    "        ]\n",
    "    )\n",
    "    overlap_flags.append(len(overlaps) > 0)\n",
    "\n",
    "detections_df['has_overlap'] = overlap_flags\n",
    "\n",
    "# Split airings into overlapping and non-overlapping subsets\n",
    "overlapping_airings = detections_df[detections_df['has_overlap'] == True]\n",
    "non_overlapping_airings = detections_df[detections_df['has_overlap'] == False]\n",
    "\n",
    "# Function to calculate response curves for a subset of airings\n",
    "def calculate_response_curve_for_airing(airing, time_window, sources):\n",
    "    airing_time = airing['detection_timestamp']\n",
    "    relevant_sessions = activity_session_na_df[\n",
    "        (activity_session_na_df['session_timestamp'] >= (airing_time - time_window)) &\n",
    "        (activity_session_na_df['session_timestamp'] <= (airing_time + time_window)) &\n",
    "        (activity_session_na_df['calculated_source'].isin(sources))\n",
    "    ]\n",
    "    relevant_sessions['time_diff'] = (relevant_sessions['session_timestamp'] - airing_time).dt.total_seconds()  # Seconds\n",
    "    return relevant_sessions\n",
    "\n",
    "def calculate_response_curve_parallel(airings_subset, time_window, sources):\n",
    "    session_curves = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Parallelize the computation of response curves for each airing\n",
    "        futures = [\n",
    "            executor.submit(calculate_response_curve_for_airing, airing, time_window, sources)\n",
    "            for _, airing in airings_subset.iterrows()\n",
    "        ]\n",
    "        for future in futures:\n",
    "            session_curves.append(future.result())\n",
    "\n",
    "    # Combine all session curves\n",
    "    if session_curves:\n",
    "        return pd.concat(session_curves)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Calculate response curves for overlapping and non-overlapping airings\n",
    "time_window = pd.Timedelta(minutes=10)\n",
    "sources = activity_session_na_df['calculated_source'].unique()\n",
    "\n",
    "response_curve_overlap = calculate_response_curve_parallel(overlapping_airings, time_window, sources)\n",
    "response_curve_no_overlap = calculate_response_curve_parallel(non_overlapping_airings, time_window, sources)\n",
    "\n",
    "# Plot response curves by source for overlapping and non-overlapping airings\n",
    "plt.figure(figsize=(12, 6))\n",
    "bins = range(-600, 601, 10)  # Bin every 10 seconds\n",
    "\n",
    "sources = ['google-ppc']\n",
    "for source in sources:\n",
    "    # Overlapping airings\n",
    "    source_data_overlap = response_curve_overlap[response_curve_overlap['calculated_source'] == source]\n",
    "    plt.hist(\n",
    "        source_data_overlap['time_diff'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f\"Overlapping - {source}\",\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "    # Non-overlapping airings\n",
    "    source_data_no_overlap = response_curve_no_overlap[response_curve_no_overlap['calculated_source'] == source]\n",
    "    plt.hist(\n",
    "        source_data_no_overlap['time_diff'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f\"Non-Overlapping - {source}\",\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "# Set vertical scale to match the peak of the other chart\n",
    "plt.ylim(0, 0.00225)\n",
    "    \n",
    "plt.title('Response Curve Comparison by Source: Overlapping vs Non-Overlapping Airings')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare statistics for overlapping and non-overlapping airings by source\n",
    "overlap_stats = response_curve_overlap.groupby('calculated_source')['time_diff'].agg(['mean', 'std', 'count'])\n",
    "no_overlap_stats = response_curve_no_overlap.groupby('calculated_source')['time_diff'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"Statistics for Overlapping Airings by Source:\\n\", overlap_stats)\n",
    "print(\"Statistics for Non-Overlapping Airings by Source:\\n\", no_overlap_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_session_na_df['calculated_source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_session_na_df['calculated_source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['google-ppc', 'direct', 'other', 'bing', 'google-organic', 'yahoo']\n",
    "# Overlapping airings\n",
    "\n",
    "source_data_overlap2 = response_curve_overlap.loc[response_curve_overlap['calculated_source'].isin(sources)].copy().reset_index(drop=True)\n",
    "plt.hist(\n",
    "    source_data_overlap2['time_diff'],\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    label=f\"Overlapping - Airings\",\n",
    "    density=True\n",
    ")\n",
    "\n",
    "# Non-overlapping airings\n",
    "source_data_no_overlap2 = response_curve_no_overlap[response_curve_no_overlap['calculated_source'].isin(sources)].copy().reset_index(drop=True)\n",
    "plt.hist(\n",
    "    source_data_no_overlap2['time_diff'],\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    label=f\"Non-Overlapping - Airings\",\n",
    "    density=True\n",
    ")\n",
    "# Set vertical scale to match the peak of the other chart\n",
    "plt.ylim(0, 0.00125)\n",
    "\n",
    "plt.title(f'Response Curve Comparison for Targeted Sources ({sources}): Overlapping vs Non-Overlapping Airings')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "source_data_overlap2 = response_curve_overlap.loc[~response_curve_overlap['calculated_source'].isin(sources)].copy().reset_index(drop=True)\n",
    "plt.hist(\n",
    "    source_data_overlap2['time_diff'],\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    label=f\"Overlapping - Airings\",\n",
    "    density=True\n",
    ")\n",
    "\n",
    "# Non-overlapping airings\n",
    "source_data_no_overlap2 = response_curve_no_overlap[~response_curve_no_overlap['calculated_source'].isin(sources)].copy().reset_index(drop=True)\n",
    "plt.hist(\n",
    "    source_data_no_overlap2['time_diff'],\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    label=f\"Non-Overlapping - Airings\",\n",
    "    density=True\n",
    ")\n",
    "# Set vertical scale to match the peak of the other chart\n",
    "plt.ylim(0, 0.00125)\n",
    "\n",
    "plt.title(f'Response Curve Comparison for non Targeted Sources (not {sources}): Overlapping vs Non-Overlapping Airings')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['google-ppc', 'direct', 'other', 'bing', 'google-organic', 'yahoo', 'youtube', 'instagram', 'reddit', 'trustpilot', 'yelp', 'print', 'display', 'pinterest', 'amazon']\n",
    "# Overlapping airings\n",
    "for source in sources:\n",
    "    source_data_overlap2 = response_curve_overlap.loc[response_curve_overlap['calculated_source'] == source].copy().reset_index(drop=True)\n",
    "    plt.hist(\n",
    "        source_data_overlap2['time_diff'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f\"Overlapping - Airings\",\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "    # Non-overlapping airings\n",
    "    source_data_no_overlap2 = response_curve_no_overlap[response_curve_no_overlap['calculated_source'] == source].copy().reset_index(drop=True)\n",
    "    plt.hist(\n",
    "        source_data_no_overlap2['time_diff'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f\"Non-Overlapping - Airings\",\n",
    "        density=True\n",
    "    )\n",
    "    # Set vertical scale to match the peak of the other chart\n",
    "    plt.ylim(0, 0.00225)\n",
    "\n",
    "    plt.title(f'Response Curve Comparison by Targeted Source ({source}): Overlapping vs Non-Overlapping Airings')\n",
    "    plt.xlabel('Time Difference (seconds)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "try:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    activity_session_na_df['session_timestamp'] = pd.to_datetime(activity_session_na_df['session_time']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_localize('UTC')\n",
    "\n",
    "# Match sessions with airings and calculate lag\n",
    "time_window = pd.Timedelta(minutes=10)\n",
    "session_curves = []\n",
    "\n",
    "# Calculate response curves for each airing\n",
    "lag_data = []\n",
    "\n",
    "for i, airing in detections_df.iterrows():\n",
    "    airing_time = airing['detection_timestamp']\n",
    "    ad_length = airing.get('ad_length', 'Unknown')  # Use 'Unknown' if ad_length isn't provided\n",
    "    \n",
    "    # Filter sessions within the 10-minute window\n",
    "    relevant_sessions = activity_session_na_df[\n",
    "        (activity_session_na_df['session_timestamp'] >= (airing_time - time_window)) &\n",
    "        (activity_session_na_df['session_timestamp'] <= (airing_time + time_window))\n",
    "    ]\n",
    "    relevant_sessions['time_diff'] = (relevant_sessions['session_timestamp'] - airing_time).dt.total_seconds()  # Time difference in seconds\n",
    "    \n",
    "    if len(relevant_sessions) > 0:\n",
    "        # Calculate kernel density estimation to find peak response\n",
    "        kde = gaussian_kde(relevant_sessions['time_diff'])\n",
    "        time_range = np.linspace(-600, 600, 1200)  # Range of time differences (seconds)\n",
    "        kde_values = kde(time_range)\n",
    "        peak_time = time_range[np.argmax(kde_values)]  # Peak time relative to airing\n",
    "        \n",
    "        # Store lag data\n",
    "        lag_data.append({\n",
    "            'airing_id': airing['occurrence_id'],\n",
    "            'ad_length': ad_length,\n",
    "            'lag': peak_time  # Lag in seconds\n",
    "        })\n",
    "    session_curves.append(relevant_sessions)\n",
    "\n",
    "# Combine all session curves\n",
    "all_sessions_df = pd.concat(session_curves)\n",
    "lag_df = pd.DataFrame(lag_data)\n",
    "\n",
    "# Plot lag distribution by ad length\n",
    "plt.figure(figsize=(10, 6))\n",
    "for ad_length in lag_df['ad_length'].unique():\n",
    "    subset = lag_df[lag_df['ad_length'] == ad_length]\n",
    "    plt.hist(subset['lag'], bins=30, alpha=0.5, label=f\"{ad_length}s Ad Length\", density=True)\n",
    "\n",
    "plt.title('Lag Distribution by Ad Length')\n",
    "plt.xlabel('Lag (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare lag statistics by ad length\n",
    "lag_stats = lag_df.groupby('ad_length')['lag'].agg(['mean', 'std', 'count'])\n",
    "print(\"Lag Statistics by Ad Length:\\n\", lag_stats)\n",
    "\n",
    "# Response curve comparison by ad length\n",
    "plt.figure(figsize=(12, 6))\n",
    "bins = range(-600, 601, 10)  # Bin every 10 seconds\n",
    "\n",
    "for ad_length in detections_df['ad_length'].unique():\n",
    "    # Filter sessions for this ad length\n",
    "    airings_subset = detections_df[detections_df['ad_length'] == ad_length]\n",
    "    session_curves_length = []\n",
    "    for i, airing in airings_subset.iterrows():\n",
    "        airing_time = airing['detection_timestamp']\n",
    "        relevant_sessions = activity_session_na_df[\n",
    "            (activity_session_na_df['session_timestamp'] >= (airing_time - time_window)) &\n",
    "            (activity_session_na_df['session_timestamp'] <= (airing_time + time_window))\n",
    "        ]\n",
    "        relevant_sessions['time_diff'] = (relevant_sessions['session_timestamp'] - airing_time).dt.total_seconds()\n",
    "        session_curves_length.append(relevant_sessions)\n",
    "    \n",
    "    if session_curves_length:\n",
    "        sessions_df = pd.concat(session_curves_length)\n",
    "        plt.hist(\n",
    "            sessions_df['time_diff'], \n",
    "            bins=bins, \n",
    "            alpha=0.5, \n",
    "            label=f\"{ad_length}s Ad Length\", \n",
    "            density=True\n",
    "        )\n",
    "\n",
    "plt.title('Response Curves by Ad Length')\n",
    "plt.xlabel('Time Difference (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define targeted sources\n",
    "targeted_sources = ['google-ppc', 'direct', 'other', 'bing', 'google-organic', 'yahoo']\n",
    "\n",
    "# Filter web sessions to only include targeted sources\n",
    "filtered_sessions = activity_session_na_df[\n",
    "    activity_session_na_df['calculated_source'].isin(targeted_sources)\n",
    "]\n",
    "\n",
    "# Step 1: Filter to only non-overlapping airings\n",
    "non_overlapping_airings = detections_df[detections_df['has_overlap'] == False]\n",
    "\n",
    "try:\n",
    "    filtered_sessions['session_timestamp'] = pd.to_datetime(filtered_sessions['session_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    filtered_sessions['session_timestamp'] = pd.to_datetime(filtered_sessions['session_time']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['detection_timestamp'] = pd.to_datetime(detections_df['detection_timestamp']).dt.tz_localize('UTC')\n",
    "try:\n",
    "    detections_df['attribution_end_time'] = pd.to_datetime(detections_df['attribution_end_time']).dt.tz_convert('UTC')\n",
    "except:\n",
    "    detections_df['attribution_end_time'] = pd.to_datetime(detections_df['attribution_end_time']).dt.tz_localize('UTC')\n",
    "\n",
    "# Time-decay function\n",
    "def calculate_credit(possible_sessions, airing_time, decay_factor=0.01):\n",
    "    \"\"\"\n",
    "    Add context to sessions: time_diff and credit_weight using a decay function.\n",
    "    \"\"\"\n",
    "    updated_sessions = []\n",
    "    for session in possible_sessions:\n",
    "        time_diff = (session['session_timestamp'] - airing_time).total_seconds()\n",
    "        # Credit weight using time-decay; no credit before airing starts\n",
    "        credit_weight = np.exp(-decay_factor * max(time_diff, 0)) if time_diff >= 0 else 0\n",
    "        updated_sessions.append({\n",
    "            'activity_session_id': session['activity_session_id'],\n",
    "            'calculated_source': session['calculated_source'],\n",
    "            'time_diff': time_diff,\n",
    "            'credit_weight': credit_weight\n",
    "        })\n",
    "    return updated_sessions\n",
    "\n",
    "# Step 1: Collect all possible sessions\n",
    "def collect_possible_sessions(airing_row, sessions_df):\n",
    "    airing_start = airing_row['detection_timestamp']\n",
    "    airing_end = airing_row['attribution_end_time']\n",
    "    possible_sessions = sessions_df[\n",
    "        (sessions_df['session_timestamp'] >= airing_start) &\n",
    "        (sessions_df['session_timestamp'] <= airing_end)\n",
    "    ]\n",
    "    return possible_sessions[['activity_session_id', 'calculated_source', 'session_timestamp']].to_dict(orient='records')\n",
    "\n",
    "# Step 2: Update detections_df with full context\n",
    "def add_context_to_detections(detections_df, sessions_df):\n",
    "    detections_df['possible_attributed_sessions'] = detections_df.apply(\n",
    "        lambda row: collect_possible_sessions(row, sessions_df), axis=1\n",
    "    )\n",
    "    detections_df['attributed_sessions_with_context'] = detections_df.apply(\n",
    "        lambda row: calculate_credit(row['possible_attributed_sessions'], row['detection_timestamp']), axis=1\n",
    "    )\n",
    "    return detections_df\n",
    "\n",
    "# Apply the updated function\n",
    "detections_df = add_context_to_detections(detections_df, filtered_sessions)\n",
    "\n",
    "# Preview the updated detections_df\n",
    "print(detections_df[['occurrence_id', 'detection_timestamp', 'attribution_end_time', 'attributed_sessions_with_context']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot bell curve for each source\n",
    "sources = ['Direct', 'Search', 'Unknown']\n",
    "for source in sources:\n",
    "    source_data = all_sessions_pd[all_sessions_pd['source'] == source]\n",
    "    plt.hist(source_data['time_diff'], bins=30, alpha=0.5, label=source, density=True)\n",
    "\n",
    "plt.title('Web Session Response Curves')\n",
    "plt.xlabel('Time Difference (minutes)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Statistics for each source\n",
    "statistics = all_sessions_pd.groupby('source')['time_diff'].agg(['mean', 'std', 'count'])\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Overlapping Airings Detection\", dataframe=detections_df.loc[detections_df['overlaps'] == True, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes: airings_df and web_sessions_df\n",
    "# airings_df: ['airing_time', 'dma', 'attribution_window']\n",
    "# web_sessions_df: ['session_time', 'dma']\n",
    "\n",
    "# Step 1: Calculate attribution windows\n",
    "detections_df['attribution_end_time'] = detections_df['detection_timestamp'] + pd.to_timedelta(detections_df['attribution_window'], unit='s')\n",
    "detections_df['airing_time'] = detections_df['detection_timestamp']\n",
    "airings_df = detections_df.copy()\n",
    "web_sessions_df = activity_session_df.copy()\n",
    "# Step 2: Match sessions to airings\n",
    "def match_sessions(airings_df, web_sessions_df):\n",
    "    matches = []\n",
    "    for _, airing in airings_df.iterrows():\n",
    "        # Filter web sessions within time range and matching DMA\n",
    "        valid_sessions = web_sessions_df[\n",
    "            (web_sessions_df['session_time'] >= airing['detection_timestamp']) &\n",
    "            (web_sessions_df['session_time'] <= airing['attribution_end_time']) &\n",
    "            (web_sessions_df['geo_neustar_id'] == airing['geo_neustar_id'])\n",
    "        ]\n",
    "        valid_sessions = valid_sessions.copy()\n",
    "        valid_sessions['airing_time'] = airing['airing_time']\n",
    "        matches.append(valid_sessions)\n",
    "\n",
    "    # Combine all matches\n",
    "    matched_df = pd.concat(matches, ignore_index=True)\n",
    "    return matched_df\n",
    "\n",
    "# Step 3: Apply function\n",
    "matched_sessions = match_sessions(airings_df, web_sessions_df)\n",
    "\n",
    "# Step 4: Analyze results\n",
    "# Example: Response curve by time since airing\n",
    "matched_sessions['time_since_airing'] = (matched_sessions['session_time'] - matched_sessions['airing_time']).dt.total_seconds()\n",
    "response_curve = matched_sessions.groupby('time_since_airing').size()\n",
    "response_curve.plot(kind='line', title='Response Curve')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
