{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import functions.core_functions as core_functions\n",
    "import functions.pyarrow_functions as pyarrow_functions\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe.utils import assert_eq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import json\n",
    "import gc\n",
    "import cudf\n",
    "import os\n",
    "import yaml\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "importlib.reload(core_functions)\n",
    "importlib.reload(pyarrow_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = yaml.safe_load(open('table-schemas.yaml'))\n",
    "\n",
    "detections_python = table_schema['detections_python_schema']\n",
    "detections_python\n",
    "\n",
    "n90_schema_dict = {item['name']: item['type'] for item in detections_python}\n",
    "veil_schema_dict = {item['name']: item['type'] for item in detections_python if 'geo_' not in item['name']}\n",
    "final_detections_cols = []\n",
    "for item in detections_python:\n",
    "    final_detections_cols.append(item['name'])\n",
    "    # print(f\"{item['name']} = {item['type']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veil_schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlikely to be used\n",
    "from google.cloud import bigquery\n",
    "def convert_schema_to_bigquery(schema):\n",
    "    # Mapping from your data types to BigQuery data types\n",
    "    type_mapping = {\n",
    "        'int64': 'INTEGER',\n",
    "        'float64': 'FLOAT',\n",
    "        'string': 'STRING',\n",
    "        'bool': 'BOOLEAN',\n",
    "        'datetime64[ns, UTC]': 'TIMESTAMP',\n",
    "        'datetime64[ns]': 'TIMESTAMP',  # If any datetime fields lack timezone info\n",
    "        'object': 'STRING',  # Pandas may use 'object' for string data\n",
    "        # Add more mappings if necessary\n",
    "    }\n",
    "    \n",
    "    bigquery_schema = []\n",
    "    for field_name, field_type in schema.items():\n",
    "        # Map the data type to BigQuery type\n",
    "        bq_type = type_mapping.get(field_type, 'STRING')  # Default to STRING if type not found\n",
    "        \n",
    "        # Create SchemaField\n",
    "        schema_field = bigquery.SchemaField(\n",
    "            name=field_name,\n",
    "            field_type=bq_type,\n",
    "            mode='NULLABLE'  # Assuming all fields are nullable; adjust if needed\n",
    "        )\n",
    "        \n",
    "        bigquery_schema.append(schema_field)\n",
    "    \n",
    "    return bigquery_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_schema_to_bigquery(n90_schema_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "resp = core_functions.initialize_clients(service_account_secret_name='SA_ADHOC_BILLING')\n",
    "resp2 = core_functions.initialize_clients(service_account_secret_name='SA_N90_CORE_APPS')\n",
    "\n",
    "config = resp.get('config')\n",
    "bigquery_client = resp.get('clients').get('bigquery_client')\n",
    "n90_bigquery_client = resp2.get('clients').get('bigquery_client')\n",
    "storage_client = resp.get('clients').get('storage_client')\n",
    "sf_client = resp.get('clients').get('sf_client')\n",
    "veil_billing = resp.get('config').get('veil_billing')\n",
    "veil_vars = resp.get('config').get('veil_billing').get('vars')\n",
    "# print(veil_billing)\n",
    "sfdc_adv_account_cols = veil_billing.get('vars').get('sfdc_adv_account_cols')\n",
    "sfdc_rate_card_cols = veil_billing.get('vars').get('sfdc_rate_card_cols')\n",
    "unknown_dma_overrides = config.get('national_dma_overrides_to_us_national')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "mongo_tables = ['dmas', 'shows']\n",
    "mongo_data = core_functions.fetch_table_data(\n",
    "    project_id=veil_billing.get('avs_project_id'),\n",
    "    dataset_id='mongo',\n",
    "    table_names=mongo_tables,\n",
    "    bigquery_client=bigquery_client\n",
    ")\n",
    "dmas_df = core_functions.fix_df_dtypes(mongo_data['dmas'])\n",
    "shows_df = core_functions.fix_df_dtypes(mongo_data['shows'])\n",
    "shows_df['length'] = shows_df['length'].fillna(0.0).astype('Int64')\n",
    "master_channel_sql = f\"\"\"\n",
    "    SELECT * from `adhoc-billing.avs_billing_process.master_channels_expanded`\n",
    "\"\"\"\n",
    "master_channel_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(master_channel_sql, bigquery_client))\n",
    "dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype('Int64')\n",
    "mask = dmas_df['neustar_id'].isin(unknown_dma_overrides)\n",
    "dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "dmas_df['dma_rank'] = dmas_df['dma_rank'].fillna(0).astype('Int64')\n",
    "shows_df['show_id'] = shows_df['show_id'].fillna(-6).astype('Int64')\n",
    "shows_df.rename(columns={'id': 'show_record_id'}, inplace=True)\n",
    "master_channel_df['broadcaster_id'] = master_channel_df['broadcaster_id'].fillna(-10).astype('Int64')\n",
    "\n",
    "broadcast_cal_sql = f\"\"\"\n",
    "    SELECT id as bcw_id, bcw_index, bcm_index, bcw_start_date, bcw_end_date FROM `adhoc-billing.avs_billing_process.lu_broadcast_week`\n",
    "\"\"\"\n",
    "broadcast_cal_df = core_functions.fetch_gbq_data(query=broadcast_cal_sql, bigquery_client=bigquery_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "geo_sql = f\"\"\"\n",
    "    SELECT * from `next90-core-applications.next90_analytics.geos` WHERE geo_type = 'dma'\n",
    "\"\"\"\n",
    "geo_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(query=geo_sql, bigquery_client=n90_bigquery_client))\n",
    "\n",
    "int_cols = ['geo_location', 'geo_neustar_id','geo_us_msa_id', 'geo_us_county_fips_id','geo_ca_cma_id']\n",
    "for col in int_cols:\n",
    "    geo_df[col] = geo_df[col].fillna(-1).astype('Int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# def generate_month_list(start_date, end_date):\n",
    "#     \"\"\"\n",
    "#     Generate a list of months in 'YYYY-MM' format from start_date to end_date (inclusive).\n",
    "    \n",
    "#     Args:\n",
    "#         start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "#         end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of months in 'YYYY-MM' format.\n",
    "#     \"\"\"\n",
    "#     start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "#     end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "#     if start > end:\n",
    "#         raise ValueError(\"start_date must be before or equal to end_date\")\n",
    "    \n",
    "#     month_list = []\n",
    "#     current = start\n",
    "#     while current <= end:\n",
    "#         month_list.append(current.strftime(\"%Y-%m\"))\n",
    "#         current += relativedelta(months=1)\n",
    "    \n",
    "#     return month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "veil_storage_options = None\n",
    "veil_storage_options = config.get('VEIL_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "n90_storage_options = None\n",
    "n90_storage_options = config.get('N90_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "s4_storage_options = None\n",
    "s4_storage_options = config.get('S4_STORAGE_OPTIONS')\n",
    "\n",
    "veil_billing_bucket = None\n",
    "veil_billing_bucket = config.get('veil_billing').get('billing_gcs_bucket_id')\n",
    "\n",
    "# process_df['profile__attributes']\n",
    "\n",
    "s4_bucket = 'n90-data-lake-stl'\n",
    "s4_output_prefix = 'veil/detections_v2'\n",
    "\n",
    "\n",
    "n90_bucket = None\n",
    "n90_bucket = 'n90_veil_partner'\n",
    "n90_bucket_2 = 'n90-data-lake'\n",
    "\n",
    "veil_output_prefix = None\n",
    "veil_output_prefix = 'detections_v2'\n",
    "n90_output_prefix = None\n",
    "n90_output_prefix = 'advocado-looker/avs_prod/detections_v2'\n",
    "n90_output_prefix_2 = 'avs_prod/detections_v2'\n",
    "\n",
    "# importlib.reload(core_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Monthly Start\n",
    "# read partitioned parquet files from s3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_input_prefix = 'veil/encodings_v2'\n",
    "encodings_df = core_functions.fix_df_dtypes(pyarrow_functions.load_encodings_for_detections(s4_storage_options, s4_bucket, encodings_input_prefix))\n",
    "encodings_df.head()\n",
    "# encodings_df['length_in_seconds'] = encodings_df['length_in_seconds'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add bcw and bcm as segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: look at whether to use this or not\n",
    "def process_detections(df, encodings_df):\n",
    "    df.dtypes\n",
    "\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], utc=True)\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['tv_show_id'] = df['tv_show_id'].fillna(-10) \n",
    "    df['group_occurrence_id'] = df['group_occurrence_id'].fillna(-6)\n",
    "    # dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "    df['occurrence_id'] = df['occurrence_id'].astype(float)\n",
    "    # # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "    # dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "    # dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "    df = df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "    shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "    # mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "    df = df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "    geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "    df = df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "    gc.collect()\n",
    "\n",
    "    needed_encodings = df['encoding_id'].unique().tolist()\n",
    "    \n",
    "    \n",
    "    encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    \n",
    "    encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "    needed_encodings_df = encodings_df.loc[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "    df = df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "    len(needed_encodings_df)\n",
    "    df = df.sort_values(by=['occurrence_id', 'year', 'month', 'day'])\n",
    "    billing_last_updated = pd.Timestamp.utcnow()\n",
    "    df['billing_det_last_updated'] = billing_last_updated\n",
    "    df['billing_det_last_updated'] = pd.to_datetime(df['billing_det_last_updated'], utc=True)\n",
    "    billing_last_audit_id = core_functions.generate_uuid()\n",
    "    df['billing_det_last_updated'] = billing_last_audit_id\n",
    "    df.head()\n",
    "\n",
    "    df_clean = core_functions.enforce_schema(df, n90_schema)\n",
    "\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    # core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    # print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    core_functions.write_hive_partitioned_parquet(df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "    # new cell\n",
    "    partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "    # new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "    #  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "    # change to veil format\n",
    "    veil_keys = list(veil_schema.keys())\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    core_functions.write_hive_partitioned_parquet(df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    # core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    # print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "    # new cell\n",
    "    partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "    del df_clean\n",
    "    del needed_encodings_df\n",
    "    gc.collect()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "reload_detections = True\n",
    "test_mode = True\n",
    "test_filter = \"WHERE date_time >= '2024-11-01' AND date_time < '2024-12-31'\"\n",
    "detections_table = None\n",
    "if reload_detections:\n",
    "    detections_table = 'adhoc-billing.avs_billing_process.avs_detections_master'\n",
    "    dates_sql = f\"\"\"\n",
    "    SELECT  cast(min(date_time) as string) as min_date , cast(current_date() as string) as max_date\n",
    "    FROM `{detections_table}` {test_filter}\n",
    "\n",
    "    \"\"\"\n",
    "else:\n",
    "    detections_table = 'adhoc-billing.avs_billing_process.detections'\n",
    "    dates_sql = f\"\"\"\n",
    "        SELECT  CAST(DATE_ADD(DATE(MAX(date_time)), INTERVAL 1 DAY) AS STRING) as min_date , cast(current_date() as string) as max_date\n",
    "        FROM `{detections_table}`\n",
    "        \"\"\"\n",
    "dates_df = core_functions.fetch_gbq_data(query=dates_sql, bigquery_client=bigquery_client)\n",
    "min_date = dates_df['min_date'][0]\n",
    "max_date = dates_df['max_date'][0]\n",
    "\n",
    "print(f\"min_date: {min_date}, max_date: {max_date}\")\n",
    "detections_df = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detections_df:\n",
    "    print('detections_df exists')\n",
    "else:\n",
    "    print('detections_df does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if detections_df:\n",
    "    del detections_df\n",
    "    gc.collect()\n",
    "\n",
    "process_month_group_sql = f\"\"\"\n",
    "    select distinct CONCAT(EXTRACT(YEAR FROM date_time),'-',FORMAT('%02d', EXTRACT(MONTH FROM date_time))) as process_month_group\n",
    "    FROM `{detections_table}` \n",
    "    WHERE date_time >= '{min_date}' AND date_time < '{max_date}'\n",
    "    ORDER BY process_month_group\n",
    "\"\"\"\n",
    "\n",
    "process_month_groups = core_functions.fetch_gbq_data(query=process_month_group_sql, bigquery_client=bigquery_client)\n",
    "process_month_groups = process_month_groups['process_month_group'].tolist()\n",
    "process_month_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "completed_months = []\n",
    "\n",
    "for process_month_group in process_month_groups:\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    detections_sql = f\"\"\"\n",
    "        SELECT CAST(occurrence_id AS FLOAT64) AS occurrence_id, encoding_id, date_time, broadcaster_id, cost, tv_show_id, origin, group_occurrence_id, \n",
    "        EXTRACT(YEAR FROM date_time) AS year, EXTRACT(MONTH FROM date_time) as month, EXTRACT(DAY FROM date_time) as day,\n",
    "        CONCAT(EXTRACT(YEAR FROM date_time),'-',FORMAT('%02d', EXTRACT(MONTH FROM date_time))) as process_month_group,\n",
    "        last_updated, last_audit_id \n",
    "\n",
    "        FROM `{detections_table}` \n",
    "\n",
    "        WHERE date_time > '{min_date}' AND date_time < '{max_date}'\n",
    "        AND CONCAT(EXTRACT(YEAR FROM date_time),'-',FORMAT('%02d', EXTRACT(MONTH FROM date_time))) = '{process_month_group}'\n",
    "        \"\"\"\n",
    "    detections_df = core_functions.fetch_gbq_data(query=detections_sql, bigquery_client=bigquery_client)\n",
    "    \n",
    "    detections_df['occurrence_id'] = detections_df['occurrence_id'].astype('Float64')\n",
    "    detections_df['cost'] = detections_df['cost'].astype('Float64')\n",
    "    \n",
    "    min_occurrence_id = detections_df['occurrence_id'].min()\n",
    "    max_occurrence_id = detections_df['occurrence_id'].max()\n",
    "\n",
    "    archive_sql = f\"\"\"\n",
    "    select occurrence_id , any_value(affiliate) as affiliate, any_value(callsign) as callsign, any_value(dma_id) as dma_id\n",
    "    from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "    where occurrence_id >= {min_occurrence_id} and occurrence_id <= {max_occurrence_id}\n",
    "    group by occurrence_id\n",
    "    \"\"\"\n",
    "\n",
    "    # archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "    print(len(detections_df))\n",
    "    # print(len(archive_df))\n",
    "    detection2 = detections_df.merge(master_channel_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "\n",
    "    # detection2.loc[detection2['affiliate'].isnull(), 'affiliate'] \n",
    "\n",
    "    detections_df = detection2.copy()\n",
    "    del detection2\n",
    "    gc.collect()\n",
    "\n",
    "    print('starting the append of encodings')\n",
    "    # prod is good from april 2024 on the 8th\n",
    "    encoding_ids = detections_df['encoding_id'].unique().tolist()\n",
    "    \n",
    "    needed_encodings_df = encodings_df.loc[encodings_df['encoding_id'].isin(encoding_ids)].copy()\n",
    "    # # def process_detections(df, encodings_df):\n",
    "    df = detections_df.copy().reset_index(drop=True)\n",
    "    df = df.sort_values(by=['occurrence_id', 'date_time'], ascending=[True, False])\n",
    "    df = df.drop_duplicates(subset=['occurrence_id'], keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    # # # def process_detections(df):\n",
    "    # df.head()\n",
    "    # # for col in df.columns:\n",
    "    # #     print(f\"'{col}': '{df[col].dtype}',\")\n",
    "\n",
    "    # df['date_time'] = pd.to_datetime(df['date_time'], utc=True)\n",
    "    # df['year'] = df['date_time'].dt.year\n",
    "    # df['month'] = df['date_time'].dt.month\n",
    "    # df['day'] = df['date_time'].dt.day\n",
    "    df['tv_show_id'] = df['tv_show_id'].fillna(-10).astype('Int64')\n",
    "    df['group_occurrence_id'] = df['group_occurrence_id'].fillna(-6)\n",
    "    # dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "    # df['occurrence_id'] = df['occurrence_id'].astype(float)\n",
    "    # # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "    # dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "    # dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "    print('starting the merge of dmas')\n",
    "    df = df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "    shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "    # mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "    df = df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "    geo_df['geo_location'] = geo_df['geo_location'].astype('Int64')\n",
    "    df = df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "    gc.collect()\n",
    "\n",
    "    # needed_encodings = df['encoding_id'].unique().tolist()\n",
    "    # encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "    # encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "    # needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "    df = df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "    print('adding broadcast cal details')\n",
    "    df['key'] = 1\n",
    "    broadcast_cal_df['key'] = 1\n",
    "    df['airing_date_time'] = pd.to_datetime(df['date_time'])\n",
    "    broadcast_cal_df['bcw_start_date'] = pd.to_datetime(broadcast_cal_df['bcw_start_date'])\n",
    "    broadcast_cal_df['bcw_end_date'] = pd.to_datetime(broadcast_cal_df['bcw_end_date'])\n",
    "    ref_df = None\n",
    "    ref_df = broadcast_cal_df.loc[(broadcast_cal_df['bcw_start_date'] >= df['airing_date_time'].min()) & (broadcast_cal_df['bcw_end_date'] <= df['airing_date_time'].max())]\n",
    "    merged_df = None\n",
    "    merged_df = pd.merge(df, ref_df, on='key').drop(['key', 'airing_date_time'], axis=1)\n",
    "    merged_df['bc_year_index'] = merged_df['bcm_index'].astype(str).str[:4].astype('Int64')\n",
    "    merged_df['bcm_index'] = merged_df['bcm_index'].astype('Float64')\n",
    "    merged_df['bcw_index'] = merged_df['bcw_index'].astype('Float64')\n",
    "    merged_df['segments_format_id_group'] = merged_df['segments_format_id_group'].astype('Int64')\n",
    "    merged_df['dma_rank'] = merged_df['dma_rank'].astype('Int64')\n",
    "    merged_df['neustar_id'] = merged_df['neustar_id'].astype('Int64')\n",
    "    merged_df['show_id'] = merged_df['show_id'].astype('Int64')\n",
    "\n",
    "    merged_df['last_updated'] = merged_df['last_updated'].apply(lambda x: core_functions.time_to_seconds(x))\n",
    "    merged_df\n",
    "    # df2 = core_functions.process_detections_segments(merged_df)\n",
    "    print('Starting paralell processing')\n",
    "    \n",
    "    \n",
    "    original_dtypes = merged_df.dtypes.to_dict()\n",
    "    merged_data_dict = merged_df.to_dict(orient='records')\n",
    "    processed_data = core_functions.process_detections_in_parallel(merged_data_dict)\n",
    "    df = pd.DataFrame.from_records(processed_data)\n",
    "\n",
    "    # Restore original data types\n",
    "    for col, dtype in original_dtypes.items():\n",
    "        if col in df:\n",
    "            df[col] = df[col].astype(dtype)\n",
    "\n",
    "    # df = df2[final_detections_cols].sort_values(by=['year', 'month', 'bcm_index', 'bcw_index', 'day', 'occurrence_id']).copy().reset_index(drop=True)\n",
    "    \n",
    "    billing_last_updated = pd.Timestamp.utcnow()\n",
    "    df['billing_det_last_updated'] = billing_last_updated\n",
    "    df['billing_det_last_updated'] = pd.to_datetime(df['billing_det_last_updated'], utc=True)\n",
    "    billing_last_audit_id = core_functions.generate_uuid()\n",
    "    df['billing_det_last_updated'] = billing_last_audit_id\n",
    "    df.head()\n",
    "\n",
    "    print('beginning to write')\n",
    "    partition_cols = None\n",
    "    partition_cols = ['year', 'month']\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    core_functions.write_hive_partitioned_parquet(df, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "    core_functions.write_hive_partitioned_parquet(df, n90_bucket_2, n90_output_prefix_2, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket_2}/{n90_output_prefix_2}\")\n",
    "    # new cell\n",
    "    # partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "    # new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "    #  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "    # change to veil format\n",
    "    \n",
    "    core_functions.write_hive_partitioned_parquet_s4(df, s4_bucket, s4_output_prefix, partition_cols, s4_storage_options)\n",
    "    print(f\"Finished writing to {s4_bucket}/{s4_output_prefix}\")\n",
    "    \n",
    "    veil_keys = list(veil_schema_dict.keys())\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    core_functions.write_hive_partitioned_parquet(df[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    # core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    # print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "    # new cell\n",
    "    partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "    del df\n",
    "    del needed_encodings_df\n",
    "    gc.collect()\n",
    "    completed_months.append(process_month_group)\n",
    "    print(f\"Completed {process_month_group} with {len(df)} rows\")\n",
    "    print('------------------------------------------')\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "print('All months completed: ', completed_months)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_cal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Refactor the worker function\n",
    "def process_month_ckunk(chunk, shared_data, media='TV'):\n",
    "    # Unpack shared data\n",
    "    master_channel_df = shared_data['master_channel_df']\n",
    "    encodings_df = shared_data['encodings_df']\n",
    "    dmas_df = shared_data['dmas_df']\n",
    "    geo_df = shared_data['geo_df']\n",
    "    shows_df = shared_data['shows_df']\n",
    "    broadcast_cal_df = shared_data['broadcast_cal_df']\n",
    "\n",
    "    # Process chunk as before\n",
    "    chunk['occurrence_id'] = chunk['occurrence_id'].astype('Float64')\n",
    "    chunk['cost'] = chunk['cost'].astype('Float64')\n",
    "    \n",
    "    chunk2 = chunk.merge(master_channel_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "\n",
    "    # detection2.loc[detection2['affiliate'].isnull(), 'affiliate'] \n",
    "\n",
    "    chunk = chunk2.copy()\n",
    "    del chunk2\n",
    "    gc.collect()\n",
    "\n",
    "    print('starting the append of encodings')\n",
    "    # prod is good from april 2024 on the 8th\n",
    "    _encoding_ids = chunk['encoding_id'].unique().tolist()\n",
    "    \n",
    "    _needed_encodings_df = encodings_df.loc[encodings_df['encoding_id'].isin(_encoding_ids)].copy()\n",
    "    # # def process_detections(df, encodings_df):\n",
    "    _df = chunk.copy().reset_index(drop=True)\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    _df = _df.sort_values(by=['occurrence_id', 'date_time'], ascending=[True, False])\n",
    "    _df = _df.drop_duplicates(subset=['occurrence_id'], keep='first')\n",
    "    _df = _df.reset_index(drop=True)\n",
    "\n",
    "    _df['tv_show_id'] = _df['tv_show_id'].fillna(-10).astype('Int64')\n",
    "    _df['group_occurrence_id'] = _df['group_occurrence_id'].fillna(-6)\n",
    "\n",
    "    print('starting the merge of dmas')\n",
    "    _df = _df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "    shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "    # mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "    _df = _df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "    geo_df['geo_location'] = geo_df['geo_location'].astype('Int64')\n",
    "    _df = _df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "    gc.collect()\n",
    "\n",
    "    # needed_encodings = df['encoding_id'].unique().tolist()\n",
    "    # encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "    # encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "    # needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "    _df = _df.merge(_needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "    print('adding broadcast cal details')\n",
    "    _df['key'] = 1\n",
    "    broadcast_cal_df['key'] = 1\n",
    "    _df['airing_date_time'] = pd.to_datetime(df['date_time'])\n",
    "    broadcast_cal_df['bcw_start_date'] = pd.to_datetime(broadcast_cal_df['bcw_start_date'])\n",
    "    broadcast_cal_df['bcw_end_date'] = pd.to_datetime(broadcast_cal_df['bcw_end_date'])\n",
    "    _ref_df = None\n",
    "    _ref_df = broadcast_cal_df.loc[(broadcast_cal_df['bcw_start_date'] >= _df['airing_date_time'].min()) & (broadcast_cal_df['bcw_end_date'] <= _df['airing_date_time'].max())]\n",
    "    _merged_df = None\n",
    "    _merged_df = pd.merge(_df, _ref_df, on='key').drop(['key', 'airing_date_time'], axis=1)\n",
    "    _merged_df['bc_year_index'] = _merged_df['bcm_index'].astype(str).str[:4].astype('Int64')\n",
    "    _merged_df['bcm_index'] = _merged_df['bcm_index'].astype('Float64')\n",
    "    _merged_df['bcw_index'] = _merged_df['bcw_index'].astype('Float64')\n",
    "    _merged_df['segments_format_id_group'] = _merged_df['segments_format_id_group'].astype('Int64')\n",
    "    _merged_df['dma_rank'] = _merged_df['dma_rank'].astype('Int64')\n",
    "    _merged_df['neustar_id'] = _merged_df['neustar_id'].astype('Int64')\n",
    "    _merged_df['show_id'] = _merged_df['show_id'].astype('Int64')\n",
    "\n",
    "    _merged_df['last_updated'] = _merged_df['last_updated'].apply(lambda x: core_functions.time_to_seconds(x))\n",
    "    _merged_df\n",
    "    # df2 = core_functions.process_detections_segments(merged_df)\n",
    "    print('Starting paralell processing')\n",
    "    _df2 = core_functions.process_detections_in_parallel(_merged_df)\n",
    "\n",
    "    _df = _df2[final_detections_cols].sort_values(by=['year', 'month', 'bcm_index', 'bcw_index', 'day', 'occurrence_id']).copy().reset_index(drop=True)\n",
    "    del d_df2f2\n",
    "    gc.collect()\n",
    "    \n",
    "    billing_last_updated = pd.Timestamp.utcnow()\n",
    "    df['billing_det_last_updated'] = billing_last_updated\n",
    "    df['billing_det_last_updated'] = pd.to_datetime(df['billing_det_last_updated'], utc=True)\n",
    "    billing_last_audit_id = core_functions.generate_uuid()\n",
    "    df['billing_det_last_updated'] = billing_last_audit_id\n",
    "    \n",
    "    # Explicit garbage collection\n",
    "    del chunk2\n",
    "    gc.collect()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month_in_parallel(df, shared_data, media='TV', num_workers=10, chunk_size=500000):\n",
    "    # Split DataFrame into chunks\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Use partial to pass shared data to each worker\n",
    "    worker_func = partial(process_month_ckunk, shared_data=shared_data, media=media)\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(worker_func, chunks)\n",
    "    \n",
    "    # Concatenate the processed chunks into a single DataFrame\n",
    "    _processed_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Perform garbage collection after combining chunks\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "\n",
    "    return _processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack shared data into a dictionary\n",
    "shared_data = {\n",
    "    'master_channel_df': master_channel_df,\n",
    "    'encodings_df': encodings_df,\n",
    "    'dmas_df': dmas_df,\n",
    "    'geo_df': geo_df,\n",
    "    'shows_df': shows_df,\n",
    "    'broadcast_cal_df': broadcast_cal_df\n",
    "}\n",
    "\n",
    "for process_month_group in process_month_groups:\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    detections_sql = f\"\"\"\n",
    "        SELECT CAST(occurrence_id AS FLOAT64) AS occurrence_id, encoding_id, date_time, broadcaster_id, cost, tv_show_id, origin, group_occurrence_id, \n",
    "        EXTRACT(YEAR FROM date_time) AS year, EXTRACT(MONTH FROM date_time) as month, EXTRACT(DAY FROM date_time) as day,\n",
    "        CONCAT(EXTRACT(YEAR FROM date_time),'-',FORMAT('%02d', EXTRACT(MONTH FROM date_time))) as process_month_group,\n",
    "        last_updated, last_audit_id \n",
    "\n",
    "        FROM `{detections_table}` \n",
    "\n",
    "        WHERE date_time > '{min_date}' AND date_time < '{max_date}'\n",
    "        AND CONCAT(EXTRACT(YEAR FROM date_time),'-',FORMAT('%02d', EXTRACT(MONTH FROM date_time))) = '{process_month_group}'\n",
    "        \"\"\"\n",
    "    detections_df = core_functions.fetch_gbq_data(query=detections_sql, bigquery_client=bigquery_client)\n",
    "    \n",
    "    # Process in parallel\n",
    "    processed_df = process_month_in_parallel(detections_df, shared_data)\n",
    "    print('beginning to write')\n",
    "    \n",
    "    partition_cols = ['year', 'month']\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    core_functions.write_hive_partitioned_parquet(processed_df, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "    core_functions.write_hive_partitioned_parquet(processed_df, n90_bucket_2, n90_output_prefix_2, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket_2}/{n90_output_prefix_2}\")\n",
    "    \n",
    "    core_functions.write_hive_partitioned_parquet_s4(processed_df, s4_bucket, s4_output_prefix, partition_cols, s4_storage_options)\n",
    "    print(f\"Finished writing to {s4_bucket}/{s4_output_prefix}\")\n",
    "    \n",
    "    veil_keys = list(veil_schema_dict.keys())\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    core_functions.write_hive_partitioned_parquet(processed_df[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    \n",
    "    del processed_df\n",
    "    gc.collect()\n",
    "    completed_months.append(process_month_group)\n",
    "    print(f\"Completed {process_month_group} with {len(detections_df)} rows\")\n",
    "    print('------------------------------------------')\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "print('All months completed: ', completed_months)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(needed_encodings_df['encoding_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_cal_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2_df = detections_df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d2_df.loc[d2_df['sfdc_account_id'].isnull()]\n",
    "# encodings_df = needed_encodings_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(core_functions)\n",
    "\n",
    "    #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(core_functions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Filter the rows where the airing datetime falls within the week range\n",
    "# result_df = merged_df[\n",
    "#     (merged_df['date_time'] >= merged_df['bcw_start_date']) &\n",
    "#     (merged_df['date_time'] < merged_df['bcw_end_date'])\n",
    "# ]\n",
    "# # .drop(columns=['week_start', 'week_end'])\n",
    "# len(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f\"'{col}': '{df[col].dtype}', {df[col][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_functions.print_dataframe_parquet_schema(df, 'detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.loc[df['is_barter'].isnull(), 'is_barter'] = False\n",
    "# df['length_in_seconds'] = df['length_in_seconds'].fillna(0).astype(str)\n",
    "\n",
    "# df_clean = core_functions.enforce_schema(df, n90_schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_functions.print_dataframe_python_schema(df, 'detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = core_functions.enforce_schema(df, n90_schema)\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "# print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "# new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "#  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "# change to veil format\n",
    "veil_keys = list(veil_schema.keys())\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "# print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "del df_clean\n",
    "del needed_encodings_df\n",
    "gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_list = [['2022-01-01', '2022-02-01'], ['2022-02-01', '2022-03-01'], ['2022-03-01', '2022-04-01'], ['2022-04-01', '2022-05-01'], ['2022-05-01', '2022-06-01'], ['2022-06-01', '2022-07-01'], ['2022-07-01', '2022-08-01'], ['2022-08-01', '2022-09-01'], ['2022-09-01', '2022-10-01'], ['2022-10-01', '2022-11-01'], ['2022-11-01', '2022-12-01'], ['2022-12-01', '2023-01-01'], ['2023-01-01', '2023-02-01'], ['2023-02-01', '2023-03-01'], ['2023-03-01', '2023-04-01'], ['2023-04-01', '2023-05-01'], ['2023-05-01', '2023-06-01'], ['2023-06-01', '2023-07-01'], ['2023-07-01', '2023-08-01'], ['2023-08-01', '2023-09-01'], ['2023-09-01', '2023-10-01'], ['2023-10-01', '2023-11-01'], ['2023-11-01', '2023-12-01'], ['2023-12-01', '2024-01-01'], ['2024-01-01', '2024-02-01'], ['2024-02-01', '2024-03-01']]\n",
    "\n",
    "\n",
    "for dates in date_list:\n",
    "    new_min = dates[0]\n",
    "    new_max = dates[1]\n",
    "    \n",
    "    archive_sql = f\"\"\"\n",
    "    select occurrence_id , detection_encoding_id as encoding_id, broadcaster_id, detection_timestamp as date_time, cost, tv_show_id, origin, group_occurrence_id, last_updated, last_audit_id, \n",
    "\n",
    "    affiliate, callsign,  dma_id\n",
    "    from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "    where detection_timestamp >= '{new_min}' and detection_timestamp < '{new_max}'\n",
    "    and clone_of is null\n",
    "    \"\"\"\n",
    "\n",
    "    archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "    # archive_df['occurrence_id'] = archive_df['occurrence_id'].astype(int)\n",
    "    archive_df['group_occurrence_id'] = archive_df['encoding_id'].astype(int)\n",
    "    archive_df.dtypes\n",
    "    detections_df = archive_df.copy()\n",
    "    del archive_df\n",
    "    gc.collect()\n",
    "    process_detections(detections_df, encodings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dates in date_list:\n",
    "    new_min = dates[0]\n",
    "    new_max = dates[1]\n",
    "    \n",
    "    archive_sql = f\"\"\"\n",
    "    select occurrence_id , detection_encoding_id as encoding_id, broadcaster_id, detection_timestamp as date_time, cost, tv_show_id, origin, group_occurrence_id, last_updated, last_audit_id, \n",
    "\n",
    "    affiliate, callsign,  dma_id\n",
    "    from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "    where detection_timestamp >= '{new_min}' and detection_timestamp < '{new_max}'\n",
    "    and clone_of is null\n",
    "    \"\"\"\n",
    "\n",
    "    archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "    # archive_df['occurrence_id'] = archive_df['occurrence_id'].astype(int)\n",
    "    archive_df['group_occurrence_id'] = archive_df['encoding_id'].astype(int)\n",
    "    archive_df.dtypes\n",
    "    detections_df = archive_df.copy()\n",
    "    del archive_df\n",
    "    gc.collect()\n",
    "    process_detections(detections_df, encodings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def generate_overlapping_date_sets(start_date, num_sets):\n",
    "    \"\"\"\n",
    "    Generate overlapping date sets.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        num_sets (int): Number of overlapping date sets to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: List of date sets in the format [['YYYY-MM-DD', 'YYYY-MM-DD'], ...].\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    date_sets = []\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        end = start + relativedelta(months=1)\n",
    "        date_sets.append([start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")])\n",
    "        start = end  # Update start to be the end of the previous set\n",
    "\n",
    "    return date_sets\n",
    "\n",
    "start_date = \"2022-01-01\"\n",
    "num_sets = 30  # Number of overlapping date sets to generate\n",
    "date_sets = generate_overlapping_date_sets(start_date, num_sets)\n",
    "\n",
    "print(date_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detections_df2 = detections_df.merge(master_channel_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# # detections_df2['affiliate'].isna()\n",
    "# detections_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detections_df.dtypes\n",
    "\n",
    "detections_df['date_time'] = pd.to_datetime(detections_df['date_time'], utc=True)\n",
    "detections_df['year'] = detections_df['date_time'].dt.year\n",
    "detections_df['month'] = detections_df['date_time'].dt.month\n",
    "detections_df['day'] = detections_df['date_time'].dt.day\n",
    "detections_df['tv_show_id'] = detections_df['tv_show_id'].fillna(-10) \n",
    "detections_df['group_occurrence_id'] = detections_df['group_occurrence_id'].fillna(-6)\n",
    "# dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "detections_df['occurrence_id'] = detections_df['occurrence_id'].astype(float)\n",
    "# # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "# dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "detections_df = detections_df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "# mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "detections_df = detections_df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "detections_df = detections_df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "gc.collect()\n",
    "\n",
    "needed_encodings = detections_df['encoding_id'].unique().tolist()\n",
    "encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "detections_df = detections_df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "len(needed_encodings_df)\n",
    "detections_df = detections_df.sort_values(by=['occurrence_id', 'year', 'month', 'day'])\n",
    "billing_last_updated = pd.Timestamp.utcnow()\n",
    "detections_df['billing_det_last_updated'] = billing_last_updated\n",
    "detections_df['billing_det_last_updated'] = pd.to_datetime(detections_df['billing_det_last_updated'], utc=True)\n",
    "billing_last_audit_id = core_functions.generate_uuid()\n",
    "detections_df['billing_det_last_updated'] = billing_last_audit_id\n",
    "detections_df.head()\n",
    "\n",
    "detections_df_clean = core_functions.enforce_schema(detections_df, n90_schema)\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "# print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "core_functions.write_hive_partitioned_parquet(detections_df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "# new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "#  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "# change to veil format\n",
    "veil_keys = list(veil_schema.keys())\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "core_functions.write_hive_partitioned_parquet(detections_df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "# print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "del detections_df_clean\n",
    "del needed_encodings_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_list = generate_month_list(min_date, max_date)\n",
    "# month_list\n",
    "\n",
    "# start_date = f'{month_list[0]}-01'\n",
    "# end_date = f'{month_list[1]}-01'\n",
    "# detections_sql = f\"\"\"\n",
    "#     with cleanMasterChannels AS (\n",
    "#     SELECT channel_id, affiliate, callsign, monitored_channels, dma_id, safe_cast(broadcaster_id as int64) as broadcaster_id \n",
    "#     FROM `{veil_billing.get('avs_project_id')}.{veil_billing.get('mongo_dataset_id')}.master_channels`\n",
    "\n",
    "#     )\n",
    "#     select * from \n",
    "#     `{veil_billing.get('avs_project_id')}.{veil_billing.get('avs_dataset_id')}.detections` d\n",
    "#     left join \n",
    "#     cleanMasterChannels mc\n",
    "#     using(broadcaster_id)\n",
    "#     WHERE date_time >= '{start_date}' AND date_time < '{end_date}'\n",
    "#     \"\"\"\n",
    "# detections_df = core_functions.fetch_gbq_data(query=detections_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"GOOGLE_APPLICATION_CREDENTIALS:\", os.environ['GOOGLE_APPLICATION_CREDENTIALS'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
