{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import functions.core_functions as core_functions\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe.utils import assert_eq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import json\n",
    "import gc\n",
    "import cudf\n",
    "import os\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "importlib.reload(core_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veil_schema = {\n",
    "    'occurrence_id': 'int64',\n",
    "    'encoding_id': 'int64',\n",
    "    'broadcaster_id': 'int64',\n",
    "    'cost': 'float64',\n",
    "    'tv_show_id': 'int64',\n",
    "    'origin': 'string',\n",
    "    'group_occurrence_id': 'int64',\n",
    "    'last_updated': 'datetime64[ns, UTC]',\n",
    "    'last_audit_id': 'string',\n",
    "    'date_time': 'datetime64[ns, UTC]',\n",
    "    'year': 'int64',\n",
    "    'month': 'int64',\n",
    "    'day': 'int64',\n",
    "    'affiliate': 'string',\n",
    "    'callsign': 'string',\n",
    "    'dma_id': 'string',\n",
    "    'kantar_region': 'string',\n",
    "    'neustar_id': 'int64',\n",
    "    'name': 'string',\n",
    "    'timezone': 'string',\n",
    "    'dma_rank': 'int64',\n",
    "    'id': 'string',\n",
    "    'show_id': 'int64',\n",
    "    'is_barter': 'bool',\n",
    "    'is_cable': 'bool',\n",
    "    'is_network': 'bool',\n",
    "    'is_spot': 'bool',\n",
    "    'is_canadian': 'bool',\n",
    "    'is_hispanic': 'bool',\n",
    "    'is_local_cable': 'bool',\n",
    "    'is_active': 'bool',\n",
    "    'length': 'float64',\n",
    "    'show_name': 'string',\n",
    "    'format_id': 'int64',\n",
    "    'customer_id': 'int64',\n",
    "    'sfdc_account_id': 'string',\n",
    "    'sfdc_advertiser_id': 'string',\n",
    "    'aeis_id': 'int64',\n",
    "    'billing_det_last_updated': 'datetime64[ns, UTC]',\n",
    "}\n",
    "\n",
    "n90_schema = {\n",
    "   'occurrence_id': 'int64',\n",
    "   'encoding_id': 'int64',\n",
    "   'broadcaster_id': 'int64',\n",
    "   'cost': 'float64',\n",
    "   'tv_show_id': 'int64',\n",
    "   'origin': 'string',\n",
    "   'group_occurrence_id': 'int64',\n",
    "   'last_updated': 'datetime64[ns, UTC]',\n",
    "   'last_audit_id': 'string',\n",
    "   'date_time': 'datetime64[ns, UTC]',\n",
    "   'year': 'int64',\n",
    "   'month': 'int64',\n",
    "   'day': 'int64',\n",
    "   'affiliate': 'string',\n",
    "   'callsign': 'string',\n",
    "   'dma_id': 'string',\n",
    "   'kantar_region': 'string',\n",
    "   'neustar_id': 'int64',\n",
    "   'name': 'string',\n",
    "   'timezone': 'string',\n",
    "   'dma_rank': 'int64',\n",
    "   'id': 'string',\n",
    "   'show_id': 'int64',\n",
    "   'is_barter': 'bool',\n",
    "   'is_cable': 'bool',\n",
    "   'is_network': 'bool',\n",
    "   'is_spot': 'bool',\n",
    "   'is_canadian': 'bool',\n",
    "   'is_hispanic': 'bool',\n",
    "   'is_local_cable': 'bool',\n",
    "   'is_active': 'bool',\n",
    "   'length': 'float64',\n",
    "   'show_name': 'string',\n",
    "   'geo_location': 'string',\n",
    "   'geo_type': 'string',\n",
    "   'geo_latitude': 'float64',\n",
    "   'geo_longitude': 'float64',\n",
    "   'geo_city': 'string',\n",
    "   'geo_state': 'string',\n",
    "   'geo_country': 'string',\n",
    "   'geo_neustar_id': 'string',\n",
    "   'geo_us_msa_id': 'string',\n",
    "   'geo_us_county_fips_id': 'string',\n",
    "   'geo_ca_cma_id': 'string',\n",
    "   'geo_gm_zip_code': 'string',\n",
    "   'format_id': 'int64',\n",
    "   'encoder_group_id': 'int64',\n",
    "   'encoded_timestamp': 'datetime64[ns, UTC]',\n",
    "   'clone_of': 'int64',\n",
    "   'status': 'string',\n",
    "   'last_updated_encoding': 'datetime64[ns, UTC]',\n",
    "   'last_audit_id_encoding': 'string',\n",
    "   'encoder_id': 'int64',\n",
    "   'detection_end_date': 'datetime64[ns, UTC]',\n",
    "   'encoded_timestamp_epoch': 'int64',\n",
    "   'attributes_advertiser': 'string',\n",
    "   'attributes_audience': 'string',\n",
    "   'attributes_audience_2': 'string',\n",
    "   'attributes_cable_estimate': 'string',\n",
    "   'attributes_campaign': 'string',\n",
    "   'attributes_category': 'string',\n",
    "   'attributes_client_code': 'string',\n",
    "   'attributes_commercial_id': 'string',\n",
    "   'attributes_contour_id': 'string',\n",
    "   'attributes_creative_offer': 'string',\n",
    "   'attributes_description': 'string',\n",
    "   'attributes_donovan_agency_advertiser_code': 'string',\n",
    "   'attributes_donovan_agency_estimate_code': 'string',\n",
    "   'attributes_donovan_agency_product_code': 'string',\n",
    "   'attributes_eid': 'string',\n",
    "   'attributes_group': 'string',\n",
    "   'attributes_hd_sd': 'string',\n",
    "   'attributes_id': 'string',\n",
    "   'attributes_isci': 'string',\n",
    "   'length_in_seconds': 'string',\n",
    "   'attributes_length': 'string',\n",
    "   'attributes_lob': 'string',\n",
    "   'attributes_media_type': 'string',\n",
    "   'attributes_message': 'string',\n",
    "   'attributes_misc': 'string',\n",
    "   'attributes_module_code': 'string',\n",
    "   'attributes_offer': 'string',\n",
    "   'attributes_offer_2': 'string',\n",
    "   'attributes_phone_number': 'string',\n",
    "   'attributes_product_code': 'string',\n",
    "   'attributes_product_name': 'string',\n",
    "   'attributes_project_name': 'string',\n",
    "   'attributes_quality': 'string',\n",
    "   'attributes_revision': 'string',\n",
    "   'attributes_show_name': 'string',\n",
    "   'attributes_slug': 'string',\n",
    "   'attributes_sport_id': 'string',\n",
    "   'attributes_sport_show_sub_category': 'string',\n",
    "   'attributes_spot_estimate': 'string',\n",
    "   'attributes_spot_name': 'string',\n",
    "   'attributes_tag': 'string',\n",
    "   'attributes_text': 'string',\n",
    "   'attributes_title': 'string',\n",
    "   'attributes_veil_id': 'string',\n",
    "   'attributes_version_name': 'string',\n",
    "   'attributes_year': 'string',\n",
    "   'product_code': 'string',\n",
    "   'isci': 'string',\n",
    "   'advertiser': 'string',\n",
    "   'encoder_group_name': 'string',\n",
    "   'encoder_group__deleted': 'bool',\n",
    "   'encoder_group__last_audit_id': 'string',\n",
    "   'encoder_group__last_updated': 'datetime64[ns, UTC]',\n",
    "   'aeis_id': 'int64',\n",
    "   'aeis__encoding_id': 'int64',\n",
    "   'aeis__encoding_offset': 'string',\n",
    "   'aeis__last_updated': 'datetime64[ns, UTC]',\n",
    "   'aeis__last_audit_id': 'string',\n",
    "   'format_name': 'string',\n",
    "   'format__profile_id': 'int64',\n",
    "   'format__customer_id': 'int64',\n",
    "   'format__report_breakup': 'string',\n",
    "   'format__deleted': 'bool',\n",
    "   'format__last_updated': 'datetime64[ns, UTC]',\n",
    "   'format__last_audit_id': 'string',\n",
    "   'sfdc_account_name': 'string',\n",
    "   'customer_id': 'int64',\n",
    "   'account_id': 'string',\n",
    "   'contract_item': 'string',\n",
    "   'customer_name': 'string',\n",
    "   'contract_number': 'string',\n",
    "   'sales_person_code': 'string',\n",
    "   'deleted': 'bool',\n",
    "   'profile_id': 'int64',\n",
    "   'profile_name': 'string',\n",
    "   'profile__deleted': 'bool',\n",
    "   'profile__default_asset_code': 'string',\n",
    "   'profile__last_updated': 'datetime64[ns, UTC]',\n",
    "   'profile__last_audit_id': 'string',\n",
    "   'ad_prod_campaign': 'string',\n",
    "   'campaign': 'string',\n",
    "   'sfdc_advertiser_id': 'string',\n",
    "   'billing_last_updated': 'datetime64[ns, UTC]',\n",
    "   'billing_last_audit_id': 'string',\n",
    "   'sfdc_account_id': 'string',\n",
    "   'billing_det_last_updated': 'datetime64[ns, UTC]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "resp = core_functions.initialize_clients(service_account_secret_name='SA_ADHOC_BILLING')\n",
    "resp2 = core_functions.initialize_clients(service_account_secret_name='SA_N90_CORE_APPS')\n",
    "\n",
    "config = resp.get('config')\n",
    "bigquery_client = resp.get('clients').get('bigquery_client')\n",
    "n90_bigquery_client = resp2.get('clients').get('bigquery_client')\n",
    "storage_client = resp.get('clients').get('storage_client')\n",
    "sf_client = resp.get('clients').get('sf_client')\n",
    "veil_billing = resp.get('config').get('veil_billing')\n",
    "veil_vars = resp.get('config').get('veil_billing').get('vars')\n",
    "# print(veil_billing)\n",
    "sfdc_adv_account_cols = veil_billing.get('vars').get('sfdc_adv_account_cols')\n",
    "sfdc_rate_card_cols = veil_billing.get('vars').get('sfdc_rate_card_cols')\n",
    "unknown_dma_overrides = config.get('national_dma_overrides_to_us_national')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "mongo_tables = ['dmas', 'shows']\n",
    "mongo_data = core_functions.fetch_table_data(\n",
    "    project_id=veil_billing.get('avs_project_id'),\n",
    "    dataset_id='mongo',\n",
    "    table_names=mongo_tables,\n",
    "    bigquery_client=bigquery_client\n",
    ")\n",
    "dmas_df = mongo_data['dmas']\n",
    "shows_df = mongo_data['shows']\n",
    "master_channel_sql = f\"\"\"\n",
    "    SELECT * from `adhoc-billing.avs_billing_process.master_channels_expanded`\n",
    "\"\"\"\n",
    "master_channel_df = core_functions.fetch_gbq_data(master_channel_sql, bigquery_client)\n",
    "dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "mask = dmas_df['neustar_id'].isin(unknown_dma_overrides)\n",
    "dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "dmas_df['dma_rank'] = dmas_df['dma_rank'].fillna(0).astype(int)\n",
    "shows_df['show_id'] = shows_df['show_id'].fillna(-6).astype(int)\n",
    "master_channel_df['broadcaster_id'] = master_channel_df['broadcaster_id'].fillna(-10).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "geo_sql = f\"\"\"\n",
    "    SELECT * from `next90-core-applications.next90_analytics.geos` WHERE geo_type = 'dma'\n",
    "\"\"\"\n",
    "geo_df = core_functions.fetch_gbq_data(query=geo_sql, bigquery_client=n90_bigquery_client)\n",
    "# n90_bigquery_client\n",
    "int_cols = ['geo_location', 'geo_neustar_id','geo_us_msa_id', 'geo_us_county_fips_id','geo_ca_cma_id']\n",
    "for col in int_cols:\n",
    "    geo_df[col] = geo_df[col].fillna(-1).astype(int)\n",
    "float_cols = ['geo_latitude', 'geo_longitude']\n",
    "for col in float_cols:\n",
    "    geo_df[col] = geo_df[col].fillna(0.0).astype(float)\n",
    "    \n",
    "string_cols = ['geo_type', 'geo_city', 'geo_state', 'geo_country', 'geo_gm_zip_code']\n",
    "for col in string_cols:\n",
    "    geo_df[col] = geo_df[col].fillna('').replace('nan', '').astype(str)\n",
    "# geo_df['geo_gm_zip_code'] = geo_df['geo_gm_zip_code'].fillna('').replace('nan', '').astype(str)\n",
    "# geo_df['geo_location']\n",
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# min_date_sql = f\"\"\"\n",
    "#     SELECT cast(cast(MIN(date_time) as date) as string) as min_date, cast(cast(MAX(date_time) as date) as string) as max_date FROM `{veil_billing.get('avs_project_id')}.{veil_billing.get('avs_dataset_id')}.detections`\n",
    "#     \"\"\"\n",
    "# min_max = core_functions.fetch_gbq_data(query=min_date_sql, bigquery_client=bigquery_client)\n",
    "# min_date = min_max['min_date'][0]\n",
    "# max_date = min_max['max_date'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# def generate_month_list(start_date, end_date):\n",
    "#     \"\"\"\n",
    "#     Generate a list of months in 'YYYY-MM' format from start_date to end_date (inclusive).\n",
    "    \n",
    "#     Args:\n",
    "#         start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "#         end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of months in 'YYYY-MM' format.\n",
    "#     \"\"\"\n",
    "#     start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "#     end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "#     if start > end:\n",
    "#         raise ValueError(\"start_date must be before or equal to end_date\")\n",
    "    \n",
    "#     month_list = []\n",
    "#     current = start\n",
    "#     while current <= end:\n",
    "#         month_list.append(current.strftime(\"%Y-%m\"))\n",
    "#         current += relativedelta(months=1)\n",
    "    \n",
    "#     return month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_tables = ['encodings']\n",
    "billing_data = core_functions.fetch_table_data(\n",
    "    project_id=veil_billing.get('billing_project_id'),\n",
    "    dataset_id=veil_billing.get('billing_dataset_id'),\n",
    "    table_names=billing_tables,\n",
    "    bigquery_client=bigquery_client\n",
    ")\n",
    "\n",
    "# Access specific DataFrames\n",
    "encodings_df = billing_data['encodings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# geo_sql = f\"\"\"\n",
    "#     SELECT * from `next90-core-applications.next90_analytics.geos` WHERE geo_type = 'dma'\n",
    "# \"\"\"\n",
    "# geo_df = core_functions.fetch_gbq_data(query=geo_sql, bigquery_client=n90_bigquery_client)\n",
    "# # n90_bigquery_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# detections_df['date_time'] = pd.to_datetime(detections_df['date_time'], utc=True)\n",
    "# detections_df['year'] = detections_df['date_time'].dt.year\n",
    "# detections_df['month'] = detections_df['date_time'].dt.month\n",
    "# detections_df['day'] = detections_df['date_time'].dt.day\n",
    "# detections_df['tv_show_id'].fillna(-10, inplace=True) \n",
    "# detections_df['group_occurrence_id'].fillna(-6, inplace=True)\n",
    "# dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "# detections_df['occurrence_id'] = detections_df['occurrence_id'].astype(float)\n",
    "# # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "# dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "# detections_df = detections_df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "# shows_df['show_id'].fillna(-5, inplace=True)\n",
    "# mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "# detections_df = detections_df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "# geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "# detections_df = detections_df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "# gc.collect()\n",
    "# # df_1 = df_1.merge(encodings_df[['encoding_id', 'format_id', 'customer_id', 'sfdc_account_id', 'sfdc_advertiser_id', 'aeis_id']], how='left', left_on='encoding_id', right_on='encoding_id')\n",
    "# detections_df = detections_df.merge(encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "# gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1_date_cols = ['date_time', 'last_updated','billing_det_last_updated']\n",
    "# df_1_int_cols = ['occurrence_id', 'encoding_id', 'broadcaster_id', 'tv_show_id', 'group_occurrence_id', 'year', 'month', 'day', 'neustar_id', 'dma_rank', 'show_id', 'format_id', 'customer_id', 'aeis_id']\n",
    "# df_1_float_cols = ['cost', 'length']\n",
    "# df_1_bool_cols = ['is_barter', 'is_cable', 'is_network', 'is_spot', 'is_canadian', 'is_hispanic', 'is_local_cable', 'is_active']\n",
    "\n",
    "\n",
    "# df_2_date_cols = ['date_time', 'last_updated','billing_det_last_updated', 'encoded_timestamp','last_updated_encoding','detection_end_date','encoder_group__last_updated','aeis__last_updated',\n",
    "#              'format__last_updated','profile__last_updated','billing_last_updated']\n",
    "# df_2_int_cols = ['occurrence_id', 'encoding_id', 'broadcaster_id', 'tv_show_id', 'group_occurrence_id', 'year', 'month', 'day', 'neustar_id', 'dma_rank', 'show_id', 'format_id', 'customer_id', 'aeis_id',\n",
    "#             'encoder_group_id', 'clone_of', 'encoder_id','encoded_timestamp_epoch','aeis__encoding_id','format__profile_id','format__customer_id',\n",
    "#             'profile_id']\n",
    "# df_2_float_cols = ['cost', 'length', 'geo_latitude', 'geo_longitude']\n",
    "# df_2_bool_cols = ['is_barter', 'is_cable', 'is_network', 'is_spot', 'is_canadian', 'is_hispanic', 'is_local_cable', 'is_active','encoder_group__deleted',\n",
    "#              'format__deleted','deleted','profile__deleted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_2.columns:\n",
    "#     print(f\"'{col}': '{df_2[col].dtype}',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enforce schema\n",
    "\n",
    "# def enforce_schema(df, schema):\n",
    "#     for column, dtype in schema.items():\n",
    "#         if dtype == 'string':\n",
    "#             df[column] = df[column].astype(str).fillna('')\n",
    "#         elif dtype == 'int64':\n",
    "#             df[column] = pd.to_numeric(df[column], errors='coerce').fillna(-1).astype('int64')\n",
    "#         elif dtype == 'float64':\n",
    "#             df[column] = pd.to_numeric(df[column], errors='coerce').fillna(-1.0).astype('float64')\n",
    "#         elif dtype == 'datetime64[ns, UTC]':\n",
    "#             df[column] = pd.to_datetime(df[column], errors='coerce', utc=True).fillna(pd.Timestamp.min.tz_localize('UTC'))\n",
    "#     return df\n",
    "\n",
    "# df_1 = enforce_schema(df_1, df_1_schema)\n",
    "# df_2 = enforce_schema(df_2, df_2_schema)\n",
    "# # for column, dtype in df_1_schema.items():\n",
    "# #     if dtype == 'string':\n",
    "# #         df_1[column] = df_1[column].astype(str).fillna('')\n",
    "# #     elif dtype == 'int64':\n",
    "# #         df_1[column] = pd.to_numeric(df_1[column], errors='coerce').fillna(-1).astype('int64')\n",
    "# #     elif dtype == 'float64':\n",
    "# #         df_1[column] = pd.to_numeric(df_1[column], errors='coerce').fillna(-1.0).astype('float64')\n",
    "# #     elif dtype == 'datetime64[ns, UTC]':\n",
    "# #         df_1[column] = pd.to_datetime(df_1[column], errors='coerce', utc=True).fillna(pd.Timestamp.min.tz_localize('UTC'))\n",
    "\n",
    "# # Validate column types\n",
    "# print(df_1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veil_storage_options = None\n",
    "veil_storage_options = config.get('VEIL_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "# n90_storage_options = None\n",
    "n90_storage_options = config.get('N90_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "veil_billing_bucket = None\n",
    "veil_billing_bucket = config.get('veil_billing').get('billing_gcs_bucket_id')\n",
    "\n",
    "# process_df['profile__attributes']\n",
    "n90_bucket = None\n",
    "n90_bucket = 'n90_veil_partner'\n",
    "veil_output_prefix = None\n",
    "veil_output_prefix = 'detections'\n",
    "n90_output_prefix = None\n",
    "n90_output_prefix = 'advocado-looker/avs_prod/detections'\n",
    "partition_cols = None\n",
    "partition_cols = ['year', 'month', 'day']\n",
    "importlib.reload(core_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Monthly Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_detections(df):\n",
    "    df.dtypes\n",
    "\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], utc=True)\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['tv_show_id'] = df['tv_show_id'].fillna(-10) \n",
    "    df['group_occurrence_id'] = df['group_occurrence_id'].fillna(-6)\n",
    "    # dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "    df['occurrence_id'] = df['occurrence_id'].astype(float)\n",
    "    # # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "    # dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "    # dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "    df = df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "    shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "    # mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "    df = df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "    geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "    df = df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "    gc.collect()\n",
    "\n",
    "    needed_encodings = df['encoding_id'].unique().tolist()\n",
    "    encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "    encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "    needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "    df = df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "    len(needed_encodings_df)\n",
    "    df = df.sort_values(by=['occurrence_id', 'year', 'month', 'day'])\n",
    "    billing_last_updated = pd.Timestamp.utcnow()\n",
    "    df['billing_det_last_updated'] = billing_last_updated\n",
    "    df['billing_det_last_updated'] = pd.to_datetime(df['billing_det_last_updated'], utc=True)\n",
    "    billing_last_audit_id = core_functions.generate_uuid()\n",
    "    df['billing_det_last_updated'] = billing_last_audit_id\n",
    "    df.head()\n",
    "\n",
    "    df_clean = core_functions.enforce_schema(df, n90_schema)\n",
    "\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    # core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    # print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    core_functions.write_hive_partitioned_parquet(df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "    # new cell\n",
    "    partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "    # new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "    #  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "    # change to veil format\n",
    "    veil_keys = list(veil_schema.keys())\n",
    "\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "    core_functions.write_hive_partitioned_parquet(df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "    print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "    # core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "    # print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "    # new cell\n",
    "    partition_cols\n",
    "    # print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "    del df_clean\n",
    "    del needed_encodings_df\n",
    "    gc.collect()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "min_date = '2024-12-01'\n",
    "max_date = '2025-01-01'\n",
    "dates_sql = f\"\"\"\n",
    "    SELECT  cast(MAX(date_time) as string) as min_date , cast(current_date() as string) as max_date\n",
    "    FROM `adhoc-billing.avs_billing_process.detections`\n",
    "    \"\"\"\n",
    "dates_df = core_functions.fetch_gbq_data(query=dates_sql, bigquery_client=bigquery_client)\n",
    "min_date = dates_df['min_date'][0]\n",
    "max_date = dates_df['max_date'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df\n",
    "# min_date = dates_df['min_date'][0]\n",
    "min_date = '2024-12-01'\n",
    "max_date = dates_df['max_date'][0]\n",
    "min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    del detections_df\n",
    "except:\n",
    "    pass\n",
    "gc.collect()\n",
    "detections_sql = f\"\"\"\n",
    "    select * from \n",
    "    `{veil_billing.get('avs_project_id')}.{veil_billing.get('avs_dataset_id')}.detections` d\n",
    "\n",
    "    WHERE date_time > '{min_date}' AND date_time < '{max_date}'\n",
    "    \"\"\"\n",
    "detections_df = core_functions.fetch_gbq_data(query=detections_sql, bigquery_client=bigquery_client)\n",
    "detections_df['occurrence_id'] = detections_df['occurrence_id'].astype(float)\n",
    "\n",
    "min_occurrence_id = detections_df['occurrence_id'].min()\n",
    "max_occurrence_id = detections_df['occurrence_id'].max()\n",
    "\n",
    "archive_sql = f\"\"\"\n",
    "select occurrence_id , any_value(affiliate) as affiliate, any_value(callsign) as callsign, any_value(dma_id) as dma_id\n",
    "from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "where occurrence_id >= {min_occurrence_id} and occurrence_id <= {max_occurrence_id}\n",
    "group by occurrence_id\n",
    "\"\"\"\n",
    "\n",
    "# archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "print(len(detections_df))\n",
    "# print(len(archive_df))\n",
    "detection2 = detections_df.merge(master_channel_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "\n",
    "# detection2.loc[detection2['affiliate'].isnull(), 'affiliate'] \n",
    "\n",
    "detections_df = detection2.copy()\n",
    "del detection2\n",
    "gc.collect()\n",
    "\n",
    "# prod is good from april 2024 on the 8th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.head()\n",
    "# detections_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(core_functions)\n",
    "\n",
    "# # # def process_detections(df, encodings_df):\n",
    "df = detections_df.copy().reset_index(drop=True)\n",
    "df = df.sort_values(by=['occurrence_id', 'date_time'], ascending=[True, False])\n",
    "df = df.drop_duplicates(subset=['occurrence_id'], keep='first')\n",
    "df = df.reset_index(drop=True)\n",
    "# # # def process_detections(df):\n",
    "# df.head()\n",
    "# # for col in df.columns:\n",
    "# #     print(f\"'{col}': '{df[col].dtype}',\")\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], utc=True)\n",
    "df['year'] = df['date_time'].dt.year\n",
    "df['month'] = df['date_time'].dt.month\n",
    "df['day'] = df['date_time'].dt.day\n",
    "df['tv_show_id'] = df['tv_show_id'].fillna(-10) \n",
    "df['group_occurrence_id'] = df['group_occurrence_id'].fillna(-6)\n",
    "# dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "df['occurrence_id'] = df['occurrence_id'].astype(float)\n",
    "# # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "# dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "df = df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "# mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "df = df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "df = df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "gc.collect()\n",
    "\n",
    "needed_encodings = df['encoding_id'].unique().tolist()\n",
    "encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "df = df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "len(needed_encodings_df)\n",
    "df = df.sort_values(by=['occurrence_id', 'year', 'month', 'day'])\n",
    "billing_last_updated = pd.Timestamp.utcnow()\n",
    "df['billing_det_last_updated'] = billing_last_updated\n",
    "df['billing_det_last_updated'] = pd.to_datetime(df['billing_det_last_updated'], utc=True)\n",
    "billing_last_audit_id = core_functions.generate_uuid()\n",
    "df['billing_det_last_updated'] = billing_last_audit_id\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.loc[df['is_barter'].isnull(), 'is_barter'] = False\n",
    "# df['length_in_seconds'] = df['length_in_seconds'].fillna(0).astype(str)\n",
    "\n",
    "df_clean = core_functions.enforce_schema(df, n90_schema)\n",
    "\n",
    "# df.loc[df['last_updated_encoding'].isnull(),'last_updated_encoding']\n",
    "# for col, expected_type in n90_schema.items():\n",
    "#     if col in df.columns:\n",
    "#         print(f\"{col}: DataFrame type = {df[col].dtype}, Expected type = {expected_type}\")\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "# print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "# new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "#  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "# change to veil format\n",
    "veil_keys = list(veil_schema.keys())\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "# print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "del df_clean\n",
    "del needed_encodings_df\n",
    "gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = core_functions.enforce_schema(df, n90_schema)\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "# print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "# new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "#  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "# change to veil format\n",
    "veil_keys = list(veil_schema.keys())\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "core_functions.write_hive_partitioned_parquet(df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "# print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "del df_clean\n",
    "del needed_encodings_df\n",
    "gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_list = [['2022-01-01', '2022-02-01'], ['2022-02-01', '2022-03-01'], ['2022-03-01', '2022-04-01'], ['2022-04-01', '2022-05-01'], ['2022-05-01', '2022-06-01'], ['2022-06-01', '2022-07-01'], ['2022-07-01', '2022-08-01'], ['2022-08-01', '2022-09-01'], ['2022-09-01', '2022-10-01'], ['2022-10-01', '2022-11-01'], ['2022-11-01', '2022-12-01'], ['2022-12-01', '2023-01-01'], ['2023-01-01', '2023-02-01'], ['2023-02-01', '2023-03-01'], ['2023-03-01', '2023-04-01'], ['2023-04-01', '2023-05-01'], ['2023-05-01', '2023-06-01'], ['2023-06-01', '2023-07-01'], ['2023-07-01', '2023-08-01'], ['2023-08-01', '2023-09-01'], ['2023-09-01', '2023-10-01'], ['2023-10-01', '2023-11-01'], ['2023-11-01', '2023-12-01'], ['2023-12-01', '2024-01-01'], ['2024-01-01', '2024-02-01'], ['2024-02-01', '2024-03-01']]\n",
    "\n",
    "\n",
    "for dates in date_list:\n",
    "    new_min = dates[0]\n",
    "    new_max = dates[1]\n",
    "    \n",
    "    archive_sql = f\"\"\"\n",
    "    select occurrence_id , detection_encoding_id as encoding_id, broadcaster_id, detection_timestamp as date_time, cost, tv_show_id, origin, group_occurrence_id, last_updated, last_audit_id, \n",
    "\n",
    "    affiliate, callsign,  dma_id\n",
    "    from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "    where detection_timestamp >= '{new_min}' and detection_timestamp < '{new_max}'\n",
    "    and clone_of is null\n",
    "    \"\"\"\n",
    "\n",
    "    archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "    # archive_df['occurrence_id'] = archive_df['occurrence_id'].astype(int)\n",
    "    archive_df['group_occurrence_id'] = archive_df['encoding_id'].astype(int)\n",
    "    archive_df.dtypes\n",
    "    detections_df = archive_df.copy()\n",
    "    del archive_df\n",
    "    gc.collect()\n",
    "    process_detections(detections_df, encodings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dates in date_list:\n",
    "    new_min = dates[0]\n",
    "    new_max = dates[1]\n",
    "    \n",
    "    archive_sql = f\"\"\"\n",
    "    select occurrence_id , detection_encoding_id as encoding_id, broadcaster_id, detection_timestamp as date_time, cost, tv_show_id, origin, group_occurrence_id, last_updated, last_audit_id, \n",
    "\n",
    "    affiliate, callsign,  dma_id\n",
    "    from `adhoc-billing.avs_billing_process.billing_records_archive`\n",
    "    where detection_timestamp >= '{new_min}' and detection_timestamp < '{new_max}'\n",
    "    and clone_of is null\n",
    "    \"\"\"\n",
    "\n",
    "    archive_df = core_functions.fetch_gbq_data(query=archive_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "    # archive_df['occurrence_id'] = archive_df['occurrence_id'].astype(int)\n",
    "    archive_df['group_occurrence_id'] = archive_df['encoding_id'].astype(int)\n",
    "    archive_df.dtypes\n",
    "    detections_df = archive_df.copy()\n",
    "    del archive_df\n",
    "    gc.collect()\n",
    "    process_detections(detections_df, encodings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def generate_overlapping_date_sets(start_date, num_sets):\n",
    "    \"\"\"\n",
    "    Generate overlapping date sets.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        num_sets (int): Number of overlapping date sets to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: List of date sets in the format [['YYYY-MM-DD', 'YYYY-MM-DD'], ...].\n",
    "    \"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    date_sets = []\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        end = start + relativedelta(months=1)\n",
    "        date_sets.append([start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")])\n",
    "        start = end  # Update start to be the end of the previous set\n",
    "\n",
    "    return date_sets\n",
    "\n",
    "start_date = \"2022-01-01\"\n",
    "num_sets = 30  # Number of overlapping date sets to generate\n",
    "date_sets = generate_overlapping_date_sets(start_date, num_sets)\n",
    "\n",
    "print(date_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detections_df2 = detections_df.merge(master_channel_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# # detections_df2['affiliate'].isna()\n",
    "# detections_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detections_df.dtypes\n",
    "\n",
    "detections_df['date_time'] = pd.to_datetime(detections_df['date_time'], utc=True)\n",
    "detections_df['year'] = detections_df['date_time'].dt.year\n",
    "detections_df['month'] = detections_df['date_time'].dt.month\n",
    "detections_df['day'] = detections_df['date_time'].dt.day\n",
    "detections_df['tv_show_id'] = detections_df['tv_show_id'].fillna(-10) \n",
    "detections_df['group_occurrence_id'] = detections_df['group_occurrence_id'].fillna(-6)\n",
    "# dmas_df['dma_rank'].replace('', 0).fillna(0, inplace=True)\n",
    "\n",
    "detections_df['occurrence_id'] = detections_df['occurrence_id'].astype(float)\n",
    "# # detections_df = detections_df.merge(master_channels_df, how='left', left_on='broadcaster_id', right_on='broadcaster_id')\n",
    "# dmas_df.loc[mask, 'neustar_id'] = 808080\n",
    "# dmas_df['neustar_id'] = dmas_df['neustar_id'].fillna(808080).astype(int)\n",
    "\n",
    "detections_df = detections_df.merge(dmas_df, how='left', left_on='dma_id', right_on='dma_id')\n",
    "\n",
    "shows_df['show_id'] = shows_df['show_id'].fillna(-5)\n",
    "# mask = ((dmas_df['dma_id'].isin(unknown_dma_overrides)) )\n",
    "detections_df = detections_df.merge(shows_df, how='left', left_on='tv_show_id', right_on='show_id')\n",
    "\n",
    "\n",
    "geo_df['geo_location'] = geo_df['geo_location'].astype(int)\n",
    "detections_df = detections_df.merge(geo_df, how='left', left_on='neustar_id', right_on='geo_location')\n",
    "gc.collect()\n",
    "\n",
    "needed_encodings = detections_df['encoding_id'].unique().tolist()\n",
    "encodings_df.sort_values(by=['encoding_id', 'billing_last_updated'], ascending=[True, False], inplace=True)\n",
    "encodings_df = encodings_df.drop_duplicates(subset=['encoding_id'], keep='first')\n",
    "needed_encodings_df = encodings_df[encodings_df['encoding_id'].isin(needed_encodings)].copy()\n",
    "detections_df = detections_df.merge(needed_encodings_df, how='left', left_on='encoding_id', right_on='encoding_id', suffixes=('', '_encoding'))\n",
    "\n",
    "len(needed_encodings_df)\n",
    "detections_df = detections_df.sort_values(by=['occurrence_id', 'year', 'month', 'day'])\n",
    "billing_last_updated = pd.Timestamp.utcnow()\n",
    "detections_df['billing_det_last_updated'] = billing_last_updated\n",
    "detections_df['billing_det_last_updated'] = pd.to_datetime(detections_df['billing_det_last_updated'], utc=True)\n",
    "billing_last_audit_id = core_functions.generate_uuid()\n",
    "detections_df['billing_det_last_updated'] = billing_last_audit_id\n",
    "detections_df.head()\n",
    "\n",
    "detections_df_clean = core_functions.enforce_schema(detections_df, n90_schema)\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "# core_functions.write_hive_partitioned_parquet(df_1, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "# print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "core_functions.write_hive_partitioned_parquet(detections_df_clean, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "# new data starts 4/9/2024 - check if mid-day or midnight \t\n",
    "#  2024-04-08 13:10:23 UTC is the first detection in the new data\n",
    "\n",
    "# change to veil format\n",
    "veil_keys = list(veil_schema.keys())\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "core_functions.write_hive_partitioned_parquet(detections_df_clean[veil_keys], veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# core_functions.write_hive_partitioned_parquet(df_2, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "# print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "# new cell\n",
    "partition_cols\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "del detections_df_clean\n",
    "del needed_encodings_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_list = generate_month_list(min_date, max_date)\n",
    "# month_list\n",
    "\n",
    "# start_date = f'{month_list[0]}-01'\n",
    "# end_date = f'{month_list[1]}-01'\n",
    "# detections_sql = f\"\"\"\n",
    "#     with cleanMasterChannels AS (\n",
    "#     SELECT channel_id, affiliate, callsign, monitored_channels, dma_id, safe_cast(broadcaster_id as int64) as broadcaster_id \n",
    "#     FROM `{veil_billing.get('avs_project_id')}.{veil_billing.get('mongo_dataset_id')}.master_channels`\n",
    "\n",
    "#     )\n",
    "#     select * from \n",
    "#     `{veil_billing.get('avs_project_id')}.{veil_billing.get('avs_dataset_id')}.detections` d\n",
    "#     left join \n",
    "#     cleanMasterChannels mc\n",
    "#     using(broadcaster_id)\n",
    "#     WHERE date_time >= '{start_date}' AND date_time < '{end_date}'\n",
    "#     \"\"\"\n",
    "# detections_df = core_functions.fetch_gbq_data(query=detections_sql, bigquery_client=bigquery_client)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"GOOGLE_APPLICATION_CREDENTIALS:\", os.environ['GOOGLE_APPLICATION_CREDENTIALS'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
