{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "\n",
    "# Connect to Hive Metastore\n",
    "conn = hive.Connection(host='10.11.0.10', port=10000, username='anonymous')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SHOW DATABASES')\n",
    "\n",
    "print(\"Databases:\")\n",
    "for database in cursor.fetchall():\n",
    "    print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"GCS_to_Minio_Transform\")\n",
    "         # Enable Hive support so we can integrate with Hive metastore:\n",
    "         .enableHiveSupport()\n",
    "         # GCS configuration (assuming you've put the GCS connector jar in the classpath)\n",
    "         .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "         .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/developer/keys/gcp-keys/advocado-billing/adhoc-billing-advocado-billing--5e634048a179.json\")\n",
    "         # Minio configuration\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://minio.local-stl.n90.co:9000\")\n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", \"5flC58yYrT3qHV8OA0l7\")\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", \"Y0tDcY4evyUvSqX6NlAoWsD4Rkn2VIdb4uH2lGu2\")\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3://n90-data-lake-stl/veil/encodings_v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# def load_encodings_for_detections(storage_options, bucket_name, hive_table_path, platform='s3', scheme='https', region='bws-stl'):\n",
    "    # Construct the S3 URI\n",
    "scheme='https'\n",
    "region='bws-stl'\n",
    "bucket_name='n90-data-lake-stl'\n",
    "hive_table_path='veil/encodings_v2'\n",
    "access_key='5flC58yYrT3qHV8OA0l7'\n",
    "secret_key='Y0tDcY4evyUvSqX6NlAoWsD4Rkn2VIdb4uH2lGu2'\n",
    "endpoint_override='https://s4.api-access.com:9000'\n",
    "    \n",
    "s3_uri = f\"{bucket_name}/{hive_table_path}/\"\n",
    "print(s3_uri)\n",
    "\n",
    "# Initialize the S3FileSystem with MinIO configuration\n",
    "# access_key = storage_options.get('key')\n",
    "print(access_key)\n",
    "# secret_key = storage_options.get('secret')\n",
    "print(secret_key)\n",
    "# endpoint_override = storage_options.get('client_kwargs').get('endpoint_url')\n",
    "print(endpoint_override)\n",
    "s3_fs = fs.S3FileSystem(\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    endpoint_override=endpoint_override,\n",
    "    scheme=scheme,  # Use 'https' if SSL is enabled\n",
    "    region=region  # MinIO requires a region; can be arbitrary\n",
    ")\n",
    "\n",
    "# Define the partitioning scheme (Hive-style)\n",
    "partitioning = ds.partitioning(\n",
    "    flavor='hive'\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ds.dataset(\n",
    "    s3_uri,\n",
    "    format='parquet',\n",
    "    partitioning=partitioning,\n",
    "    filesystem=s3_fs\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the dataset to a PyArrow Table\n",
    "table = dataset.to_table()\n",
    "table_subset = table.select(['encoding_id', 'isci', 'aeis_id', 'encoded_timestamp', 'format_id', 'format_name', 'customer_id', 'customer_name', 'sfdc_account_id', 'sfdc_account_name', 'sfdc_advertiser_id','attributes_cable_estimate', 'attributes_spot_estimate',\n",
    "                            'encoder_group_id',  'encoder_id', 'encoder_group_name', 'length_in_seconds',\n",
    "                            'billing_last_updated', 'billing_last_audit_id', 'clone_of'])\n",
    "\n",
    "# Optionally, convert to pandas DataFrame\n",
    "df = table_subset.to_pandas()\n",
    "\n",
    "# return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "import pandas as pd\n",
    "\n",
    "minio_endpoint = 'https://s4.api-access.com:9000'  # Replace with your MinIO endpoint\n",
    "minio_access_key = '5flC58yYrT3qHV8OA0l7'\n",
    "minio_secret_key = 'Y0tDcY4evyUvSqX6NlAoWsD4Rkn2VIdb4uH2lGu2'\n",
    "bucket_name = 'n90-data-lake-stl'\n",
    "hive_table_path = 'veil/encodings_v2/'  # Ensure it ends with '/'\n",
    "\n",
    "\n",
    "def load_encodings_for_detections(storage_options, bucket_name, hive_table_path, platform='s3', scheme='https', region='bws-stl'):\n",
    "    # Construct the S3 URI\n",
    "    s3_uri = f\"{bucket_name}/{hive_table_path}/\"\n",
    "\n",
    "    # Initialize the S3FileSystem with MinIO configuration\n",
    "    s3_fs = fs.S3FileSystem(\n",
    "        access_key=storage_options.get('access_key'),\n",
    "        secret_key=storage_options.get('secret_key'),\n",
    "        endpoint_override=storage_options.get('client_kwargs').get('endpoint_url'),\n",
    "        scheme='https',  # Use 'https' if SSL is enabled\n",
    "        region=region  # MinIO requires a region; can be arbitrary\n",
    "    )\n",
    "\n",
    "    # Define the partitioning scheme (Hive-style)\n",
    "    partitioning = ds.partitioning(\n",
    "        flavor='hive'\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = ds.dataset(\n",
    "        s3_uri,\n",
    "        format='parquet',\n",
    "        partitioning=partitioning,\n",
    "        filesystem=s3_fs\n",
    "    )\n",
    "\n",
    "    # Convert the dataset to a PyArrow Table\n",
    "    table = dataset.to_table()\n",
    "    table_subset = table.select(['encoding_id', 'isci', 'aeis_id', 'encoded_timestamp', 'format_id', 'format_name', 'customer_id', 'customer_name', 'sfdc_account_id', 'sfdc_account_name', 'sfdc_advertiser_id','attributes_cable_estimate', 'attributes_spot_estimate',\n",
    "                             'encoder_group_id',  'encoder_id', 'encoder_group_name', 'length_in_seconds',\n",
    "                             'billing_last_updated', 'billing_last_audit_id', 'clone_of'])\n",
    "\n",
    "    # Optionally, convert to pandas DataFrame\n",
    "    df = table_subset.to_pandas()\n",
    "\n",
    "    return df\n",
    "# Construct the S3 URI\n",
    "s3_uri = f\"{bucket_name}/{hive_table_path}/\"\n",
    "\n",
    "# Initialize the S3FileSystem with MinIO configuration\n",
    "s3_fs = fs.S3FileSystem(\n",
    "    access_key=minio_access_key,\n",
    "    secret_key=minio_secret_key,\n",
    "    endpoint_override=minio_endpoint,\n",
    "    scheme='https',  # Use 'https' if SSL is enabled\n",
    "    region='bws-stl'  # MinIO requires a region; can be arbitrary\n",
    ")\n",
    "\n",
    "# Define the partitioning scheme (Hive-style)\n",
    "partitioning = ds.partitioning(\n",
    "    flavor='hive'\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ds.dataset(\n",
    "    s3_uri,\n",
    "    format='parquet',\n",
    "    partitioning=partitioning,\n",
    "    filesystem=s3_fs\n",
    ")\n",
    "\n",
    "# Convert the dataset to a PyArrow Table\n",
    "table = dataset.to_table()\n",
    "table_subset = table.select(['encoding_id', 'isci', 'aeis_id', 'encoded_timestamp', 'format_id', 'format_name', 'customer_id', 'customer_name', 'sfdc_account_id', 'sfdc_account_name', 'sfdc_advertiser_id','attributes_cable_estimate', 'attributes_spot_estimate',\n",
    "                             'encoder_group_id',  'encoder_id', 'encoder_group_name', 'length_in_seconds',\n",
    "                             'billing_last_updated', 'billing_last_audit_id', 'clone_of'])\n",
    "\n",
    "# Optionally, convert to pandas DataFrame\n",
    "df = table_subset.to_pandas()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f'Col Name: {col}')\n",
    "    print(f'Col Type: {df[col].dtype}')\n",
    "    print(f'Col Sample Values: {df[col][1006510]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "file_path='/home/developer/keys/project-keys/colab-settings.yaml'\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Configuration file not found at: {file_path}\")\n",
    "except yaml.YAMLError as e:\n",
    "    raise ValueError(f\"Error parsing YAML configuration file: {e}\")\n",
    "\n",
    "# minio_endpoint = 'https://s4.api-access.com:9000'  # Replace with your MinIO endpoint\n",
    "# minio_access_key = '5flC58yYrT3qHV8OA0l7'\n",
    "# minio_secret_key = 'Y0tDcY4evyUvSqX6NlAoWsD4Rkn2VIdb4uH2lGu2'\n",
    "# bucket_name = 'n90-data-lake-stl'\n",
    "# hive_table_path = 'veil/encodings_v2/'  # Ensure it ends with '/'\n",
    "\n",
    "# resp = {}\n",
    "# resp = core_functions.initialize_clients(service_account_secret_name='SA_ADHOC_BILLING')\n",
    "# resp2 = core_functions.initialize_clients(service_account_secret_name='SA_N90_CORE_APPS')\n",
    "\n",
    "# config = resp.get('config')\n",
    "storage_options = config.get('VEIL_GCS_STORAGE_OPTIONS')\n",
    "storage_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storage_options = config.get('VEIL_GCS_STORAGE_OPTIONS')\n",
    "storage_options\n",
    "bucket_name = config.get('veil_billing').get('billing_gcs_bucket_id')\n",
    "hive_table_path = 'encodings_v2'\n",
    "# bucket_name = 'n90-data-lake-stl'\n",
    "\n",
    "\n",
    "\n",
    "# Construct the GCS URI\n",
    "gcs_uri = f\"{bucket_name}/{hive_table_path}/\"\n",
    "\n",
    "# Initialize the GcsFileSystem with GCS configuration\n",
    "gcs_fs = fs.GcsFileSystem(\n",
    "    access_token=storage_options.get('access_token')  # Provide GCS access token if needed\n",
    ")\n",
    "\n",
    "# Define the partitioning scheme (Hive-style)\n",
    "partitioning = ds.partitioning(\n",
    "    flavor='hive'\n",
    ")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ds.dataset(\n",
    "    gcs_uri,\n",
    "    format='parquet',\n",
    "    partitioning=partitioning,\n",
    "    filesystem=gcs_fs\n",
    ")\n",
    "\n",
    "# Convert the dataset to a PyArrow Table\n",
    "table = dataset.to_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
