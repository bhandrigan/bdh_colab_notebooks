{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "\n",
    "# Connect to Hive Metastore\n",
    "conn = hive.Connection(host='10.11.0.10', port=10000, username='anonymous')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SHOW DATABASES')\n",
    "\n",
    "print(\"Databases:\")\n",
    "for database in cursor.fetchall():\n",
    "    print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"GCS_to_Minio_Transform\")\n",
    "         # Enable Hive support so we can integrate with Hive metastore:\n",
    "         .enableHiveSupport()\n",
    "         # GCS configuration (assuming you've put the GCS connector jar in the classpath)\n",
    "         .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "         .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/developer/keys/gcp-keys/advocado-billing/adhoc-billing-advocado-billing--5e634048a179.json\")\n",
    "         # Minio configuration\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://minio.local-stl.n90.co:9000\")\n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", \"5flC58yYrT3qHV8OA0l7\")\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", \"Y0tDcY4evyUvSqX6NlAoWsD4Rkn2VIdb4uH2lGu2\")\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"gs://my-gcs-bucket/path/to/data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
