{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import functions.core_functions as core_functions\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe.utils import assert_eq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import json\n",
    "import gc\n",
    "import cudf\n",
    "import yaml\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "importlib.reload(core_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "resp = core_functions.initialize_clients()\n",
    "\n",
    "config = resp.get('config')\n",
    "bigquery_client = resp.get('clients').get('bigquery_client')\n",
    "storage_client = resp.get('clients').get('storage_client')\n",
    "sf_client = resp.get('clients').get('sf_client')\n",
    "veil_billing = resp.get('config').get('veil_billing')\n",
    "veil_vars = resp.get('config').get('veil_billing').get('vars')\n",
    "# print(veil_billing)\n",
    "sfdc_adv_account_cols = veil_billing.get('vars').get('sfdc_adv_account_cols')\n",
    "sfdc_rate_card_cols = veil_billing.get('vars').get('sfdc_rate_card_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = yaml.safe_load(open('table-schemas.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_python = table_schema['encodings_python_schema']\n",
    "encodings_parquet = table_schema['encodings_parquet_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_python\n",
    "encodings_python\n",
    "for item in encodings_python:\n",
    "    print(f\"{item['name']} = {item['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload_encodings = True\n",
    "#  Need to refactor code updates to take last_updated into consideration and add a view to bq to only keep one record per encoding_id\n",
    "\n",
    "if reload_encodings:\n",
    "    avs_tables = ['encodings','encoders', 'encoder_groups', 'formats', 'customers', 'profiles', 'aeismaps']\n",
    "else:\n",
    "    avs_tables = ['encoders', 'encoder_groups', 'formats', 'customers', 'profiles', 'aeismaps']\n",
    "avs_data = core_functions.fetch_table_data(\n",
    "    project_id=veil_billing.get('avs_project_id'),\n",
    "    dataset_id=veil_billing.get('avs_dataset_id'),\n",
    "    table_names=avs_tables,\n",
    "    bigquery_client=bigquery_client\n",
    ")\n",
    "\n",
    "# Access specific DataFrames\n",
    "encoding_sql = f\"\"\"\n",
    "    with includedEncodings as (\n",
    "    select distinct encoding_id from `adhoc-billing.avs_billing_process.encodings_complete` \n",
    "\n",
    "    ),\n",
    "    lastUpdated as (\n",
    "    select \n",
    "    max(billing_last_updated) \n",
    "    -- max(TIMESTAMP_seconds(cast((last_updated/1000000000) as int)))\n",
    "    as billing_last_updated\n",
    "    from `adhoc-billing.avs_billing_process.encodings_complete` \n",
    "    ),\n",
    "    newEncodings AS (\n",
    "    select e.* from `bigquery-sandbox-393916.prod_avs.encodings` e\n",
    "    left join includedEncodings ie\n",
    "    using (encoding_id)\n",
    "    where ie.encoding_id is null\n",
    "    )\n",
    "    select * from newEncodings\n",
    "    union all\n",
    "    select e.* from `bigquery-sandbox-393916.prod_avs.encodings` e\n",
    "    where cast(e.last_updated as timestamp) > (select billing_last_updated from lastUpdated)\n",
    "    and encoding_id not in (select encoding_id from newEncodings)\n",
    "\"\"\"\n",
    "if not reload_encodings:\n",
    "    encodings_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(encoding_sql, bigquery_client).dropna(subset=['encoded_timestamp']))\n",
    "# encodings_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(encoding_sql, bigquery_client).dropna(subset=['encoded_timestamp']))\n",
    "if reload_encodings:\n",
    "    encodings_sql = f\"\"\" select * from `adhoc-billing.avs_billing_process.avs_encodings_master` where status = 'encoded'\"\"\"\n",
    "    encodings_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(encodings_sql, bigquery_client))\n",
    "    if 'aeis_id' in encodings_df.columns:\n",
    "        encodings_df.drop(columns=['aeis_id'], inplace=True)\n",
    "\n",
    "encodings_df = (encodings_df.loc[(encodings_df['status'] == 'encoded')]).sort_values(by=['format_id', 'encoding_id'], ascending=[True, True]).copy().reset_index(drop=True)\n",
    "encodings_df['segments_format_id_group'] = encodings_df['format_id'].apply(core_functions.assign_segment_group)\n",
    "encoders_df = core_functions.fix_df_dtypes(avs_data['encoders'])\n",
    "encoder_groups_df = core_functions.fix_df_dtypes(avs_data['encoder_groups'])\n",
    "formats_df = core_functions.fix_df_dtypes(avs_data['formats'])\n",
    "customers_df = core_functions.fix_df_dtypes(avs_data['customers'])\n",
    "profiles_df = core_functions.fix_df_dtypes(avs_data['profiles'])\n",
    "aeismaps_df = core_functions.fix_df_dtypes(avs_data['aeismaps'])\n",
    "\n",
    "# Directly sort the Series, extract unique values, and convert to a list\n",
    "\n",
    "encoding_format_ids = encodings_df['format_id'].sort_values().unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_mask = encodings_df['format_id'] == 13568\n",
    "# encodings_df = encodings_df[test_mask].copy()\n",
    "# encoding_format_ids\n",
    "len(encodings_df)\n",
    "# 1666255\n",
    "# 2070-01-01 05:00:00+00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "billing_tables = [\n",
    "    'sfdc_bvs_customer__c_obj',\n",
    "    'sfdc_bvs_format__c_obj',\n",
    "    'sfdc_account_obj',\n",
    "    'sfdc_advertiser__c_obj',\n",
    "    'sfdc_rate_card__c_obj'\n",
    "]\n",
    "billing_data = core_functions.fetch_table_data(\n",
    "    project_id=veil_billing.get('billing_project_id'),\n",
    "    dataset_id=veil_billing.get('billing_dataset_id'),\n",
    "    table_names=billing_tables,\n",
    "    bigquery_client=bigquery_client\n",
    ")\n",
    "\n",
    "# Access specific DataFrames\n",
    "sfdc_bvs_customer_df = core_functions.fix_df_dtypes(billing_data['sfdc_bvs_customer__c_obj'])\n",
    "sfdc_account_df = core_functions.fix_df_dtypes(billing_data['sfdc_account_obj'])\n",
    "sfdc_advertiser_df = core_functions.fix_df_dtypes(billing_data['sfdc_advertiser__c_obj'])\n",
    "sfdc_rate_card_df = core_functions.fix_df_dtypes(billing_data['sfdc_rate_card__c_obj'])\n",
    "sfdc_bvs_format_df = core_functions.fix_df_dtypes(billing_data['sfdc_bvs_format__c_obj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prepped_encodings_df = core_functions.fix_df_dtypes(core_functions.pre_prep_dataframe(encodings_df))\n",
    "\n",
    "# len(encodings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(prepped_encodings_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_format_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prepped_encodings_df)\n",
    "\n",
    "prepped_encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_encodings_df = prepped_encodings_df.sort_values(['format_id', 'encoding_id']).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_processed_encodings_df = pd.DataFrame()\n",
    "# for format_id in encoding_format_ids:\n",
    "#     if format_id == 0:\n",
    "#         continue\n",
    "#     else:\n",
    "#         mask = processed_encodings_df['format_id'] == format_id\n",
    "#         print(f\"Processing format_id: {format_id}...\")\n",
    "#         format_df = processed_encodings_df.loc[processed_encodings_df['format_id'] == format_id].copy()\n",
    "#         # df = core_functions.clean_encodings_df(format_df)\n",
    "#         df = format_df\n",
    "#         clean_processed_encodings_df = pd.concat([clean_processed_encodings_df, df], ignore_index=True)\n",
    "    \n",
    "\n",
    "\n",
    "# clean_processed_encodings_df = core_functions.clean_encodings_df(processed_encodings_df)\n",
    "# clean_processed_encodings_df\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1000  # Change to 1000 if desired\n",
    "\n",
    "# Initialize an empty DataFrame to store the cleaned results\n",
    "clean_processed_encodings_df = pd.DataFrame()\n",
    "\n",
    "# Define your encoding_format_ids as per your corrected approach\n",
    "encoding_format_ids = encodings_df['format_id'].sort_values().unique().tolist()\n",
    "\n",
    "# Remove format_id = 0 beforehand to avoid checking inside the loop\n",
    "encoding_format_ids = [fid for fid in encoding_format_ids if fid != 0]\n",
    "\n",
    "# Split the format_id list into chunks\n",
    "format_id_batches = chunks(encoding_format_ids, batch_size)\n",
    "\n",
    "# Optionally, wrap the batches with tqdm for a progress bar\n",
    "# for batch_num, format_id_batch in enumerate(tqdm(format_id_batches, desc=\"Processing Batches\"), start=1):\n",
    "for batch_num, format_id_batch in enumerate(format_id_batches, start=1):\n",
    "    print(f\"Processing batch {batch_num} with {len(format_id_batch)} format_ids...\")\n",
    "    \n",
    "    # Create a mask for the current batch of format_ids\n",
    "    mask = processed_encodings_df['format_id'].isin(format_id_batch)\n",
    "    \n",
    "    # Filter the DataFrame for the current batch\n",
    "    format_df = processed_encodings_df.loc[mask].copy()\n",
    "    \n",
    "    # Apply your cleaning function (uncomment and use as needed)\n",
    "    # df = core_functions.clean_encodings_df(format_df)\n",
    "    \n",
    "    # For demonstration, we'll assume no cleaning is needed\n",
    "    df = format_df\n",
    "    \n",
    "    # Concatenate the cleaned batch to the main DataFrame\n",
    "    clean_processed_encodings_df = pd.concat([clean_processed_encodings_df, df], ignore_index=True)\n",
    "\n",
    "# Optional: Reset index if needed\n",
    "clean_processed_encodings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Batch processing completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in encodings_df.columns:\n",
    "    print(f\"{col}: {processed_encodings_df[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions\n",
    "# Ensure 'description' column does not contain None values\n",
    "processed_encodings_df = clean_processed_encodings_df.copy()\n",
    "processed_encodings_df['description'] = processed_encodings_df['attributes_description'].fillna('')\n",
    "processed_encodings_df['product_code'] = processed_encodings_df['attributes_product_code'].fillna('')\n",
    "\n",
    "processed_encodings_df['product_name'] = processed_encodings_df['attributes_product_name'].fillna('')\n",
    "processed_encodings_df['donovan_agency_product_code'] = processed_encodings_df['attributes_donovan_agency_product_code'].fillna('')\n",
    "processed_encodings_df['isci'] = processed_encodings_df['attributes_isci'].fillna('')\n",
    "processed_encodings_df['project_name'] = processed_encodings_df['attributes_project_name'].fillna('')\n",
    "processed_encodings_df['advertiser'] = processed_encodings_df['attributes_advertiser'].fillna('')\n",
    "\n",
    "processed_encodings_df['client_code'] = processed_encodings_df['attributes_client_code'].fillna('')\n",
    "processed_encodings_df['donovan_agency_advertiser_code'] = processed_encodings_df['attributes_donovan_agency_advertiser_code'].fillna('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_processed_encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions\n",
    "conditions = [\n",
    "    processed_encodings_df['product_code'].notnull(),\n",
    "    processed_encodings_df['product_code'].isnull() & processed_encodings_df['product_name'].notnull(),\n",
    "    processed_encodings_df['product_code'].isnull() & processed_encodings_df['product_name'].isnull() & processed_encodings_df['donovan_agency_product_code'].notnull(),\n",
    "    processed_encodings_df['product_code'].isnull() & processed_encodings_df['product_name'].isnull() & processed_encodings_df['donovan_agency_product_code'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & ~processed_encodings_df['description'].str.startswith(('TV', 'RA')),\n",
    "    processed_encodings_df['product_code'].isnull() & processed_encodings_df['product_name'].isnull() & processed_encodings_df['donovan_agency_product_code'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & processed_encodings_df['description'].str.startswith(('TV', 'RA'))\n",
    "]\n",
    "\n",
    "# Define corresponding values\n",
    "choices = [\n",
    "    processed_encodings_df['product_code'],\n",
    "    processed_encodings_df['product_name'],\n",
    "    processed_encodings_df['donovan_agency_product_code'],\n",
    "    processed_encodings_df['description'].str[26:30].str.strip(),\n",
    "    processed_encodings_df['description'].str[6:10].str.strip()\n",
    "]\n",
    "\n",
    "# Apply conditions and choices to create the new column\n",
    "processed_encodings_df['product_code'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "\n",
    "\n",
    "# Define conditions\n",
    "conditions = [\n",
    "    processed_encodings_df['isci'].notnull(),\n",
    "    processed_encodings_df['isci'].isnull() & processed_encodings_df['project_name'].notnull(),\n",
    "    processed_encodings_df['isci'].isnull() & processed_encodings_df['project_name'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & ~processed_encodings_df['description'].str.startswith(('TV', 'RA')),\n",
    "    processed_encodings_df['isci'].isnull() & processed_encodings_df['project_name'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & processed_encodings_df['description'].str.startswith(('TV', 'RA'))\n",
    "]\n",
    "\n",
    "# Define corresponding values\n",
    "choices = [\n",
    "    processed_encodings_df['isci'],\n",
    "    processed_encodings_df['project_name'],\n",
    "    processed_encodings_df['description'].str[8:18].str.strip(),\n",
    "    processed_encodings_df['description'].str[18:38].str.strip()\n",
    "]\n",
    "\n",
    "# Apply conditions and choices to create the new column\n",
    "processed_encodings_df['isci'] = np.select(conditions, choices, default=None)\n",
    "\n",
    "\n",
    "# Define conditions\n",
    "conditions = [\n",
    "    processed_encodings_df['advertiser'].notnull(),\n",
    "    processed_encodings_df['advertiser'].isnull() & processed_encodings_df['client_code'].notnull(),\n",
    "    processed_encodings_df['advertiser'].isnull() & processed_encodings_df['client_code'].isnull() & processed_encodings_df['donovan_agency_advertiser_code'].notnull(),\n",
    "    processed_encodings_df['advertiser'].isnull() & processed_encodings_df['client_code'].isnull() & processed_encodings_df['donovan_agency_advertiser_code'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & ~processed_encodings_df['description'].str.startswith(('TV', 'RA')),\n",
    "    processed_encodings_df['advertiser'].isnull() & processed_encodings_df['client_code'].isnull() & processed_encodings_df['donovan_agency_advertiser_code'].isnull() & processed_encodings_df['description'].notnull() & processed_encodings_df['description'].str.len() > 10 & processed_encodings_df['description'].str.startswith(('TV', 'RA'))\n",
    "]\n",
    "\n",
    "# Define corresponding values\n",
    "choices = [\n",
    "    processed_encodings_df['advertiser'],\n",
    "    processed_encodings_df['client_code'],\n",
    "    processed_encodings_df['donovan_agency_advertiser_code'],\n",
    "    processed_encodings_df['description'].str[22:26].str.strip(),\n",
    "    processed_encodings_df['description'].str[2:6].str.strip()\n",
    "]\n",
    "\n",
    "# Apply conditions and choices to create the new column\n",
    "processed_encodings_df['advertiser'] = np.select(conditions, choices, default=None)\n",
    "processed_encodings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_sfdc_bvs_customer_df = core_functions.clean_sfdc_df(sfdc_bvs_customer_df, id_col='sfdc_bvs_customer_id', name_col='sfdc_bvs_customer_name')\n",
    "clean_sfdc_account_df = core_functions.clean_sfdc_df(sfdc_account_df, id_col='sfdc_account_id', name_col='sfdc_account_name')\n",
    "clean_sfdc_advertiser_df = core_functions.clean_sfdc_df(sfdc_advertiser_df, id_col='sfdc_advertiser_id', name_col='sfdc_advertiser_name')\n",
    "clean_sfdc_rate_card_df = core_functions.clean_sfdc_df(sfdc_rate_card_df, id_col='sfdc_rate_card_id', name_col='rate_card_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_encodings_df['attributes_product_code']\n",
    "processed_encodings_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sfdc_advertiser_df['sfdc_advertiser_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sfdc_account_df\n",
    "\n",
    "clean_sfdc_account_df.sort_values(by='sfdc_account_id', inplace=True)\n",
    "\n",
    "# sfdc_advertiser_df['sfdc_advertiser_id'] = sfdc_advertiser_df['id']\n",
    "# sfdc_advertiser_df['sfdc_advertiser_name'] = sfdc_advertiser_df['name']\n",
    "# sfdc_advertiser_df['sfdc_account_id'] = sfdc_advertiser_df['account']\n",
    "\n",
    "clean_sfdc_advertiser_df.sort_values(by=['sfdc_account_id', 'sfdc_advertiser_id', 'encoding_format_id', 'encoding_advertiser', 'encoding_product_code', 'encoding_module_code'], inplace=True)\n",
    "# # # sfdc_account_df[['sfdc_account_id', 'sfdc_account_name']] \n",
    "clean_sfdc_adv_account_df = clean_sfdc_advertiser_df.merge(clean_sfdc_account_df, how='left', on='sfdc_account_id', suffixes=('_adv', '_acc'))\n",
    "clean_sfdc_adv_account_df = clean_sfdc_adv_account_df[sfdc_adv_account_cols].copy()\n",
    "clean_sfdc_rate_card_df = clean_sfdc_rate_card_df[sfdc_rate_card_cols].copy()\n",
    "clean_sfdc_adv_account_rate_card_df = clean_sfdc_adv_account_df[sfdc_adv_account_cols].merge(clean_sfdc_rate_card_df[sfdc_rate_card_cols], how='left', on='sfdc_rate_card_id', suffixes=('_adv', '_rc')).copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# .columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes_list = ['product_code', 'product_name', 'donovan_agency_product_code', 'description', 'isci', 'project_name', 'advertiser', 'client_code',\n",
    "#                    'cable_estimate', 'spot_estimate', 'campaign', 'audience', 'audience2', 'category', 'comercial_id', 'contour_id', 'creative_offer',\n",
    "#                    'donovan_agency_advertiser_code', 'donovan_agency_estimate_code', 'eid', 'group', 'hd_sd', 'id', 'length', 'lob', 'media_type',\n",
    "#                    'message', 'misc', 'module_code', 'offer', 'offer_2', 'phone_number', 'quality', 'revision', 'show_name', 'slug', 'sport_id',\n",
    "#                    'sport_show_sub_category', 'spot_name', 'tag', 'text', 'title', 'veil_id', 'version_name', 'year']\n",
    "# for attr in attributes_list:\n",
    "#     encodings_df[attr] = encodings_df['attributes'].apply(lambda x: x.get(attr))\n",
    "# encodings_df['product_code'] = encodings_df['attributes'].apply(lambda x: x.get('product_code'))\n",
    "\n",
    "# Define conditions\n",
    "# Ensure 'description' column does not contain None values\n",
    "\n",
    "\n",
    "#  new cell\n",
    "\n",
    "# sfdc_account_df['sfdc_account_id'] = sfdc_account_df['Id']\n",
    "# sfdc_account_df['sfdc_account_name'] = sfdc_account_df['Name']\n",
    "# sfdc_bvs_customer_df['customer_id'] = sfdc_bvs_customer_df['customer_id__c'].astype(int)\n",
    "# sfdc_bvs_customer_df['sfdc_account_id'] = sfdc_bvs_customer_df['Account__c']\n",
    "account_cols = ['sfdc_account_id', 'sfdc_account_name']\n",
    "customer_cols = ['customer_id', 'sfdc_account_id']\n",
    "clean_sfdc_cust_account_df = clean_sfdc_account_df[account_cols].merge(clean_sfdc_bvs_customer_df[customer_cols], on='sfdc_account_id', how='inner', suffixes=('_sfdc_account', '_sfdc_customer'))\n",
    "\n",
    "clean_sfdc_cust_account_df\n",
    "sfdc_bvs_cust_account_df = clean_sfdc_cust_account_df.merge(customers_df, left_on='customer_id', right_on='customer_id', how='inner', suffixes=('_sfdc', '_avs'))\n",
    "\n",
    "# new cell\n",
    "\n",
    "core_functions.rename_columns(encoder_groups_df, 'encoder_group_')\n",
    "core_functions.rename_columns(encoders_df, 'encoder_')\n",
    "core_functions.rename_columns(formats_df, 'format_')\n",
    "core_functions.rename_columns(customers_df, 'customer_')\n",
    "core_functions.rename_columns(profiles_df, 'profile_')\n",
    "core_functions.rename_columns(aeismaps_df, 'aeis_')\n",
    "\n",
    "# clean_processed_encodings_df\n",
    "\n",
    "# encodings_df['encoding_id'] = encodings_df['encoding_id'].astype(int)\n",
    "encoders_groups_df = processed_encodings_df.merge(encoder_groups_df, left_on='encoder_group_id', right_on='encoder_group_id', how='left').drop_duplicates(subset=['encoding_id']).copy()\n",
    "encoders_groups_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoders_groups_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoders_groups_df.drop_duplicates(subset=['encoding_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders_groups_df[encoders_groups_df['encoding_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_encodings_df.head()\n",
    "len(processed_encodings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders_groups_df\n",
    "cols_in_processed_encodings_df = processed_encodings_df.columns.to_list()\n",
    "cols_in_processed_encodings_df = list(set(cols_in_processed_encodings_df) - {'encoding_id'})\n",
    "for col in cols_in_processed_encodings_df:\n",
    "    if col in encoders_groups_df.columns:\n",
    "        encoders_groups_df = encoders_groups_df.drop(col, axis=1)\n",
    "encoders_groups_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoders_groups_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encodings_encoders_df = processed_encodings_df.merge(encoders_groups_df, left_on='encoding_id', right_on='encoding_id', how='left', suffixes=('', '_dupe'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_encoders_df.head()\n",
    "len(encodings_encoders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aeismaps_df.sort_values(by=['aeis__encoding_id', 'aeis_id', ], inplace=True)\n",
    "\n",
    "# processed_aeismaps_df = processed_aeismaps_df.loc[(processed_aeismaps_df['aeis_id'].fillna(0.0).astype(int) > 0) & (aeismaps_df['aeis__encoding_id'].fillna(0.0).astype(int) > 0)]\n",
    "aeismaps_df.drop_duplicates(subset=('aeis__encoding_id' ), inplace=True)\n",
    "aeismaps_df\n",
    "print(len(aeismaps_df))\n",
    "\n",
    "encodings_aeis_df = encodings_encoders_df.merge(aeismaps_df, left_on='encoding_id', right_on='aeis__encoding_id', how='left')\n",
    "encodings_aeis_df\n",
    "\n",
    "# processed_formats_df['format__customer_id'] = formats_df['format__customer_id'].astype(int)\n",
    "# formats_df['format__profile_id'] = formats_df['format__profile_id'].astype(int)\n",
    "# sfdc_bvs_cust_account_df['customer_id'] = sfdc_bvs_cust_account_df['customer_id'].astype(int)\n",
    "formats_customers_df = formats_df.merge(sfdc_bvs_cust_account_df, left_on='format__customer_id', right_on='customer_id', how='left')\n",
    "formats_customers_df\n",
    "\n",
    "formats_customers_profiles_df = formats_customers_df.merge(profiles_df, left_on='format__profile_id', right_on='profile_id', how='left')\n",
    "formats_customers_profiles_df\n",
    "\n",
    "encodings_bvs_df = encodings_aeis_df.merge(formats_customers_profiles_df, on='format_id',  how='left', suffixes=('', '_drop'))\n",
    "for col in encodings_bvs_df.columns:\n",
    "    if col.endswith('_drop'):\n",
    "        encodings_bvs_df.drop(columns=col, inplace=True)\n",
    "encodings_bvs_df\n",
    "\n",
    "encodings_bvs_df['ad_prod_campaign'] = None\n",
    "encodings_bvs_df['advertiser'] = encodings_bvs_df['advertiser'].fillna('')\n",
    "encodings_bvs_df['product_code'] = encodings_bvs_df['product_code'].fillna('')\n",
    "encodings_bvs_df['campaign'] = encodings_bvs_df['attributes_campaign'].fillna('')\n",
    "\n",
    "# Update ad_prod_campaign\n",
    "encodings_bvs_df['ad_prod_campaign'] = encodings_bvs_df.apply(\n",
    "    lambda row: f\"{row['advertiser'].strip()}-{row['product_code'].strip()}-{row['campaign'].strip()}\".replace(' ', '_') if pd.isnull(row['ad_prod_campaign']) else row['ad_prod_campaign'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# new cell\n",
    "\n",
    "encodings_bvs_df.sort_values(by=[ 'encoding_id', 'last_updated','sfdc_account_id', 'format_id', 'profile_id', 'customer_id'], inplace=True)\n",
    "\n",
    "# new cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_df.drop_duplicates(subset=['encoding_id'], keep='last', inplace=True)\n",
    "encodings_bvs_df.head()\n",
    "\n",
    "len(encodings_bvs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_df.sort_values(by=[ 'format_id', 'encoding_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting with {len(encodings_bvs_df)} rows\")\n",
    "accounted_for = 0\n",
    "print(f\"Accounted for {accounted_for} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sfdc_advertiser_df['match_type'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting with {len(encodings_bvs_df)} rows\")\n",
    "accounted_for = 0\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_product_code_multiple'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_product_code_multiple_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_product_code_multiple_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_product_code'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_product_code_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_product_code_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_advertiser'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_advertiser_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_advertiser_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_product_code_ignore_format'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_product_code_ignore_format_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_product_code_ignore_format_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_advertiser_ignore_format'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_advertiser_ignore_format_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_advertiser_ignore_format_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoding_format'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoding_format_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoding_format_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "print(\"finished normal matches\")\n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'encoder_group'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "encoder_group_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(encoder_group_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'] == 'Clone'\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "clone_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(clone_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "\n",
    "\n",
    "\n",
    "mask = clean_sfdc_advertiser_df['match_type'].isna()\n",
    "print(len(clean_sfdc_advertiser_df.loc[mask]))\n",
    "null_match_sfdc_advertiser_df = clean_sfdc_advertiser_df.loc[mask].copy()\n",
    "accounted_for += len(null_match_sfdc_advertiser_df)\n",
    "print(f\"Accounted for {accounted_for} rows\")\n",
    "print(f\"Started with {len(clean_sfdc_advertiser_df)} rows and accounted for {accounted_for} rows\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# array(['encoding_format', 'encoder_group', 'Clone',\n",
    "    #    'encoding_product_code_multiple', None, 'encoding_advertiser',\n",
    "    #    'encoding_product_code', 'encoding_advertiser_ignore_format'],\n",
    "    #   dtype=object)\n",
    "# 14327 total\n",
    "# encoding_format: 14174\n",
    "# encoder_group: 32\n",
    "# Clone: 2\n",
    "# encoding_product_code_multiple: 17\n",
    "# encoding_advertiser: 7\n",
    "# encoding_product_code: 37\n",
    "# encoding_advertiser_ignore_format: 45\n",
    "#  null: 13\n",
    "# print(14327 - 14174 - 32 - 2 - 17 - 7 - 37 - 45 - 13)\n",
    "# clean_sfdc_advertiser_df.loc[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_encoding_product_code_multiple_sfdc_advertiser_df = encodings_bvs_df.merge(encoding_product_code_multiple_sfdc_advertiser_df, left_on='format_id', right_on='encoding_format_id', how='left', suffixes=('', '_encoding_format')).dropna(subset=['sfdc_advertiser_id'])\n",
    "print(f\"Starting with {len(clean_sfdc_advertiser_df)} rows\")\n",
    "print(f\"Accounted for {len(encoding_product_code_multiple_sfdc_advertiser_df)} rows\")\n",
    "# [\"ALDR\", \"ALGM\", \"ALHP\"]\n",
    "processed = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "encoding_product_code_multiple_sfdc_advertiser_df['product_code_list_list'] = encoding_product_code_multiple_sfdc_advertiser_df['product_code_list'].apply(lambda x: x.split(','))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# Filter the main DataFrame to include only relevant sfdc_account_ids\n",
    "meta_df = encoding_product_code_multiple_sfdc_advertiser_df\n",
    "df_name = 'encoding_product_code_multiple_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "    print(f\"Processing {df_name}...\")\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['product_code'].fillna('')\n",
    "    # working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "\n",
    "    # Explode product_code_list_list into multiple rows\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "    encoding_expanded_df = encoding_expanded_df.explode('product_code_list_list')\n",
    "    encoding_expanded_df['product_code_list_list'] = encoding_expanded_df['product_code_list_list'].str.replace('\"', '').copy()\n",
    "    encoding_expanded_df['encoding_format_id'] = encoding_expanded_df['encoding_format_id'].astype('Int64')\n",
    "    encoding_expanded_df[['product_code_list_list', 'encoding_format_id', 'sfdc_advertiser_id']]\n",
    "    # encoding_expanded_master_df = encoding_expanded_df.sort_values(by=['encoding_format_id', 'product_code_list_list'], inplace=True)\n",
    "    encoding_expanded_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df.drop_duplicates(subset=['encoding_format_id', 'product_code_list_list'], keep='first', inplace=True)\n",
    "    # encoding_expanded_master_df['encoding_format_id'].fillna('0', inplace=True)\n",
    "    # encoding_expanded_master_df['product_code_list_list'].fillna('', inplace=True)\n",
    "    # encoding_expanded_master2_df = encoding_expanded_master_df.drop_duplicates(subset=['encoding_format_id', 'product_code_list_list'], keep='first')\n",
    "    # len(encoding_expanded_master2_df)\n",
    "    working_df['format_id'] = working_df['format_id'].astype('Int64')\n",
    "    # Perform the merge based on format_id and product_code\n",
    "    merged_df = working_df.merge(\n",
    "        encoding_expanded_df,\n",
    "        left_on=['format_id', 'product_code'],\n",
    "        right_on=['encoding_format_id', 'product_code_list_list'],\n",
    "        how='left', suffixes=('', '_encoding_format')\n",
    "    )\n",
    "    merged_df = merged_df.dropna(subset=['sfdc_advertiser_id'])\n",
    "    merged_df['encoding_id'] = merged_df['encoding_id'].astype('Int64')\n",
    "    working_df['encoding_id'] = working_df['encoding_id'].astype('Int64')\n",
    "    merged_df[['encoding_id','sfdc_account_id']]\n",
    "    print(f\"Starting with {len(working_df)} rows\")\n",
    "    working_df2 = working_df.merge(merged_df[['encoding_id', 'sfdc_advertiser_id']], on='encoding_id', how='left',)\n",
    "    print(f\"Accounted for {len(working_df2)} rows\")\n",
    "    working_df2 = working_df2.dropna(subset=['sfdc_advertiser_id'])\n",
    "    \n",
    "    # # Assign the advertiser ID to the original DataFrame\n",
    "    # working_df['sfdc_advertiser_id'] = merged_df['sfdc_advertiser_id']\n",
    "    # # working_df.dropna(subset=['sfdc_advertiser_id'])\n",
    "    # working_df\n",
    "    working_df2.columns.tolist()\n",
    "\n",
    "    new_encodings_bvs_df = pd.DataFrame(columns=working_df2.columns)\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, working_df2], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "    processed.append(df_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "meta_df = encoding_product_code_sfdc_advertiser_df\n",
    "df_name = 'encoding_product_code_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[(encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)) & ~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['product_code'].fillna('')\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    # working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "\n",
    "    encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_name', 'sfdc_advertiser_id', 'encoding_format_id', 'encoding_product_code', 'product_code']]\n",
    "    encoding_expanded_df.rename(columns={'encoding_format_id': 'format_id'}, inplace=True)\n",
    "    encoding_expanded_df.drop(columns=['product_code'], inplace=True)\n",
    "    encoding_expanded_df.rename(columns={'enc_product_code': 'product_code'}, inplace=True)\n",
    "    encoding_expanded_df\n",
    "    encoding_expanded_df = encoding_expanded_df.dropna(subset=['sfdc_advertiser_id']).copy()\n",
    "    encoding_expanded_slim_df = encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_id', 'format_id', 'product_code']].copy()\n",
    "    encoding_expanded_slim_df\n",
    "\n",
    "    merged_df = working_df.merge(encoding_expanded_slim_df, on=['sfdc_account_id', 'format_id', 'product_code'], how='left')\n",
    "    merged_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    merged_df\n",
    "    new_encodings_bvs_df\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, merged_df], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "\n",
    "    processed.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# encoding_product_code_ignore_format\n",
    "meta_df = encoding_product_code_ignore_format_sfdc_advertiser_df\n",
    "df_name = 'encoding_product_code_ignore_format_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[(encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)) & ~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['product_code'].fillna('')\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "\n",
    "    encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_name', 'sfdc_advertiser_id',  'encoding_product_code', 'product_code']]\n",
    "    # encoding_expanded_df.rename(columns={'encoding_format_id': 'format_id'}, inplace=True)\n",
    "    encoding_expanded_df.drop(columns=['product_code'], inplace=True)\n",
    "    encoding_expanded_df.rename(columns={'enc_product_code': 'product_code'}, inplace=True)\n",
    "    encoding_expanded_df\n",
    "    encoding_expanded_df = encoding_expanded_df.dropna(subset=['sfdc_advertiser_id']).copy()\n",
    "    encoding_expanded_slim_df = encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_id',  'product_code']].copy()\n",
    "    encoding_expanded_slim_df\n",
    "\n",
    "    merged_df = working_df.merge(encoding_expanded_slim_df, on=['sfdc_account_id',  'product_code'], how='left')\n",
    "    merged_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    merged_df\n",
    "    new_encodings_bvs_df\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, merged_df], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "\n",
    "    processed.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# encoding_advertiser\n",
    "meta_df = encoding_advertiser_sfdc_advertiser_df\n",
    "df_name = 'encoding_advertiser_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[(encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)) & ~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['advertiser'].fillna('')\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "\n",
    "    encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_name', 'sfdc_advertiser_id', 'encoding_format_id', 'enc_advertiser']]\n",
    "    encoding_expanded_df.rename(columns={'encoding_format_id': 'format_id'}, inplace=True)\n",
    "    encoding_expanded_df.drop(columns=['product_code'], inplace=True)\n",
    "    encoding_expanded_df.rename(columns={'enc_advertiser': 'advertiser'}, inplace=True)\n",
    "    encoding_expanded_df\n",
    "    encoding_expanded_df = encoding_expanded_df.dropna(subset=['sfdc_advertiser_id']).copy()\n",
    "    encoding_expanded_slim_df = encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_id', 'format_id', 'advertiser']].copy()\n",
    "    encoding_expanded_slim_df\n",
    "\n",
    "    merged_df = working_df.merge(encoding_expanded_slim_df, on=['sfdc_account_id', 'format_id', 'advertiser'], how='left')\n",
    "    merged_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    merged_df\n",
    "    new_encodings_bvs_df\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, merged_df], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "\n",
    "    processed.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# encoding_advertiser_ignore_format\n",
    "meta_df = encoding_advertiser_ignore_format_sfdc_advertiser_df\n",
    "df_name = 'encoding_advertiser_ignore_format_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[(encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)) & ~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['advertiser'].fillna('')\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "\n",
    "    encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_name', 'sfdc_advertiser_id',  'enc_advertiser']]\n",
    "    # encoding_expanded_df.rename(columns={'encoding_format_id': 'format_id'}, inplace=True)\n",
    "    encoding_expanded_df.drop(columns=['product_code'], inplace=True)\n",
    "    encoding_expanded_df.rename(columns={'enc_advertiser': 'advertiser'}, inplace=True)\n",
    "    encoding_expanded_df\n",
    "    encoding_expanded_df = encoding_expanded_df.dropna(subset=['sfdc_advertiser_id']).copy()\n",
    "    encoding_expanded_slim_df = encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_id',  'advertiser']].copy()\n",
    "    encoding_expanded_slim_df\n",
    "\n",
    "    merged_df = working_df.merge(encoding_expanded_slim_df, on=['sfdc_account_id', 'advertiser'], how='left')\n",
    "    merged_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    merged_df\n",
    "    new_encodings_bvs_df\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, merged_df], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "\n",
    "    processed.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# encoding_format\n",
    "meta_df = encoding_format_sfdc_advertiser_df\n",
    "df_name = 'encoding_format_sfdc_advertiser_df'\n",
    "if df_name in processed:\n",
    "    print(f\"Already processed {df_name}\")\n",
    "else:\n",
    "    sfdc_account_ids = meta_df['sfdc_account_id'].unique().tolist()\n",
    "    working_df = encodings_bvs_df[(encodings_bvs_df['sfdc_account_id'].isin(sfdc_account_ids)) & ~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))].copy()\n",
    "\n",
    "    # Ensure 'product_code' column is filled with empty strings for null values\n",
    "    working_df['product_code'] = working_df['advertiser'].fillna('')\n",
    "    if 'sfdc_advertiser_id' in working_df.columns:\n",
    "        working_df.drop(columns=['sfdc_advertiser_id'], inplace=True)\n",
    "    encoding_expanded_df = meta_df.copy()\n",
    "\n",
    "    encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_name', 'sfdc_advertiser_id', 'encoding_format_id']]\n",
    "    encoding_expanded_df.rename(columns={'encoding_format_id': 'format_id'}, inplace=True)\n",
    "    # encoding_expanded_df.drop(columns=['product_code'], inplace=True)\n",
    "    # encoding_expanded_df.rename(columns={'enc_advertiser': 'advertiser'}, inplace=True)\n",
    "    encoding_expanded_df\n",
    "    encoding_expanded_df = encoding_expanded_df.dropna(subset=['sfdc_advertiser_id']).copy()\n",
    "    encoding_expanded_slim_df = encoding_expanded_df[['sfdc_account_id', 'sfdc_advertiser_id', 'format_id']].copy()\n",
    "    encoding_expanded_slim_df\n",
    "\n",
    "    merged_df = working_df.merge(encoding_expanded_slim_df, on=['sfdc_account_id', 'format_id'], how='left')\n",
    "    merged_df.dropna(subset=['sfdc_advertiser_id'], inplace=True)\n",
    "    merged_df\n",
    "    new_encodings_bvs_df\n",
    "    new_encodings_bvs_df = pd.concat([new_encodings_bvs_df, merged_df], ignore_index=True)\n",
    "    new_encodings_encodings_ids = new_encodings_bvs_df['encoding_id'].unique().tolist()\n",
    "    len(new_encodings_bvs_df)\n",
    "    # 1686537\n",
    "\n",
    "    processed.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "new_encodings_bvs_df.sort_values(by=['format_id', 'encoding_id'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# new_encodings_bvs_merge_df = new_encodings_bvs_df[['encoding_id', 'sfdc_advertiser_id']].drop_\n",
    "# new_encodings_bvs_merge_df\n",
    "# encodings_bvs_df_to_write = encodings_bvs_df.merge(new_encodings_bvs_merge_df, on='encoding_id', how='left')\n",
    "# encodings_bvs_df_to_write\n",
    "len(encodings_bvs_df)\n",
    "len(encodings_bvs_df[~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))]) + len(new_encodings_bvs_df)\n",
    "encodings_bvs_df['sfdc_advertiser_id'] = ''\n",
    "# 1686537\n",
    "# 1686866\n",
    "encodings_bvs_df_to_write = pd.concat([encodings_bvs_df[~(encodings_bvs_df['encoding_id'].isin(new_encodings_encodings_ids))], new_encodings_bvs_df], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# encodings_bvs_df_to_write.sort_values(by=['sfdc_account_id'], inplace=True)\n",
    "billing_last_updated = pd.Timestamp.utcnow().floor('s')\n",
    "encodings_bvs_df_to_write['billing_last_updated'] = billing_last_updated\n",
    "billing_last_audit_id = core_functions.generate_uuid()\n",
    "encodings_bvs_df_to_write['billing_last_audit_id'] = billing_last_audit_id\n",
    "# encodings_bvs_df_to_write\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_functions.print_dataframe_parquet_schema(encodings_bvs_df_to_write, 'encodings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_functions.print_dataframe_bigquery_schema_yaml(encodings_bvs_df_to_write, 'encodings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_encodings_segments(df):\n",
    "#     df['segments_date'] = pd.to_datetime(df['encoded_timestamp']).dt.date\n",
    "#     df['segments_day_of_week'] = pd.to_datetime(df['encoded_timestamp']).dt.day_name()\n",
    "#     df['segments_device'] = 'tv'\n",
    "#     df['segments_month'] = pd.to_datetime(df['encoded_timestamp']).dt.to_period('M')\n",
    "#     df['segments_quarter'] = pd.to_datetime(df['encoded_timestamp']).dt.to_period('Q')\n",
    "#     df['segments_week'] = pd.to_datetime(df['encoded_timestamp']).dt.to_period('W')\n",
    "#     df['segments_year'] = pd.to_datetime(df['encoded_timestamp']).dt.to_period('Y')\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_df_to_write2 = core_functions.process_encodings_segments(encodings_bvs_df_to_write)\n",
    "encodings_bvs_df_to_write2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_functions.print_dataframe_parquet_schema(encodings_bvs_df_to_write2, 'encodings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_df_to_write2['clone_of'] = encodings_bvs_df_to_write2['clone_of'].astype('Int64').fillna(-11)\n",
    "encodings_bvs_df_to_write2['customer_id'] = encodings_bvs_df_to_write2['customer_id'].astype('Int64').fillna(0)\n",
    "string_cols = ['encoded_timestamp',  'last_updated', 'detection_end_date', 'encoder_group__last_updated','aeis__last_updated' ,'format__last_updated', 'profile__last_updated',\n",
    "               'billing_last_updated']\n",
    "\n",
    "# for col in encodings_bvs_df_to_write2.columns:\n",
    "#     if encodings_bvs_df_to_write2[col].dtype == 'object':\n",
    "#         encodings_bvs_df_to_write2[col] = encodings_bvs_df_to_write2[col].astype('string')\n",
    "#     if col in string_cols:\n",
    "#         encodings_bvs_df_to_write2[col] = encodings_bvs_df_to_write2[col].astype('string')\n",
    "\n",
    "core_functions.print_dataframe_python_schema(encodings_bvs_df_to_write2, 'encodings')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encodings_bvs_df_to_write2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping to test\n",
    "valid_final_cols = ['encoding_id',\n",
    " 'format_id',\n",
    " 'encoder_group_id',\n",
    " 'encoded_timestamp',\n",
    " 'clone_of',\n",
    " 'status',\n",
    " 'last_updated',\n",
    " 'last_audit_id',\n",
    " 'encoder_id',\n",
    " 'detection_end_date',\n",
    " 'encoded_timestamp_epoch',\n",
    " 'attributes_advertiser',\n",
    " 'attributes_audience',\n",
    " 'attributes_audience_2',\n",
    " 'attributes_cable_estimate',\n",
    " 'attributes_campaign',\n",
    " 'attributes_category',\n",
    " 'attributes_client_code',\n",
    " 'attributes_commercial_id',\n",
    " 'attributes_contour_id',\n",
    " 'attributes_creative_offer',\n",
    " 'attributes_description',\n",
    " 'attributes_donovan_agency_advertiser_code',\n",
    " 'attributes_donovan_agency_estimate_code',\n",
    " 'attributes_donovan_agency_product_code',\n",
    " 'attributes_eid',\n",
    " 'attributes_group',\n",
    " 'attributes_hd_sd',\n",
    " 'attributes_id',\n",
    " 'attributes_isci',\n",
    " 'length_in_seconds',\n",
    " 'attributes_length',\n",
    " 'attributes_lob',\n",
    " 'attributes_media_type',\n",
    " 'attributes_message',\n",
    " 'attributes_misc',\n",
    " 'attributes_module_code',\n",
    " 'attributes_offer',\n",
    " 'attributes_offer_2',\n",
    " 'attributes_phone_number',\n",
    " 'attributes_product_code',\n",
    " 'attributes_product_name',\n",
    " 'attributes_project_name',\n",
    " 'attributes_quality',\n",
    " 'attributes_revision',\n",
    " 'attributes_show_name',\n",
    " 'attributes_slug',\n",
    " 'attributes_sport_id',\n",
    " 'attributes_sport_show_sub_category',\n",
    " 'attributes_spot_estimate',\n",
    " 'attributes_spot_name',\n",
    " 'attributes_tag',\n",
    " 'attributes_text',\n",
    " 'attributes_title',\n",
    " 'attributes_veil_id',\n",
    " 'attributes_version_name',\n",
    " 'attributes_year',\n",
    " 'product_code',\n",
    " 'isci',\n",
    " 'advertiser',\n",
    " 'encoder_group_name',\n",
    " 'encoder_group__deleted',\n",
    " 'encoder_group__last_audit_id',\n",
    " 'encoder_group__last_updated',\n",
    " 'aeis_id',\n",
    " 'aeis__encoding_id',\n",
    " 'aeis__encoding_offset',\n",
    " 'aeis__last_updated',\n",
    " 'aeis__last_audit_id',\n",
    " 'format_name',\n",
    " 'format__profile_id',\n",
    " 'format__customer_id',\n",
    " 'format__report_breakup',\n",
    " 'format__deleted',\n",
    " 'format__last_updated',\n",
    " 'format__last_audit_id',\n",
    " 'sfdc_account_id',\n",
    " 'sfdc_account_name',\n",
    " 'customer_id',\n",
    " 'account_id',\n",
    " 'contract_item',\n",
    " 'customer_name',\n",
    " 'contract_number',\n",
    " 'sales_person_code',\n",
    " 'deleted',\n",
    " 'profile_id',\n",
    " 'profile_name',\n",
    " 'profile__deleted',\n",
    " 'profile__default_asset_code',\n",
    " 'profile__last_updated',\n",
    " 'profile__last_audit_id',\n",
    " 'ad_prod_campaign',\n",
    " 'campaign',\n",
    " 'sfdc_advertiser_id',\n",
    " 'billing_last_updated',\n",
    " 'billing_last_audit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_cols = ['encoding_id','format_id','encoder_group_id','clone_of','encoder_id','encoded_timestamp_epoch', 'length_in_seconds', 'aeis_id','aeis__encoding_id','aeis__encoding_offset','format__profile_id','format__customer_id','format__last_updated','customer_id','account_id','contract_item','contract_number']\n",
    "# date_cols = ['encoded_timestamp', 'last_updated','detection_end_date','encoder_group__last_updated','aeis__last_updated','profile__last_updated','billing_last_updated']\n",
    "# drop_cols = ['attributes','profile__attributes']\n",
    "# bool_cols = ['encoder_group__deleted','format__deleted','deleted']\n",
    "# for col in encodings_bvs_df_to_write.columns:\n",
    "#     if col in encodings_bvs_df_to_write.columns and col not in valid_final_cols:\n",
    "#         encodings_bvs_df_to_write.drop(columns=col, inplace=True)\n",
    "#     if col in int_cols:\n",
    "#         encodings_bvs_df_to_write[col] = encodings_bvs_df_to_write[col].fillna(-1).astype(int)\n",
    "#     if col in date_cols:\n",
    "#         encodings_bvs_df_to_write[col] = pd.to_datetime(encodings_bvs_df_to_write[col], errors='coerce', utc=True)\n",
    "#     if col in drop_cols and col in encodings_bvs_df_to_write.columns:\n",
    "#         encodings_bvs_df_to_write.drop(columns=col, inplace=True)\n",
    "#     if col in bool_cols:\n",
    "#         encodings_bvs_df_to_write[col] = encodings_bvs_df_to_write[col].fillna(False).astype(bool)\n",
    "#     if col in valid_final_cols and col not in encodings_bvs_df_to_write.columns:\n",
    "#         encodings_bvs_df_to_write[col] = ''\n",
    "#     if col in valid_final_cols and col not in int_cols and col not in date_cols and col not in bool_cols:\n",
    "#         encodings_bvs_df_to_write[col] = encodings_bvs_df_to_write[col].fillna('').astype(str)\n",
    "# for col in encodings_bvs_df_to_write.columns:\n",
    "#     print(f\"{col}: type: {encodings_bvs_df_to_write[col].dtype}\")\n",
    "# # encodings_bvs_df_to_write.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bvs_df_to_write2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# for col in encodings_bvs_df_to_write2.columns:\n",
    "#     if encodings_bvs_df_to_write2[col].dtype == StructType:\n",
    "#         print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Function to detect mixed types in each column\n",
    "# def detect_mixed_types(df):\n",
    "#     mixed_type_columns = []\n",
    "#     type_counts = defaultdict(set)\n",
    "    \n",
    "#     for col in df.columns:\n",
    "#         for val in df[col].dropna().unique():\n",
    "#             type_counts[col].add(type(val))\n",
    "#             if len(type_counts[col]) > 1:\n",
    "#                 mixed_type_columns.append(col)\n",
    "#                 break  # No need to check further once mixed types are found\n",
    "                \n",
    "#     return mixed_type_columns, type_counts\n",
    "\n",
    "# mixed_cols, type_counts = detect_mixed_types(df)\n",
    "\n",
    "# print(\"Columns with mixed types:\")\n",
    "# for col in mixed_cols:\n",
    "#     print(f\"{col}: {type_counts[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhive import hive\n",
    "\n",
    "\n",
    "# # Connect to Hive Metastore\n",
    "# conn = hive.Connection(host='10.11.0.10', port=10000, username='anonymous')\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute('CREATE DATABASE IF NOT EXISTS stl_data_lake')\n",
    "# cursor.execute(\"USE stl_data_lake\")\n",
    "# cursor.execute('SHOW DATABASES')\n",
    "\n",
    "# print(\"Databases:\")\n",
    "# for database in cursor.fetchall():\n",
    "#     print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype_map = {\n",
    "#     'object': 'STRING',\n",
    "#     'string': 'STRING',\n",
    "#     'int64': 'BIGINT',\n",
    "#     'Int64': 'BIGINT',\n",
    "#     'float64': 'DOUBLE',\n",
    "#     'Float64': 'DOUBLE',\n",
    "#     'datetime64[ns]': 'TIMESTAMP',\n",
    "#     'bool': 'BOOLEAN'\n",
    "# }\n",
    "\n",
    "# # Create a function to map dtype\n",
    "# def map_dtype(pandas_dtype):\n",
    "#     pandas_str = str(pandas_dtype)\n",
    "#     if pandas_str in dtype_map:\n",
    "#         return dtype_map[pandas_str]\n",
    "#     else:\n",
    "#         # Default fallback, adjust as needed\n",
    "#         return 'STRING'\n",
    "\n",
    "# # Build the column definitions for Hive\n",
    "# column_defs = []\n",
    "# for col in encodings_bvs_df_to_write2.columns:\n",
    "#     hive_type = map_dtype(encodings_bvs_df_to_write2[col].dtype)\n",
    "#     column_defs.append(f\"`{col}` {hive_type}\")\n",
    "\n",
    "# # Join all column definitions into a single string\n",
    "# columns_str = \",\\n    \".join(column_defs)\n",
    "\n",
    "# # Build the final CREATE TABLE statement\n",
    "# table_name = \"expanded_encodings_bvs\"\n",
    "# create_table_statement = f\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "#     {columns_str}\n",
    "# )\n",
    "# STORED AS PARQUET\n",
    "# \"\"\"\n",
    "\n",
    "# print(create_table_statement)\n",
    "# # cursor.execute(create_table_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute('SHOW TABLES')\n",
    "\n",
    "# print(\"Tables:\")\n",
    "# for table in cursor.fetchall():\n",
    "#     print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test utility to check the data types of the columns\n",
    "\n",
    "for col in encodings_bvs_df_to_write2.columns:\n",
    "    print(f'Column: {col} - Type: {encodings_bvs_df_to_write2[col].dtype}')\n",
    "    print(f'Sample Data: {encodings_bvs_df_to_write2[col][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings_bvs_df_to_write2['attributes'] = encodings_bvs_df_to_write2['attributes'].apply(lambda x: json.dumps(x)).astype('string')\n",
    "# encodings_bvs_df_to_write2['profile__attributes'] = encodings_bvs_df_to_write2['profile__attributes'].apply(lambda x: json.dumps(x)).astype('string')\n",
    "encodings_bvs_df_to_write2.rename(columns={'segments_format_id_group': '_FORMAT_ID_GROUP'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings_bvs_df_to_write2.drop(columns=['attributes', 'profile__attributes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cell\n",
    "\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "\n",
    "process_df = encodings_bvs_df_to_write2.copy()\n",
    "\n",
    "# nested_columns = ['attributes']\n",
    "# for col in nested_columns:\n",
    "#     process_df[col] = process_df[col].apply(lambda x: json.dumps(x) if pd.notna(x) else '')\n",
    "    \n",
    "# def parse_iso_with_timezone(ts):\n",
    "#     # Replace timezone colon\n",
    "#     ts = ts.replace(\":\", \"\", 1) if \"+\" in ts or \"-\" in ts else ts\n",
    "#     return datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "# # parsed_timestamps = [parse_iso_with_timezone(ts) for ts in process_df['encoded_timestamp'] ]\n",
    "# process_df['encoded_timestamp'] = process_df['encoded_timestamp'].apply(parse_iso_with_timezone)\n",
    "\n",
    "# print(parsed_timestamps)\n",
    "# process_df['encoded_timestamp'] = pd.to_datetime(process_df['encoded_timestamp'])\n",
    "# process_df = core_functions.convert_to_string_except_exclusions(process_df, exclude_columns=['encoded_timestamp', 'encoding_id', 'format_id','customer_id', 'profile_id', 'billing_last_updated', 'deleted', 'profile_deleted'])\n",
    "# process_df.drop(columns=['attributes'], inplace=True)\n",
    "\n",
    "process_df = process_df.sort_values(by=['segments_year', 'format_id', 'encoding_id'], ascending=True).copy().reset_index(drop=True)\n",
    "\n",
    "s4_storage_options = config.get('S4_STORAGE_OPTIONS')\n",
    "veil_storage_options = config.get('VEIL_GCS_STORAGE_OPTIONS')\n",
    "n90_storage_options = config.get('N90_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "\n",
    "s4_bucket = 'n90-data-lake-stl'\n",
    "s4_output_prefix = 'veil/encodings_v2b'\n",
    "veil_billing_bucket = config.get('veil_billing').get('billing_gcs_bucket_id')\n",
    "process_df['encoded_timestamp']\n",
    "# process_df['profile__attributes']\n",
    "n90_bucket = 'n90_veil_partner'\n",
    "n90_bucket_2 = 'n90-data-lake'\n",
    "veil_output_prefix = 'encodings_v2b'\n",
    "n90_output_prefix = 'advocado-looker/avs_prod/encodings_v2b'\n",
    "n90_output_prefix_2 = 'avs_prod/encodings_v2b'\n",
    "partition_cols = ['_FORMAT_ID_GROUP']\n",
    "\n",
    "\n",
    "core_functions.write_hive_partitioned_parquet_s4(process_df, s4_bucket, s4_output_prefix, partition_cols, s4_storage_options, spec='s3')\n",
    "print(f\"Finished writing to {s4_bucket}/{s4_output_prefix}\")\n",
    "\n",
    "\n",
    "core_functions.write_hive_partitioned_parquet(process_df, veil_billing_bucket, veil_output_prefix, partition_cols, veil_storage_options)\n",
    "print(f\"Finished writing to {veil_billing_bucket}/{veil_output_prefix}\")\n",
    "core_functions.write_hive_partitioned_parquet(process_df, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "core_functions.write_hive_partitioned_parquet(process_df, n90_bucket_2, n90_output_prefix_2, partition_cols, n90_storage_options)\n",
    "print(f\"Finished writing to {n90_bucket_2}/{n90_output_prefix_2}\")\n",
    "core_functions.write_hive_partitioned_parquet_s4(process_df, s4_bucket, s4_output_prefix, partition_cols, s4_storage_options)\n",
    "print(f\"Finished writing to {s4_bucket}/{s4_output_prefix}\")\n",
    "\n",
    "\n",
    "# # new cell\n",
    "\n",
    "# print(encodings_bvs_df_to_write.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
