{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhandrigan/bdh_colab_notebooks/blob/main/avs_billing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMbyqHAVQP0d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Packages\n",
        "!pip install simple-salesforce XlsxWriter\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "import google.cloud.bigquery\n",
        "import google.cloud.storage\n",
        "import google.oauth2\n",
        "from google.cloud import bigquery, storage\n",
        "from google.colab import auth, userdata\n",
        "from google.oauth2 import service_account\n",
        "import google.cloud.exceptions\n",
        "from google.cloud.exceptions import NotFound\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_gbq\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import csv\n",
        "import calendar\n",
        "from calendar import week\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "import uuid\n",
        "import simple_salesforce\n",
        "from simple_salesforce import Salesforce, SalesforceMalformedRequest\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "# Function to install and import modules and sub-modules\n",
        "# def install_and_import(module_name, package_name=None, names=None, alias=None):\n",
        "#     \"\"\"\n",
        "#     Attempts to import the module or specific names from the module.\n",
        "#     If the module is not installed, it installs the package and then imports.\n",
        "\n",
        "#     Parameters:\n",
        "#     - module_name (str): The module to import.\n",
        "#     - package_name (str): The pip package name to install if the module is not found.\n",
        "#                           If None, assumes package name is the same as module name.\n",
        "#     - names (list): List of specific names to import from the module.\n",
        "#                     If None, imports the module itself.\n",
        "#     - alias (str): Alias to assign to the imported module or name.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         if names:\n",
        "#             module = importlib.import_module(module_name)\n",
        "#             for name in names:\n",
        "#                 imported_name = getattr(module, name)\n",
        "#                 if alias and len(names) == 1:\n",
        "#                     sys.modules[__name__].__dict__[alias] = imported_name\n",
        "#                 else:\n",
        "#                     sys.modules[__name__].__dict__[name] = imported_name\n",
        "#             print(f\"Imported {names} from {module_name}\")\n",
        "#         else:\n",
        "#             imported_module = importlib.import_module(module_name)\n",
        "#             if alias:\n",
        "#                 sys.modules[__name__].__dict__[alias] = imported_module\n",
        "#             else:\n",
        "#                 sys.modules[__name__].__dict__[module_name] = imported_module\n",
        "#             print(f\"Imported {module_name}\")\n",
        "#     except ImportError:\n",
        "#         if package_name is None:\n",
        "#             package_name = module_name.replace('.', '-')\n",
        "#         print(f\"Installing package {package_name}\")\n",
        "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "#         # Try importing again\n",
        "#         try:\n",
        "#             if names:\n",
        "#                 module = importlib.import_module(module_name)\n",
        "#                 for name in names:\n",
        "#                     imported_name = getattr(module, name)\n",
        "#                     if alias and len(names) == 1:\n",
        "#                         sys.modules[__name__].__dict__[alias] = imported_name\n",
        "#                     else:\n",
        "#                         sys.modules[__name__].__dict__[name] = imported_name\n",
        "#                 print(f\"Installed {package_name} and imported {names} from {module_name}\")\n",
        "#             else:\n",
        "#                 imported_module = importlib.import_module(module_name)\n",
        "#                 if alias:\n",
        "#                     sys.modules[__name__].__dict__[alias] = imported_module\n",
        "#                 else:\n",
        "#                     sys.modules[__name__].__dict__[module_name] = imported_module\n",
        "#                 print(f\"Installed {package_name} and imported {module_name}\")\n",
        "#         except ImportError as e:\n",
        "#             print(f\"Failed to import {module_name} after installing {package_name}: {e}\")\n",
        "\n",
        "# # Mapping of module names to pip package names\n",
        "# module_to_pip = {\n",
        "#     \"google.cloud.bigquery\": \"google-cloud-bigquery\",\n",
        "#     \"pandas_gbq\": \"pandas-gbq\",\n",
        "#     \"simple_salesforce\": \"simple-salesforce\",\n",
        "#     \"google.colab\": \"google-colab\",\n",
        "#     \"google.oauth2\": \"google-auth\",\n",
        "#     \"google.cloud.storage\": \"google-cloud-storage\",\n",
        "#     \"google.cloud.exceptions\": \"google-cloud-exceptions\",\n",
        "#     \"requests\": \"requests\",\n",
        "#     \"xlsxwriter\": \"XlsxWriter\",\n",
        "#     \"pandas\": \"pandas\",\n",
        "#     \"numpy\": \"numpy\",\n",
        "#     \"uuid\": \"uuid\",\n",
        "#     \"re\": None,\n",
        "#     \"os\": None,\n",
        "#     \"json\": None,\n",
        "#     \"datetime\": None,\n",
        "#     \"time\": None,\n",
        "#     \"random\": None,\n",
        "#     \"csv\": None,\n",
        "#     \"calendar\": None,\n",
        "# }\n",
        "\n",
        "# # List of modules to check and import\n",
        "# modules_to_import = [\n",
        "#     {\"module_name\": \"google.cloud\", \"names\": [\"bigquery\", \"storage\"]},\n",
        "#     {\"module_name\": \"pandas_gbq\", \"names\": None},\n",
        "#     {\"module_name\": \"simple_salesforce\", \"names\": [\"Salesforce\", \"SalesforceMalformedRequest\"]},\n",
        "#     {\"module_name\": \"google.colab\", \"names\": [\"auth\"]},\n",
        "#     {\"module_name\": \"google.oauth2\", \"names\": [\"service_account\"]},\n",
        "#     {\"module_name\": \"google.cloud.storage\", \"names\": None},\n",
        "#     {\"module_name\": \"google.cloud.exceptions\", \"names\": [\"NotFound\"]},\n",
        "#     {\"module_name\": \"requests\", \"names\": None},\n",
        "#     {\"module_name\": \"xlsxwriter\", \"names\": None},\n",
        "#     {\"module_name\": \"pandas\", \"alias\": \"pd\", \"names\": None},\n",
        "#     {\"module_name\": \"numpy\", \"alias\": \"np\", \"names\": None},\n",
        "#     {\"module_name\": \"datetime\", \"names\": [\"datetime\", \"timedelta\"]},\n",
        "#     {\"module_name\": \"os\", \"names\": None},\n",
        "#     {\"module_name\": \"json\", \"names\": None},\n",
        "#     {\"module_name\": \"re\", \"names\": None},\n",
        "#     {\"module_name\": \"uuid\", \"names\": None},\n",
        "#     {\"module_name\": \"csv\", \"names\": None},\n",
        "#     {\"module_name\": \"time\", \"names\": None},\n",
        "#     {\"module_name\": \"random\", \"names\": None},\n",
        "#     {\"module_name\": \"calendar\", \"names\": None},\n",
        "#     {\"module_name\": \"calendar\", \"names\": ['week']},\n",
        "# ]\n",
        "\n",
        "# # Install and import modules\n",
        "# for module_info in modules_to_import:\n",
        "#     module_name = module_info.get('module_name')\n",
        "#     package_name = module_to_pip.get(module_name)\n",
        "#     names = module_info.get('names')\n",
        "#     alias = module_info.get('alias')\n",
        "#     install_and_import(module_name, package_name, names, alias)\n",
        "\n",
        "\n",
        "print('Imported [\\'userdata\\'] from google.colab')\n",
        "# Now you can use the imported modules and names directly\n",
        "# For example, pandas is available as 'pd', numpy as 'np', etc.\n",
        "\n",
        "# Example usage:\n",
        "# df = pd.DataFrame()\n",
        "# array = np.array([1, 2, 3])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w-wghKOw81nq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atSDPxGvXkKN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set Vars\n",
        "# @markdown Enter the target billing month in ####.## format with #### being the year and ## being the broadcast month number from 01 to 12.\n",
        "bcm_index_list_str = \"2024.11\" # @param {type:\"string\"}\n",
        "bcm_index_list = [float(x) for x in bcm_index_list_str.split(\",\")]\n",
        "bcm_index = bcm_index_list[-1]\n",
        "# bcm_index = bcm_index_list[0]\n",
        "# project_id = \"adhoc-billing\"\n",
        "# @markdown Sync Salesforce with BVS before processing if this is the first time processing or data has changed and needs to be updated.\n",
        "sync_bvs_with_sfdc_before_processing = True # @param {type:\"boolean\"}\n",
        "sync_salesforce_with_bvs_before_processing = True # @param {type:\"boolean\"}\n",
        "sync_salesforce_type = \"FULL\" # @param [\"FULL\",\"LIMITED\",\"BILLING_RUN\"]\n",
        "limited_salesforce_objects = [\"Task\",\"Account\",\"Product2\",\"Pricebook2\",\"PricebookEntry\",\"Advertiser__c\",\"Rate_Card__c\",\"Encoder_Device__c\",\"BVS_Format__c\",\"BVS_Profile__c\",\"BVS_Product_Code__c\",\"BVS_Client_Code__c\",\"BVS_Customer__c\",\"Encoder_Group__c\",\"Encoding_Advertiser__c\",\"Encoding_Module_Code__c\",\"Encoding_Product_Code__c\"] # @param\n",
        "billing_run_salesforce_objects = [\"Task\",\"Account\",\"Opportunity\",\"OpportunityLineItem\",\"Advertiser__c\",\"Rate_Card__c\"] # @param\n",
        "# @markdown Rebuild encodings if sfdc data or bvs data has changed that could affect ownership or alignment of encodings with advertisers.\n",
        "rebuild_encodings = False # @param {type:\"boolean\"}\n",
        "# @markdown Rebuild the detection archive if sfdc or bvs data has changed that could affect billing outcomes, you are changing billing months to assess or starting a new billing month.\n",
        "rebuild_detection_archive = True # @param {type:\"boolean\"}\n",
        "# @markdown Generate Encoder Excel Backup\n",
        "gen_encoder_excel = False # @param {type:\"boolean\"}\n",
        "# @markdown Generate Weekly Excel Backup\n",
        "gen_weekly_excel = False # @param {type:\"boolean\"}\n",
        "# @markdown Generate Monthly Invoice Excel Backup\n",
        "gen_monthly_excel = True # @param {type:\"boolean\"}\n",
        "# # @markdown Generate Monthly Usage Excel Backup\n",
        "# generate_monthly_usage_excel = True # @param {type:\"boolean\"}\n",
        "# # @markdown Generate Monthly Tiered Excel Backup\n",
        "# generate_monthly_tiered_excel = True # @param {type:\"boolean\"}\n",
        "# # @markdown Generate Monthly Detections Excel Backup\n",
        "# generate_monthly_detections_excel = True # @param {type:\"boolean\"}\n",
        "# @markdown Generate SFDC Tasks\n",
        "generate_sfdc_tasks = False # @param {type:\"boolean\"}\n",
        "# @markdown Process SFDC Task Responses\n",
        "process_sfdc_tasks = True # @param {type:\"boolean\"}\n",
        "# @markdown Send Opportunities to SFDC\n",
        "send_opps_to_sfdc = True # @param {type:\"boolean\"}\n",
        "process_sfdc_opps_from_invoices = True # @param {type:\"boolean\"}\n",
        "\n",
        "test_mode = False # @param {type:\"boolean\"}\n",
        "storage_bucket_name = 'advocado_billing_download' # @param {type:\"string\"}\n",
        "project_id = 'adhoc-billing' # @param {type:\"string\"}\n",
        "dataset_id = 'avs_billing_process' # @param {type:\"string\"}\n",
        "encodings_matched_table_id = 'encodings_matched' # @param {type:\"string\"}\n",
        "encodings_unmatched_table_id = 'encodings_unmatched' # @param {type:\"string\"}\n",
        "encodings_matched_with_rates_table_id = 'encodings_matched_with_rates' # @param {type:\"string\"}\n",
        "avs_encodings_master_table_id = 'avs_encodings_master' # @param {type:\"string\"}\n",
        "billing_records_archive_table_id = 'billing_records_archive' # @param {type:\"string\"}\n",
        "detections_detail_table_id = 'detections_detail' # @param {type:\"string\"}\n",
        "billing_records__working_table_id = 'billing_records__working' # @param {type:\"string\"}\n",
        "billing_invoice_files_table_id = '_billing_invoice_files' # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "sync_salesforce_auth_token = userdata.get('SYNC_SALESFORCE_API_KEY')\n",
        "sync_salesforce_url = \"https://us-central1-adhoc-billing.cloudfunctions.net/sync_salesforce\" # @param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set SFDC Opportunity Vars\n",
        "opp_owner_id = '0053h000006UDKhAAO' # @param {type:\"string\"}\n",
        "opp_stage_name = 'Stage 3.5 - Billing Ready for Review' # @param {type:\"string\"}\n",
        "opp_type = 'Existing Business' # @param {type:\"string\"}\n",
        "opp_pricebook_id = '01s3h00000257iWAAQ' # @param {type:\"string\"}\n",
        "opp_record_type_id = '0123h000000MtmPAAS' # @param {type:\"string\"}\n",
        "sfdc_field_map = {}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "klvDFHuyhPoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJHZGKRuYVac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1ES53vItaRRQ"
      },
      "outputs": [],
      "source": [
        "# @title create clients\n",
        "\n",
        "\n",
        "\n",
        "USE_ENV_VARS = False\n",
        "service_account_secret_name = 'SA_ADHOC_BILLING'\n",
        "temp_credentials_location = \"/tmp/credentials.json\"\n",
        "\n",
        "def get_service_account_credentials():\n",
        "    try:\n",
        "        if USE_ENV_VARS:\n",
        "            key_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
        "            if key_path is None:\n",
        "                raise EnvironmentError(\"Failed to find 'GOOGLE_APPLICATION_CREDENTIALS' in environment variables.\")\n",
        "            return service_account.Credentials.from_service_account_file(key_path)\n",
        "        else:\n",
        "            secret_value = userdata.get(service_account_secret_name)\n",
        "            if not secret_value:\n",
        "                raise ValueError(\"Failed to retrieve service account secret from Colab userdata.\")\n",
        "            credentials_dict = json.loads(secret_value)\n",
        "            temp_credentials = temp_credentials_location\n",
        "            with open(temp_credentials, \"w\") as f:\n",
        "                json.dump(credentials_dict, f)\n",
        "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = temp_credentials\n",
        "            return service_account.Credentials.from_service_account_info(credentials_dict)\n",
        "    except Exception as e:\n",
        "        print(f\"Error obtaining service account credentials: {e}\")\n",
        "        raise\n",
        "\n",
        "# Create credentials from the service account info\n",
        "credentials = get_service_account_credentials()\n",
        "auth.authenticate_service_account()\n",
        "\n",
        "# Initialize Google BigQuery Client\n",
        "bigquery_client = bigquery.Client(credentials=credentials)\n",
        "\n",
        "# Initialize Google Cloud Storage client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Initialize Salesforce client\n",
        "sf = Salesforce(username=userdata.get('SF_USERNAME'), password=userdata.get('SF_PASSWORD'), security_token=userdata.get('SF_TOKEN'))\n",
        "\n",
        "# Get CAD to USD conversion rate\n",
        "\n",
        "conversion_rate = None\n",
        "\n",
        "def get_historical_cad_to_usd(date):\n",
        "    api_key = '0fce96fb156d4f1eb391897e9d154e44'\n",
        "    url = f'https://openexchangerates.org/api/historical/{date}.json?app_id={api_key}&symbols=CAD'\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    cad_to_usd = 1 / data['rates']['CAD']\n",
        "    return cad_to_usd\n",
        "\n",
        "def get_latest_exchange_rate_from_bq():\n",
        "    query = \"\"\"\n",
        "    SELECT rate, date\n",
        "    FROM `adhoc-billing.avs_billing_process.lu_usd_exchange_rates`\n",
        "    WHERE currency = 'CAD'\n",
        "    ORDER BY date DESC\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "    query_job = bigquery_client.query(query)\n",
        "    results = query_job.result()\n",
        "    if results.total_rows > 0:\n",
        "        for row in results:\n",
        "            return row.rate, row.date\n",
        "    return None, None\n",
        "\n",
        "def insert_exchange_rate_to_bq(date, rate):\n",
        "    insert_query = f\"\"\"\n",
        "    INSERT INTO `adhoc-billing.avs_billing_process.lu_usd_exchange_rates` (date, currency, rate)\n",
        "    VALUES ('{date}', 'CAD', {rate})\n",
        "    \"\"\"\n",
        "    bigquery_client.query(insert_query)\n",
        "\n",
        "# Main logic\n",
        "date_of_exchange = datetime.now().date()  # Get the current date as a datetime.date object\n",
        "thirty_days_ago = date_of_exchange - timedelta(days=30)  # Subtract 30 days to get the comparison date\n",
        "\n",
        "# Check for the most recent exchange rate in BigQuery\n",
        "latest_rate, latest_date = get_latest_exchange_rate_from_bq()\n",
        "\n",
        "if latest_date and latest_date >= thirty_days_ago:\n",
        "    # Use the latest rate if it is within the last 30 days\n",
        "    conversion_rate = latest_rate\n",
        "    print(f\"Using the latest CAD to USD conversion rate from BigQuery on {latest_date}: {conversion_rate}\")\n",
        "else:\n",
        "    # Fetch the rate from the API and insert it into BigQuery\n",
        "    conversion_rate = get_historical_cad_to_usd(date_of_exchange.strftime('%Y-%m-%d'))\n",
        "    insert_exchange_rate_to_bq(date_of_exchange, conversion_rate)\n",
        "    print(f\"The CAD to USD conversion rate on {date_of_exchange} was {conversion_rate}, and it has been inserted into BigQuery.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Vmme7wfhQeid"
      },
      "outputs": [],
      "source": [
        "# @title Sync Salesforce Function\n",
        "\n",
        "def sync_salesforce_tables(salesforce_url, salesforce_auth_token, sync_type, salesforce_objects=[]):\n",
        "    _sync_salesforce_url = salesforce_url\n",
        "    _sync_salesforce_auth_token = salesforce_auth_token\n",
        "    _sync_salesforce_type = sync_type\n",
        "    _custom_salesforce_objects = salesforce_objects\n",
        "\n",
        "    if sync_salesforce_with_bvs_before_processing:\n",
        "        if _sync_salesforce_type == 'FULL':\n",
        "            url = f\"{_sync_salesforce_url}\"\n",
        "            params = {'authToken': _sync_salesforce_auth_token}\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "\n",
        "            result = (f'Sync Salesforce FULL result: {response.status_code}. ' + f'Response: {response.text}')\n",
        "            print(result)\n",
        "            return result\n",
        "        elif _sync_salesforce_type == 'LIMITED':\n",
        "            data = {\n",
        "                \"authToken\": _sync_salesforce_auth_token,\n",
        "                \"sfdc_objects_to_clone\": limited_salesforce_objects\n",
        "            }\n",
        "\n",
        "            url = f\"{_sync_salesforce_url}\"\n",
        "            headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "            response = requests.post(url, json=data, headers=headers)\n",
        "            result = (f'Sync Salesforce LIMITED result: {response.status_code}. ' + f'Response: {response.text}')\n",
        "            print(result)\n",
        "            return result\n",
        "        elif _sync_salesforce_type == 'BILLING_RUN':\n",
        "            data = {\n",
        "                \"authToken\": _sync_salesforce_auth_token,\n",
        "                \"sfdc_objects_to_clone\": billing_run_salesforce_objects\n",
        "            }\n",
        "\n",
        "            url = f\"{_sync_salesforce_url}\"\n",
        "            headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "            response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "            result = (f'Sync Salesforce BILLING RUN result: {response.status_code}. ' + f'Response: {response.text}')\n",
        "            print(result)\n",
        "            return result\n",
        "        elif _sync_salesforce_type == 'CUSTOM':\n",
        "            data = {\n",
        "                \"authToken\": _sync_salesforce_auth_token,\n",
        "                \"sfdc_objects_to_clone\": _custom_salesforce_objects\n",
        "            }\n",
        "\n",
        "            url = f\"{_sync_salesforce_url}\"\n",
        "            headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "            response = requests.post(url, json=data, headers=headers)\n",
        "\n",
        "            result = (f'Sync Salesforce BILLING RUN result: {response.status_code}. ' + f'Response: {response.text}')\n",
        "            print(result)\n",
        "            return result\n",
        "\n",
        "        else:\n",
        "            print('Invalid sync_salesforce_type specified.')\n",
        "            return 'Invalid sync_salesforce_type specified.'\n",
        "\n",
        "    else:\n",
        "        print('Skipping sync Salesforce')\n",
        "        return 'Skipping sync Salesforce'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xmPT8eFg1UK5"
      },
      "outputs": [],
      "source": [
        "# @title data fetch functions\n",
        "# Define a function to fetch data from BigQuery\n",
        "def fetch_gbq_data(query):\n",
        "    try:\n",
        "        _df = bigquery_client.query(query).to_dataframe()\n",
        "        return _df\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from GBQ: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Define a function to fetch data from Salesforce\n",
        "def fetch_sfdc_data(object_name, fields):\n",
        "    try:\n",
        "        query = f\"SELECT Id, {', '.join(fields)} FROM {object_name}\"\n",
        "        result = sf.query_all(query)\n",
        "        records = result['records']\n",
        "        for record in records:\n",
        "            record.pop('attributes', None)\n",
        "        return records\n",
        "        # return result['records']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from Salesforce: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_max_last_updated(object_name):\n",
        "    try:\n",
        "        query = f\"SELECT Id, Last_Updated__c FROM {object_name} WHERE Last_Updated__c != null ORDER BY Last_Updated__c DESC LIMIT 1\"\n",
        "        result = sf.query(query)\n",
        "        if result['totalSize'] > 0:\n",
        "            record = result['records'][0]\n",
        "            last_updated = record.get('Last_Updated__c')\n",
        "            return last_updated\n",
        "        else:\n",
        "            return '1990-01-01 00:00:00'\n",
        "            # return result['records']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching max last_updated from Salesforce: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "00K2Kiql1by2"
      },
      "outputs": [],
      "source": [
        "# @title General Utilities\n",
        "\n",
        "# Function to clean the data\n",
        "# look into how to improve\n",
        "def clean_record(record):\n",
        "    cleaned_record = {}\n",
        "    for key, value in record.items():\n",
        "        if pd.isna(value):\n",
        "            cleaned_record[key] = None\n",
        "        elif key == 'Year__c' and not str(value).isdigit():\n",
        "            cleaned_record[key] = None\n",
        "        else:\n",
        "            cleaned_record[key] = value\n",
        "    return cleaned_record\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JNa9sgIW1eLQ"
      },
      "outputs": [],
      "source": [
        "#  @title SFDC Utilities\n",
        "\n",
        "# Insert salesforce\n",
        "def insert_salesforce(object_name, records, batch_size=500):\n",
        "    \"\"\"\n",
        "    Insert records into Salesforce in batches with error handling and logging.\n",
        "\n",
        "    Args:\n",
        "    - object_name (str): The Salesforce object name.\n",
        "    - records (list of dict): List of records to insert.\n",
        "    - batch_size (int): Number of records to insert per batch. Default is 200.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    total_records = len(records)\n",
        "    print(f\"Total records to insert: {total_records}\")\n",
        "\n",
        "    for i in range(0, total_records, batch_size):\n",
        "        batch = records[i:i + batch_size]\n",
        "        success_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        for record in batch:\n",
        "            try:\n",
        "                record = clean_record(record)\n",
        "                sf.__getattr__(object_name).create(record)\n",
        "                success_count += 1\n",
        "            except SalesforceMalformedRequest as e:\n",
        "                error_count += 1\n",
        "                print(f\"Error inserting record: {record}\")\n",
        "                print(f\"Error message: {e.content}\")\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                print(f\"Unexpected error inserting record: {record}\")\n",
        "                print(f\"Error message: {str(e)}\")\n",
        "\n",
        "        print(f\"Batch {i//batch_size + 1}: Successfully inserted {success_count} records, {error_count} errors\")\n",
        "\n",
        "    print(\"Insert operation completed.\")\n",
        "\n",
        "def update_salesforce(object_name, records, batch_size=200):\n",
        "    total_records = len(records)\n",
        "    print(f\"Total records to update: {total_records}\")\n",
        "\n",
        "    for i in range(0, total_records, batch_size):\n",
        "        batch = records[i:i + batch_size]\n",
        "        success_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        for record in batch:\n",
        "            record_id = record.pop('Id')  # Remove the Id field from the record data\n",
        "            record = clean_record(record)\n",
        "            try:\n",
        "                sf.__getattr__(object_name).update(record_id, record)\n",
        "                success_count += 1\n",
        "            except SalesforceMalformedRequest as e:\n",
        "                error_count += 1\n",
        "                print(f\"Error updating record: {record}\")\n",
        "                print(f\"Error message: {e.content}\")\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                print(f\"Unexpected error updating record: {record}\")\n",
        "                print(f\"Error message: {str(e)}\")\n",
        "\n",
        "        print(f\"Batch {i//batch_size + 1}: Successfully updated {success_count} records, {error_count} errors\")\n",
        "\n",
        "    print(\"Update operation completed.\")\n",
        "\n",
        "# Define a function to delete records where a specific field is empty or null\n",
        "def delete_records_with_null_field(sf, object_name, field_name):\n",
        "    query = f\"SELECT Id FROM {object_name} WHERE {field_name} = null\"\n",
        "    result = sf.query_all(query)\n",
        "    records = result['records']\n",
        "\n",
        "    # Extract the Ids of the records to be deleted\n",
        "    ids_to_delete = [record['Id'] for record in records]\n",
        "\n",
        "    # Delete the records\n",
        "    for record_id in ids_to_delete:\n",
        "        try:\n",
        "            sf.__getattr__(object_name).delete(record_id)\n",
        "            print(f\"Successfully deleted record with Id: {record_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting record with Id: {record_id}\")\n",
        "            print(f\"Error message: {e}\")\n",
        "\n",
        "# Define a function to sync data\n",
        "def sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field):\n",
        "    merged_df = pd.DataFrame()\n",
        "    object_to_sync = None\n",
        "    # Fetch data from GBQ\n",
        "    gbq_data = fetch_gbq_data(gbq_query)\n",
        "\n",
        "    # Map GBQ fields to Salesforce fields\n",
        "    gbq_data.rename(columns=gbq_to_sfdc_field_map, inplace=True)\n",
        "\n",
        "    # Convert relevant fields to timestamps and fill NA with \"1990-01-01 00:00:00\"\n",
        "    if sfdc_object_name != 'Task':\n",
        "        for col in gbq_data.columns:\n",
        "            if col.startswith('Last_Updated__c'):\n",
        "                gbq_data[col] = pd.to_datetime(gbq_data[col], errors='coerce').fillna(pd.Timestamp('1990-01-01 00:00:00'))\n",
        "                if gbq_data[col].dt.tz is None:  # Check if tz-naive\n",
        "                    gbq_data[col] = gbq_data[col].dt.tz_localize('UTC')\n",
        "                else:  # Convert tz-aware to UTC\n",
        "                    gbq_data[col] = gbq_data[col].dt.tz_convert('UTC')\n",
        "\n",
        "    # Ensure the external ID field is integer type\n",
        "    gbq_data[sfdc_external_id_field] = gbq_data[sfdc_external_id_field].astype('Int64')\n",
        "\n",
        "    print(f\"GBQ Record Count: {len(gbq_data)}\")\n",
        "    sfdc_data = pd.DataFrame()\n",
        "\n",
        "    # Fetch data from Salesforce\n",
        "    sfdc_fields = list(gbq_to_sfdc_field_map.values())\n",
        "    sfdc_raw = fetch_sfdc_data(sfdc_object_name, sfdc_fields)\n",
        "    if sfdc_raw:\n",
        "        sfdc_data = pd.DataFrame(sfdc_raw)\n",
        "        print(\"SFDC Data (raw):\")\n",
        "    print(f\"SFDC Record Count: {len(sfdc_data)}\")\n",
        "\n",
        "    # Ensure the external ID field is integer type in Salesforce data\n",
        "    if len(sfdc_data) > 0:\n",
        "        if sfdc_object_name != 'Task':\n",
        "            sfdc_data[sfdc_external_id_field] = sfdc_data[sfdc_external_id_field].astype('Int64')\n",
        "        else:\n",
        "            sfdc_data[sfdc_external_id_field] = sfdc_data[sfdc_external_id_field].astype('Float64')\n",
        "\n",
        "        # Convert relevant fields to timestamps and fill NA with \"1990-01-01 00:00:00\" in Salesforce data\n",
        "        if sfdc_object_name != 'Task':\n",
        "            for col in sfdc_data.columns:\n",
        "                if col.startswith('Last_Updated__c'):\n",
        "                    sfdc_data[col] = pd.to_datetime(sfdc_data[col], errors='coerce').fillna(pd.Timestamp('1990-01-01 00:00:00'))\n",
        "                    if sfdc_data[col].dt.tz is None:  # Check if tz-naive\n",
        "                        sfdc_data[col] = sfdc_data[col].dt.tz_localize('UTC')\n",
        "                    else:  # Convert tz-aware to UTC\n",
        "                        sfdc_data[col] = sfdc_data[col].dt.tz_convert('UTC')\n",
        "\n",
        "        # Drop rows with NaN values in the external ID field\n",
        "        sfdc_data.dropna(subset=[sfdc_external_id_field], inplace=True)\n",
        "\n",
        "    if not sfdc_data.empty:\n",
        "        print(f\"SFDC Data to process: {len(sfdc_data)}\")\n",
        "    else:\n",
        "        print('No SFDC records survived')\n",
        "\n",
        "    # Merge GBQ data with Salesforce data on the external ID field\n",
        "    if not sfdc_data.empty:\n",
        "        merged_df = pd.merge(gbq_data, sfdc_data, how='left', on=sfdc_external_id_field, suffixes=('', '_sfdc'))\n",
        "\n",
        "        # Filter records to insert (new records)\n",
        "        new_records = merged_df[merged_df['Id'].isnull()].copy()\n",
        "    else:\n",
        "        new_records = gbq_data\n",
        "        print(f\"New Records to process: {len(new_records)}\")\n",
        "\n",
        "    # Filter records to update (existing records)\n",
        "    existing_records = pd.DataFrame()\n",
        "    if not merged_df.empty:\n",
        "        if sfdc_object_name != 'Task':\n",
        "            existing_records = merged_df[(merged_df['Last_Updated__c'] > merged_df['Last_Updated__c_sfdc']) & merged_df['Id'].notnull()].copy()\n",
        "            print(f'Existing records to update: {len(existing_records)}')\n",
        "        else:\n",
        "            existing_records = merged_df[merged_df['Id'].notnull()].copy()\n",
        "            print(f'Existing records to update: {len(existing_records)}')\n",
        "    else:\n",
        "        print('No existing records to update')\n",
        "\n",
        "    # Prepare new records for insertion\n",
        "    if not new_records.empty:\n",
        "        if sfdc_object_name != 'Task':\n",
        "            new_records['Last_Updated__c'] = new_records['Last_Updated__c'].dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
        "        new_records = new_records.drop(columns=[col for col in new_records.columns if col.endswith('_sfdc')] + ['Id'], errors='ignore')\n",
        "        new_records = new_records.to_dict('records')\n",
        "        print(f\"New records to be added: {len(new_records)}\")\n",
        "        # Insert new records to Salesforce\n",
        "        if new_records:\n",
        "            insert_salesforce(sfdc_object_name, new_records)\n",
        "            object_to_sync = sfdc_object_name\n",
        "    else:\n",
        "        new_records = []\n",
        "        print('No new records to be added')\n",
        "\n",
        "    # Update existing records in Salesforce\n",
        "    if not existing_records.empty:\n",
        "        if sfdc_object_name != 'Task':\n",
        "            existing_records['Last_Updated__c'] = existing_records['Last_Updated__c'].dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
        "        existing_records = existing_records.drop(columns=[col for col in existing_records.columns if col.endswith('_sfdc')], errors='ignore')\n",
        "        existing_records = existing_records.to_dict('records')\n",
        "        print(f\"Existing records to be updated: {len(existing_records)}\")\n",
        "        update_salesforce(sfdc_object_name, existing_records)\n",
        "        object_to_sync = sfdc_object_name\n",
        "    else:\n",
        "        existing_records = []\n",
        "        print('No existing records to update')\n",
        "\n",
        "    return object_to_sync\n",
        "\n",
        "\n",
        "# Define a function to sync data\n",
        "def sync_data_ids_tasks(gbq_data, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field):\n",
        "    merged_df = pd.DataFrame()\n",
        "\n",
        "    # Map GBQ fields to Salesforce fields\n",
        "    gbq_data.rename(columns=gbq_to_sfdc_field_map, inplace=True)\n",
        "\n",
        "    # Ensure the external ID field is integer type\n",
        "\n",
        "    gbq_data[sfdc_external_id_field] = gbq_data[sfdc_external_id_field].astype('Float64')\n",
        "    # print(\"GBQ Data:\")\n",
        "    # print(gbq_data.head())\n",
        "    print(f\"GBQ Record Count: {len(gbq_data)}\")\n",
        "    sfdc_data = pd.DataFrame()\n",
        "    # Fetch data from Salesforce\n",
        "    sfdc_fields = list(gbq_to_sfdc_field_map.values())\n",
        "    sfdc_raw = fetch_sfdc_data(sfdc_object_name, sfdc_fields)\n",
        "    if sfdc_raw:\n",
        "        sfdc_data = pd.DataFrame(sfdc_raw)\n",
        "        print(\"SFDC Data (raw):\")\n",
        "    # print(sfdc_data.head())\n",
        "    print(f\"SFDC Record Count: {len(sfdc_data)}\")\n",
        "\n",
        "    # Ensure the external ID field is integer type in Salesforce data\n",
        "    if len(sfdc_data) > 0:\n",
        "\n",
        "        sfdc_data[sfdc_external_id_field] = sfdc_data[sfdc_external_id_field].astype('Float64')\n",
        "\n",
        "        # Drop rows with NaN values in the external ID field\n",
        "        sfdc_data.dropna(subset=[sfdc_external_id_field], inplace=True)\n",
        "    if not sfdc_data.empty:\n",
        "        print(f\"SFDC Data to process: {len(sfdc_data)}\")\n",
        "        # print(sfdc_data.head())\n",
        "    else:\n",
        "        print('No SFDC records survived')\n",
        "\n",
        "    # Merge GBQ data with Salesforce data on the external ID field\n",
        "    if not sfdc_data.empty:\n",
        "        merged_df = pd.merge(gbq_data, sfdc_data, how='left', on=sfdc_external_id_field, suffixes=('', '_sfdc'))\n",
        "\n",
        "        # Filter records to insert (new records)\n",
        "        new_records = merged_df[merged_df['Id'].isnull()].copy()\n",
        "    else:\n",
        "        new_records = gbq_data\n",
        "\n",
        "        print(f\"New Records to process: {len(new_records)}\")\n",
        "    # print(new_records.head())\n",
        "    # print(new_records.dtypes)\n",
        "\n",
        "    # Filter records to update (existing records)\n",
        "    existing_records = pd.DataFrame()\n",
        "    if not merged_df.empty:\n",
        "        existing_records = merged_df[merged_df['Id'].notnull()].copy()\n",
        "        print(f'Existing records to update: {len(existing_records)}')\n",
        "    else:\n",
        "        print('No existing records to update')\n",
        "\n",
        "    # Prepare new records for insertion\n",
        "    if not new_records.empty:\n",
        "        new_records = new_records.drop(columns=[col for col in new_records.columns if col.endswith('_sfdc')] + ['Id'], errors='ignore')\n",
        "        new_records = new_records.to_dict('records')\n",
        "    else:\n",
        "        new_records = []\n",
        "        print('No new records to be added')\n",
        "\n",
        "    # print(new_records)\n",
        "    # Prepare existing records for update\n",
        "    if not existing_records.empty:\n",
        "        update_fields = list(gbq_to_sfdc_field_map.values())\n",
        "        update_fields.remove(sfdc_external_id_field)  # External ID field should not be updated\n",
        "\n",
        "        # Drop _sfdc suffix fields and prepare the final update records\n",
        "        existing_records = existing_records[update_fields + ['Id']].drop(columns=[f'{col}_sfdc' for col in update_fields], errors='ignore')\n",
        "        existing_records = existing_records.to_dict('records')\n",
        "    else:\n",
        "        existing_records = []  # Ensure existing_records is an empty list if no records to update\n",
        "\n",
        "    print(f\"Existing records to be updated: {len(existing_records)}\" if existing_records else \"No existing records to update\")\n",
        "\n",
        "    # Insert new records to Salesforce\n",
        "    if new_records:\n",
        "        insert_salesforce(sfdc_object_name, new_records)\n",
        "\n",
        "    # Update existing records in Salesforce\n",
        "    if existing_records:\n",
        "        update_salesforce(sfdc_object_name, existing_records)\n",
        "\n",
        "# continue from here\n",
        "# Define a function to sync data\n",
        "def sync_data_chars(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field):\n",
        "    merged_df = pd.DataFrame()\n",
        "    # Fetch data from GBQ\n",
        "    gbq_data = fetch_gbq_data(gbq_query)\n",
        "\n",
        "    # Map GBQ fields to Salesforce fields\n",
        "    gbq_data.rename(columns=gbq_to_sfdc_field_map, inplace=True)\n",
        "\n",
        "    # Convert relevant fields to timestamps and fill NA with \"1990-01-01 00:00:00\"\n",
        "    for col in gbq_data.columns:\n",
        "        if col.startswith('Last_Updated__c'):\n",
        "            gbq_data[col] = pd.to_datetime(gbq_data[col], errors='coerce').fillna(pd.Timestamp('1990-01-01 00:00:00'))\n",
        "\n",
        "    # Ensure the external ID field is integer type\n",
        "\n",
        "    gbq_data[sfdc_external_id_field] = gbq_data[sfdc_external_id_field]\n",
        "    print(\"GBQ Data:\")\n",
        "    print(gbq_data.head())\n",
        "\n",
        "    # Fetch data from Salesforce\n",
        "    sfdc_fields = list(gbq_to_sfdc_field_map.values())\n",
        "    sfdc_raw = fetch_sfdc_data(sfdc_object_name, sfdc_fields)\n",
        "    sfdc_data = pd.DataFrame(sfdc_raw)\n",
        "    print(\"SFDC Data (raw):\")\n",
        "    print(sfdc_data.head())\n",
        "\n",
        "    # Ensure the external ID field is integer type in Salesforce data\n",
        "    sfdc_data[sfdc_external_id_field] = sfdc_data[sfdc_external_id_field].astype('Int64')\n",
        "\n",
        "    # Convert relevant fields to timestamps and fill NA with \"1990-01-01 00:00:00\" in Salesforce data\n",
        "    for col in sfdc_data.columns:\n",
        "        if col.startswith('Last_Updated__c'):\n",
        "            sfdc_data[col] = pd.to_datetime(sfdc_data[col], errors='coerce').fillna(pd.Timestamp('1990-01-01 00:00:00'))\n",
        "            # Uncomment following line to force update\n",
        "            # sfdc_data[col] = pd.Timestamp('1990-01-01 00:00:00')\n",
        "\n",
        "    # Drop rows with NaN values in the external ID field\n",
        "    sfdc_data.dropna(subset=[sfdc_external_id_field], inplace=True)\n",
        "    if not sfdc_data.empty:\n",
        "        print(\"SFDC Data (processed):\")\n",
        "        print(sfdc_data.head())\n",
        "    else:\n",
        "        print('No SFDC records survived')\n",
        "\n",
        "    # Merge GBQ data with Salesforce data on the external ID field\n",
        "    if not sfdc_data.empty:\n",
        "        merged_df = pd.merge(gbq_data, sfdc_data, how='left', on=sfdc_external_id_field, suffixes=('', '_sfdc'))\n",
        "        print(\"Merged Data:\")\n",
        "        print(merged_df.head())\n",
        "        print(merged_df.dtypes)\n",
        "\n",
        "        # Filter records to insert (new records)\n",
        "        new_records = merged_df[merged_df['Id'].isnull()].copy()\n",
        "    else:\n",
        "        new_records = gbq_data\n",
        "\n",
        "    print(\"New Records:\")\n",
        "    print(new_records.head())\n",
        "    print(new_records.dtypes)\n",
        "\n",
        "    # Filter records to update (existing records)\n",
        "    existing_records = pd.DataFrame()\n",
        "    if not merged_df.empty:\n",
        "        existing_records = merged_df[(merged_df['Last_Updated__c'] > merged_df['Last_Updated__c_sfdc']) & merged_df['Id'].notnull()].copy()\n",
        "        print(f'Existing records to update: {len(existing_records)}')\n",
        "\n",
        "    # Prepare new records for insertion\n",
        "    if not new_records.empty:\n",
        "        new_records['Last_Updated__c'] = new_records['Last_Updated__c'].dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
        "        new_records = new_records.drop(columns=[col for col in new_records.columns if col.endswith('_sfdc')] + ['Id'], errors='ignore')\n",
        "        new_records = new_records.to_dict('records')\n",
        "        print(f\"New records to be added: {len(new_records)}\")\n",
        "    else:\n",
        "        new_records = []\n",
        "\n",
        "    print(new_records)\n",
        "    # Prepare existing records for update\n",
        "    if not existing_records.empty:\n",
        "        update_fields = list(gbq_to_sfdc_field_map.values())\n",
        "        update_fields.remove(sfdc_external_id_field)  # External ID field should not be updated\n",
        "\n",
        "        # Convert Last_Updated__c field to the desired string format\n",
        "        existing_records['Last_Updated__c'] = existing_records['Last_Updated__c'].dt.strftime('%Y-%m-%d %H:%M:%S.%f').str[:-3]\n",
        "\n",
        "        # Drop _sfdc suffix fields and prepare the final update records\n",
        "        existing_records = existing_records[update_fields + ['Id']].drop(columns=[f'{col}_sfdc' for col in update_fields], errors='ignore')\n",
        "        existing_records = existing_records.to_dict('records')\n",
        "    else:\n",
        "        existing_records = []  # Ensure existing_records is an empty list if no records to update\n",
        "\n",
        "    print(f\"Existing records to be updated: {len(existing_records)}\" if existing_records else \"No existing records to update\")\n",
        "\n",
        "    # Insert new records to Salesforce\n",
        "    if new_records:\n",
        "        insert_salesforce(sfdc_object_name, new_records)\n",
        "\n",
        "    # Update existing records in Salesforce\n",
        "    if existing_records:\n",
        "        update_salesforce(sfdc_object_name, existing_records)\n",
        "\n",
        "def update_bvs_customers_lookup_on_format(sf):\n",
        "    # Query all BVS_Formats__c records without the BVS_Customer__c lookup field set\n",
        "    query = \"SELECT Id, Customer_ID__c FROM BVS_Format__c WHERE BVS_Customer__c = null\"\n",
        "    bvs_format_records = sf.query_all(query)\n",
        "    print(len(bvs_format_records))\n",
        "\n",
        "    for record in bvs_format_records['records']:\n",
        "        bvs_format_record_id = record['Id']\n",
        "        customer_id = record['Customer_ID__c']\n",
        "\n",
        "        # Search for the BVS_Customer__c record with matching customer_id__c\n",
        "        customer_query = f\"SELECT Id FROM BVS_Customer__c WHERE customer_id__c = {customer_id}\"\n",
        "        result = sf.query(customer_query)\n",
        "\n",
        "        if result['totalSize'] == 1:\n",
        "            # If a matching BVS_Customer__c record is found, get its Id\n",
        "            bvs_customer_id = result['records'][0]['Id']\n",
        "\n",
        "            # Update the BVS_Formats__c record with the BVS_Customer lookup field\n",
        "            sf.BVS_Format__c.update(bvs_format_record_id, {'BVS_Customer__c': bvs_customer_id})\n",
        "            print(f\"Updated BVS_Formats__c record {bvs_format_record_id} with BVS_Customer__c {bvs_customer_id}\")\n",
        "        else:\n",
        "            # Handle case where no matching or multiple records are found\n",
        "            print(f\"No matching or multiple BVS_Customer__c records found for Customer_ID__c: {customer_id}\")\n",
        "\n",
        "def update_bvs_profile_lookup_on_format(sf):\n",
        "    # Query all BVS_Formats__c records without the BVS_Customer__c lookup field set\n",
        "    query = \"SELECT Id, Profile_ID__c FROM BVS_Format__c WHERE BVS_Profile__c = null\"\n",
        "    bvs_format_records = sf.query_all(query)\n",
        "    print(len(bvs_format_records))\n",
        "\n",
        "    for record in bvs_format_records['records']:\n",
        "        bvs_format_record_id = record['Id']\n",
        "        profile_id = record['Profile_ID__c']\n",
        "\n",
        "        # Search for the BVS_Profile__c record with matching BVS_Profile_ID__c\n",
        "        profile_query = f\"SELECT Id FROM BVS_Profile__c WHERE BVS_Profile_ID__c = {profile_id}\"\n",
        "        result = sf.query(profile_query)\n",
        "\n",
        "        if result['totalSize'] == 1:\n",
        "            # If a matching BVS_Profile__c record is found, get its Id\n",
        "            bvs_profile_id = result['records'][0]['Id']\n",
        "\n",
        "            # Update the BVS_Formats__c record with the BVS_Profile lookup field\n",
        "            sf.BVS_Format__c.update(bvs_format_record_id, {'BVS_Profile__c': bvs_profile_id})\n",
        "            print(f\"Updated BVS_Formats__c record {bvs_format_record_id} with BVS_Profile__c {bvs_profile_id}\")\n",
        "        else:\n",
        "            # Handle case where no matching or multiple records are found\n",
        "            print(f\"No matching or multiple BVS_Profile__c records found for BVS_Profile_ID__c: {profile_id}\")\n",
        "\n",
        "def sync_data_with_precalculated_id(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field):\n",
        "    # Fetch data from GBQ\n",
        "    object_to_sync = None\n",
        "    gbq_data = fetch_gbq_data(gbq_query)\n",
        "\n",
        "    # Map GBQ fields to Salesforce fields\n",
        "    gbq_data.rename(columns=gbq_to_sfdc_field_map, inplace=True)\n",
        "\n",
        "    # print(\"GBQ Data:\")\n",
        "    # print(gbq_data.head())\n",
        "\n",
        "    # Fetch data from Salesforce\n",
        "    sfdc_fields = list(gbq_to_sfdc_field_map.values())\n",
        "    sfdc_raw = fetch_sfdc_data(sfdc_object_name, sfdc_fields)\n",
        "    sfdc_data = pd.DataFrame(sfdc_raw)\n",
        "    # print(\"SFDC Data (raw):\")\n",
        "    # print(sfdc_data.head())\n",
        "\n",
        "    # Drop rows with NaN values in the external ID field in Salesforce data if data is not empty\n",
        "    if not sfdc_data.empty:\n",
        "        sfdc_data.dropna(subset=[sfdc_external_id_field], inplace=True)\n",
        "        print(\"SFDC Data (processed):\")\n",
        "        # print(sfdc_data.head())\n",
        "    else:\n",
        "        print('No SFDC records survived')\n",
        "\n",
        "    # Merge GBQ data with Salesforce data on the external ID field\n",
        "    if not sfdc_data.empty:\n",
        "        merged_df = pd.merge(gbq_data, sfdc_data, how='left', on=sfdc_external_id_field, suffixes=('', '_sfdc'))\n",
        "        print(\"Merged Data:\")\n",
        "        # print(merged_df.head())\n",
        "        # print(merged_df.dtypes)\n",
        "\n",
        "        # Filter records to insert (new records)\n",
        "        new_records = merged_df[merged_df['Id'].isnull()].copy()\n",
        "    else:\n",
        "        new_records = gbq_data\n",
        "\n",
        "    # print(\"New Records:\")\n",
        "    # print(new_records.head())\n",
        "    # print(new_records.dtypes)\n",
        "\n",
        "    # Filter records to update (existing records)\n",
        "    existing_records = pd.DataFrame()\n",
        "    if not sfdc_data.empty and not merged_df.empty:\n",
        "        existing_records = merged_df[merged_df['Id'].notnull()].copy()\n",
        "        print(f'Existing records to update: {len(existing_records)}')\n",
        "\n",
        "    # Prepare new records for insertion\n",
        "    if not new_records.empty:\n",
        "        new_records = new_records.drop(columns=[col for col in new_records.columns if col.endswith('_sfdc')] + ['Id'], errors='ignore')\n",
        "        new_records = new_records.to_dict('records')\n",
        "        print(f\"New records to be added: {len(new_records)}\")\n",
        "    else:\n",
        "        new_records = []\n",
        "\n",
        "    print(new_records)\n",
        "    # Prepare existing records for update\n",
        "    if not existing_records.empty:\n",
        "        update_fields = list(gbq_to_sfdc_field_map.values())\n",
        "        update_fields.remove(sfdc_external_id_field)  # External ID field should not be updated\n",
        "\n",
        "        # Drop _sfdc suffix fields and prepare the final update records\n",
        "        existing_records = existing_records[update_fields + ['Id']].drop(columns=[f'{col}_sfdc' for col in update_fields], errors='ignore')\n",
        "        existing_records = existing_records.to_dict('records')\n",
        "    else:\n",
        "        existing_records = []  # Ensure existing_records is an empty list if no records to update\n",
        "\n",
        "    print(f\"Existing records to be updated: {len(existing_records)}\" if existing_records else \"No existing records to update\")\n",
        "\n",
        "    # Insert new records to Salesforce\n",
        "    if new_records:\n",
        "        insert_salesforce(sfdc_object_name, new_records)\n",
        "        object_to_sync = sfdc_object_name\n",
        "\n",
        "    # Update existing records in Salesforce\n",
        "    if existing_records:\n",
        "        update_salesforce(sfdc_object_name, existing_records)\n",
        "        object_to_sync = sfdc_object_name\n",
        "    return object_to_sync\n",
        "\n",
        "SHORTIO_API_KEY = userdata.get('SHORTIO_API_KEY')\n",
        "\n",
        "def get_short_url(long_url):\n",
        "    res = requests.post('https://api.short.io/links', json={\n",
        "        'domain': 'link.veil.global',\n",
        "        'originalURL': long_url,\n",
        "    }, headers = {\n",
        "        'authorization': SHORTIO_API_KEY,\n",
        "        'content-type': 'application/json'\n",
        "    }, )\n",
        "\n",
        "    res.raise_for_status()\n",
        "    data = res.json()\n",
        "    short_url = data['shortURL']\n",
        "    return short_url\n",
        "\n",
        "# # Update the BigQuery table\n",
        "# def update_bq_invoice_table_short_url(df, table_id):\n",
        "#     for index, row in df.iterrows():\n",
        "#         query = f\"\"\"\n",
        "#         UPDATE `{table_id}`\n",
        "#         SET excel_path_short = '{row['short_url']}'\n",
        "#         WHERE excel_path = '{row['long_url']}'\n",
        "#         AND excel_path_short IS NULL\n",
        "#         \"\"\"\n",
        "#         query_job = bigquery_client.query(query)\n",
        "#         query_job.result()  # Wait for the query to complete\n",
        "\n",
        "def update_bq_invoice_table_short_url(df, table_id):\n",
        "\n",
        "    # Step 1: Write the DataFrame to a temporary table in BigQuery\n",
        "    temp_table_name = f\"temp_table_{uuid.uuid4().hex}\"\n",
        "    temp_table_id = f\"{project_id}.{dataset_id}.{temp_table_name}\"\n",
        "\n",
        "    # Ensure the temporary dataset exists\n",
        "    dataset_ref = bigquery.DatasetReference(bigquery_client.project, dataset_id)\n",
        "    try:\n",
        "        bigquery_client.get_dataset(dataset_ref)\n",
        "    except Exception:\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        bigquery_client.create_dataset(dataset)\n",
        "\n",
        "    # Load DataFrame to the temporary table\n",
        "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_TRUNCATE')\n",
        "    load_job = bigquery_client.load_table_from_dataframe(df, temp_table_id, job_config=job_config)\n",
        "    load_job.result()  # Wait for the load job to complete\n",
        "\n",
        "    # Step 2: Perform the batch update using a single query\n",
        "    query = f\"\"\"\n",
        "    UPDATE `{table_id}` AS T\n",
        "    SET excel_path_short = S.short_url\n",
        "    FROM `{temp_table_id}` AS S\n",
        "    WHERE T.excel_path = S.long_url\n",
        "    AND T.excel_path_short IS NULL\n",
        "    \"\"\"\n",
        "    query_job = bigquery_client.query(query)\n",
        "    query_job.result()  # Wait for the query to complete\n",
        "\n",
        "    # Step 3: Delete the temporary table\n",
        "    bigquery_client.delete_table(temp_table_id)\n",
        "\n",
        "def update_bq_sfdc_opportunity_id(df, table_id):\n",
        "    dataset_id = 'avs_billing_process'\n",
        "    # Step 1: Write the DataFrame to a temporary table in BigQuery\n",
        "    temp_table_name = f\"temp_table_{uuid.uuid4().hex}\"\n",
        "    temp_table_id = f\"{project_id}.{dataset_id}.{temp_table_name}\"\n",
        "\n",
        "    # Ensure the temporary dataset exists\n",
        "    dataset_ref = bigquery.DatasetReference(bigquery_client.project, dataset_id)\n",
        "    try:\n",
        "        bigquery_client.get_dataset(dataset_ref)\n",
        "    except Exception:\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        bigquery_client.create_dataset(dataset)\n",
        "\n",
        "    # Load DataFrame to the temporary table\n",
        "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_TRUNCATE')\n",
        "    load_job = bigquery_client.load_table_from_dataframe(df, temp_table_id, job_config=job_config)\n",
        "    load_job.result()  # Wait for the load job to complete\n",
        "\n",
        "    # Step 2: Perform the batch update using a single query\n",
        "    query = f\"\"\"\n",
        "    UPDATE `{table_id}` AS T\n",
        "    SET sfdc_opportunity_id = S.sfdc_opportunity_id\n",
        "    FROM `{temp_table_id}` AS S\n",
        "    WHERE T.invoice_id = S.invoice_id\n",
        "    \"\"\"\n",
        "    query_job = bigquery_client.query(query)\n",
        "    query_job.result()  # Wait for the query to complete\n",
        "\n",
        "    # Step 3: Delete the temporary table\n",
        "    bigquery_client.delete_table(temp_table_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Billing Process Functions\n",
        "\n",
        "def update_encodings_base():\n",
        "    query = f\"\"\"\n",
        "\n",
        "\n",
        "    DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{encodings_matched_table_id}`;\n",
        "    CREATE TABLE `{project_id}.{dataset_id}.{encodings_matched_table_id}` AS\n",
        "\n",
        "    (\n",
        "        WITH baseEncodings AS (\n",
        "        SELECT distinct\n",
        "        e.encoding_id, e.format_id, e.encoder_group_id, e.encoded_timestamp,\n",
        "        e.clone_of, e.status, e.last_updated, e.last_audit_id, e.encoder_id, e.detection_end_date\n",
        "        , f.format_name,\n",
        "        -- f.customer_id,\n",
        "        f.profile_id, c.customer_name, p.profile_name,\n",
        "        CASE WHEN (e.attributes.isci is NOT NULL) THEN e.attributes.isci\n",
        "        WHEN (e.attributes.isci is NULL AND e.attributes.project_name is not null) THEN e.attributes.project_name\n",
        "        WHEN (e.attributes.isci is NULL AND e.attributes.project_name is null and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description not like 'TV%' or e.attributes.description not like 'RA%')) THEN trim(substring(e.attributes.description, 9,10))\n",
        "        WHEN (e.attributes.isci is NULL AND e.attributes.project_name is null and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description  like 'TV%' or e.attributes.description  like 'RA%')) THEN trim(substring(e.attributes.description, 19,20))\n",
        "        ELSE NULL END AS isci,\n",
        "\n",
        "        CASE WHEN (e.attributes.product_code is NOT NULL) THEN e.attributes.product_code\n",
        "        WHEN (e.attributes.product_code is NULL AND e.attributes.product_name is NOT NULL) THEN e.attributes.product_name\n",
        "        WHEN (e.attributes.product_code is NULL AND e.attributes.product_name is NULL AND e.attributes.donovan_agency_product_code is NOT NULL) THEN e.attributes.donovan_agency_product_code\n",
        "        WHEN (e.attributes.product_code is NULL AND e.attributes.product_name is NULL AND e.attributes.donovan_agency_product_code is NULL and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description not like 'TV%' or e.attributes.description not like 'RA%')) THEN trim(substring(e.attributes.description, 27,4))\n",
        "        WHEN (e.attributes.product_code is NULL AND e.attributes.product_name is NULL AND e.attributes.donovan_agency_product_code is NULL and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description  like 'TV%' or e.attributes.description  like 'RA%')) THEN trim(substring(e.attributes.description, 7,4))\n",
        "        ELSE NULL END AS product_code,\n",
        "\n",
        "        CASE WHEN (e.attributes.advertiser is NOT NULL) THEN e.attributes.advertiser\n",
        "        WHEN (e.attributes.advertiser is NULL AND e.attributes.client_code is NOT NULL) THEN e.attributes.client_code\n",
        "        WHEN (e.attributes.advertiser is NULL AND e.attributes.client_code is NULL AND e.attributes.donovan_agency_advertiser_code is NOT NULL) THEN e.attributes.donovan_agency_advertiser_code\n",
        "        WHEN (e.attributes.advertiser is NULL AND e.attributes.client_code is NULL AND e.attributes.donovan_agency_advertiser_code is NULL and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description not like 'TV%' or e.attributes.description not like 'RA%')) THEN trim(substring(e.attributes.description, 23,4))\n",
        "        WHEN (e.attributes.advertiser is NULL AND e.attributes.client_code is NULL AND e.attributes.donovan_agency_advertiser_code is NULL and e.attributes.description is not null and length(e.attributes.description) > 10 and (e.attributes.description  like 'TV%' or e.attributes.description  like 'RA%')) THEN trim(substring(e.attributes.description, 3,4))\n",
        "        ELSE NULL END AS advertiser,\n",
        "\n",
        "\n",
        "\n",
        "        sc.Account__c AS sfdc_account_id,\n",
        "        acct.Name AS sfdc_account_name,\n",
        "        f.deleted as format_deleted, c.deleted as customer_deleted, p.deleted as profile_deleted\n",
        "        FROM `{project_id}.{dataset_id}.avs_encodings_master` e\n",
        "        LEFT JOIN `bigquery-sandbox-393916.prod_avs.formats` f\n",
        "        USING (format_id)\n",
        "        LEFT JOIN `bigquery-sandbox-393916.prod_avs.customers` c\n",
        "        USING (customer_id)\n",
        "        LEFT JOIN `bigquery-sandbox-393916.prod_avs.profiles` p\n",
        "        USING (profile_id)\n",
        "        LEFT JOIN `{project_id}.{dataset_id}.sfdc_bvs_customer__c_obj` sc\n",
        "        ON c.customer_id = sc.customer_id__c\n",
        "        LEFT JOIN `{project_id}.{dataset_id}.sfdc_account_obj` acct\n",
        "        ON sc.Account__c = acct.Id\n",
        "        )\n",
        "\n",
        "        select be.*,\n",
        "            cast(null as string) AS sfdc_advertiser_id,\n",
        "            cast(null as string) as sfdc_advertiser_name,\n",
        "            cast(null as string) as sfdc_advertiser_match_type,\n",
        "            cast(null as string) AS sfdc_rate_card_id,\n",
        "            cast(null as string) AS ad_prod_campaign,\n",
        "            -- cast(null as string) AS advertiser_status,\n",
        "            -- cast(null as string) AS billing_frequency,\n",
        "            -- cast(null as string) AS invoice_format,\n",
        "            cast(null as timestamp) AS advertiser_updated,\n",
        "            e.attributes.spot_estimate as spot_estimate,\n",
        "            e.attributes.cable_estimate as cable_estimate,\n",
        "            e.attributes.campaign as campaign,\n",
        "\n",
        "\n",
        "        # e.attributes\n",
        "        from baseEncodings be\n",
        "        join `{project_id}.{dataset_id}.{avs_encodings_master_table_id}` e\n",
        "        USING (encoding_id)\n",
        "    );\n",
        "\n",
        "    UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        set ad_prod_campaign = replace( (concat(trim((advertiser)),'-',trim((product_code)), '-',trim((campaign)))),' ','_')\n",
        "        where ad_prod_campaign is null;\n",
        "\n",
        "    UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv,\n",
        "        UNNEST(SPLIT(adv.Product_Code__c, ',')) AS product_code\n",
        "        WHERE em.product_code = product_code\n",
        "        -- AND em.sfdc_account_id = adv.Account__c\n",
        "        AND adv.Encoding_Format_ID__c is NOT NULL\n",
        "        AND em.format_id = adv.Encoding_Format_ID__c\n",
        "        AND adv.Match_Type__c = 'encoding_product_code_multiple'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "\n",
        "        UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv\n",
        "        WHERE em.product_code = adv.Enc_Product_Code__c\n",
        "        -- AND em.sfdc_account_id = adv.Account__c\n",
        "        AND adv.Encoding_Format_ID__c is NOT NULL\n",
        "        AND em.format_id = adv.Encoding_Format_ID__c\n",
        "        AND adv.Match_Type__c = 'encoding_product_code'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "\n",
        "        UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv\n",
        "        WHERE em.product_code = adv.Enc_Product_Code__c\n",
        "        -- AND em.sfdc_account_id = adv.Account__c\n",
        "        AND adv.Match_Type__c = 'encoding_product_code_ignore_format'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "\n",
        "        UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv\n",
        "        WHERE em.advertiser = adv.Enc_Advertiser__c\n",
        "        -- AND em.sfdc_account_id = adv.Account__c\n",
        "        AND adv.Encoding_Format_ID__c is NOT NULL\n",
        "        AND em.format_id = adv.Encoding_Format_ID__c\n",
        "        AND adv.Match_Type__c = 'encoding_advertiser'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "\n",
        "        UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv\n",
        "        WHERE em.advertiser = adv.Enc_Advertiser__c\n",
        "        -- AND em.sfdc_account_id = adv.Account__c\n",
        "        AND adv.Match_Type__c = 'encoding_advertiser_ignore_format'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "\n",
        "        UPDATE `{project_id}.{dataset_id}.{encodings_matched_table_id}` em\n",
        "        SET em.sfdc_advertiser_id = adv.Id,\n",
        "        em.sfdc_advertiser_name = adv.Name,\n",
        "        em.sfdc_advertiser_match_type = adv.Match_Type__c,\n",
        "        em.sfdc_rate_card_id = adv.Related_Rate_Card__c,\n",
        "        -- em.advertiser_status = adv.Status__c,\n",
        "        -- em.billing_frequency = adv.Billing_Frequency__c,\n",
        "        -- em.invoice_format = adv.Invoice_Format__c,\n",
        "        em.advertiser_updated = current_timestamp()\n",
        "        FROM `{project_id}.{dataset_id}.sfdc_advertiser__c_obj` adv\n",
        "        WHERE\n",
        "        -- em.sfdc_account_id = adv.Account__c\n",
        "        -- AND\n",
        "        em.format_id  = adv.Encoding_Format_ID__c\n",
        "        AND adv.Match_Type__c = 'encoding_format'\n",
        "        AND  coalesce(em.sfdc_advertiser_id, '') = ''\n",
        "        ;\n",
        "        DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{encodings_matched_with_rates_table_id}`;\n",
        "        CREATE TABLE `{project_id}.{dataset_id}.{encodings_matched_with_rates_table_id}` AS (\n",
        "        select * from `{project_id}.{dataset_id}.{encodings_matched_table_id}`\n",
        "        left join `{project_id}.{dataset_id}.sfdc_advertiser_rate_card_prepped`\n",
        "        using (sfdc_account_id, sfdc_advertiser_id, sfdc_rate_card_id)\n",
        "\n",
        "        )\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    query_job.result()\n",
        "\n",
        "    print(f\"Encodings have been rebuilt\")\n",
        "\n",
        "def get_unmatched_advertisers():\n",
        "    query = f\"\"\"\n",
        "    select * FROM `{project_id}.{dataset_id}.billing_unmatched_advertisers_formats`\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish encodings_matched_with_rates\n",
        "    query_job.result()\n",
        "\n",
        "    # Get the results as a DataFrame\n",
        "    df2 = query_job.to_dataframe()\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(f\"Unmatched Advertisers: {len(df2)}\")\n",
        "    print(df2.head(len(df2)))\n",
        "    return\n",
        "\n",
        "def get_unmatched_accounts():\n",
        "    query = f\"\"\"\n",
        "        DECLARE missing_customer_ids ARRAY<INT64>;\n",
        "        DECLARE missing_format_ids ARRAY<INT64>;\n",
        "        DECLARE null_string STRING DEFAULT NULL;\n",
        "        DECLARE null_int INT64 DEFAULT NULL;\n",
        "        DECLARE null_datetime DATETIME DEFAULT NULL;\n",
        "\n",
        "        SET missing_customer_ids = (\n",
        "        SELECT ARRAY_AGG(customer_id)\n",
        "        FROM (\n",
        "            SELECT customer_id\n",
        "            FROM `{project_id}.{dataset_id}.sfdc_bvs_customer__c_obj` AS sfdc\n",
        "            RIGHT JOIN `bigquery-sandbox-393916.prod_avs.customers` AS prod\n",
        "            ON sfdc.customer_id__c = prod.customer_id\n",
        "            WHERE sfdc.customer_id__c IS NULL\n",
        "        )\n",
        "        );\n",
        "\n",
        "        select * from `bigquery-sandbox-393916.prod_avs.customers`\n",
        "        where customer_id in (SELECT *\n",
        "        FROM UNNEST(missing_customer_ids));\n",
        "\n",
        "        SET missing_format_ids = (\n",
        "        SELECT ARRAY_AGG(format_id)\n",
        "        FROM (\n",
        "            SELECT format_id\n",
        "            FROM `bigquery-sandbox-393916.prod_avs.formats`\n",
        "            WHERE customer_id IN (SELECT *\n",
        "        FROM UNNEST(missing_customer_ids))\n",
        "        )\n",
        "        );\n",
        "\n",
        "        DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{encodings_unmatched_table_id}`;\n",
        "        CREATE TABLE `{project_id}.{dataset_id}.{encodings_unmatched_table_id}` AS\n",
        "        (\n",
        "        SELECT e.*, f.format_name, f.customer_id, f.profile_id, c.customer_name, p.profile_name, null_string AS sfdc_account_id, null_string AS sfdc_account_name , null_string AS sfdc_advertiser_id, null_string as sfdc_advertiser_name, null_string as sfdc_advertiser_match_type, null_string AS sfdc_rate_card_id, null_string AS advertiser_status, null_string AS billing_frequency, null_string AS invoice_format, null_datetime AS last_updated2\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.encodings` e\n",
        "        JOIN `bigquery-sandbox-393916.prod_avs.formats` f\n",
        "        USING (format_id)\n",
        "        JOIN `bigquery-sandbox-393916.prod_avs.customers` c\n",
        "        USING (customer_id)\n",
        "        JOIN `bigquery-sandbox-393916.prod_avs.profiles` p\n",
        "        USING (profile_id)\n",
        "        WHERE format_id in (SELECT * FROM UNNEST(missing_format_ids)))\n",
        "\n",
        "        ;\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    select * FROM `{project_id}.{dataset_id}.{encodings_unmatched_table_id}`\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    query_job.result()\n",
        "\n",
        "    # Get the results as a DataFrame\n",
        "    df3 = query_job.to_dataframe()\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(f\"Unmatched Accounts: {len(df3)}\")\n",
        "    print(df3.head(len(df3)))\n",
        "    return\n",
        "\n",
        "def load_detections_into_archive(bcm_index):\n",
        "    query = f\"\"\"\n",
        "        DECLARE bcm_value FLOAT64 DEFAULT {bcm_index};\n",
        "\n",
        "        DECLARE rebuild_archives BOOL DEFAULT {rebuild_detection_archive};\n",
        "\n",
        "\n",
        "        DELETE FROM `{project_id}.{dataset_id}.{billing_records_archive_table_id}`\n",
        "        WHERE bcm_index = bcm_value;\n",
        "        -- DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{billing_records_archive_table_id}`;\n",
        "        INSERT INTO `{project_id}.{dataset_id}.{billing_records_archive_table_id}`\n",
        "        -- CREATE TABLE `{project_id}.{dataset_id}.{billing_records_archive_table_id}` AS\n",
        "        (select *\n",
        "        from `{project_id}.{dataset_id}.{detections_detail_table_id}`\n",
        "        join `{project_id}.{dataset_id}.{encodings_matched_with_rates_table_id}`\n",
        "        on detection_encoding_id = encoding_id\n",
        "        WHERE\n",
        "        bcm_index = bcm_value)\n",
        "        ;\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    query_job.result()\n",
        "\n",
        "    query = f\"\"\"\n",
        "        DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{billing_records__working_table_id}`;\n",
        "\n",
        "        CREATE TABLE `{project_id}.{dataset_id}.{billing_records__working_table_id}` AS\n",
        "        (\n",
        "        SELECT * FROM `{project_id}.{dataset_id}.{billing_records_archive_table_id}`\n",
        "        WHERE\n",
        "        bcm_index = {bcm_index});\n",
        "        \"\"\"\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    query_job.result()\n",
        "    print(f\"Billing records have been archived\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "        DECLARE bcm_value FLOAT64 DEFAULT {bcm_index};\n",
        "\n",
        "        DECLARE rebuild_archives BOOL DEFAULT {rebuild_detection_archive};\n",
        "\n",
        "\n",
        "        DELETE FROM `{project_id}.{dataset_id}.__detections_billing_summary`\n",
        "        WHERE bcm_index = bcm_value;\n",
        "        -- DROP TABLE IF EXISTS `{project_id}.{dataset_id}.{billing_records_archive_table_id}`;\n",
        "        INSERT INTO `{project_id}.{dataset_id}.__detections_billing_summary`\n",
        "        -- CREATE TABLE `{project_id}.{dataset_id}.{billing_records_archive_table_id}` AS\n",
        "        # (CREATE TABLE `adhoc-billing.avs_billing_process.__detections_billing_summary` AS\n",
        "        # (\n",
        "        (with modusAndChief AS (\n",
        "        select encoding_id as detection_encoding_id, sfdc_account_name, sfdc_advertiser_name, format_name from `adhoc-billing.avs_billing_process.encodings_matched`\n",
        "        where format_deleted is FALSE\n",
        "        and customer_deleted is FALSE\n",
        "        and profile_deleted is FALSE\n",
        "        ),\n",
        "        directvDish as (select broadcaster_id from `bigquery-sandbox-393916.mongo.master_channels` where dma_id in (select dma_id from `bigquery-sandbox-393916.mongo.dmas` where name in ('DIRECTV', 'DISH'))),\n",
        "\n",
        "        totals AS (\n",
        "        select bcm_index, sfdc_account_name, sfdc_advertiser_name, format_name, count(occurrence_id) as gross_detections,\n",
        "        case when (any_value(d.broadcaster_id) is null) then FALSE ELSE TRUE END AS is_directv_dish,\n",
        "        from `adhoc-billing.avs_billing_process.detections_detail`\n",
        "        join modusAndChief using (detection_encoding_id)\n",
        "        left join directvDish d using (broadcaster_id)\n",
        "        group by bcm_index, sfdc_account_name, sfdc_advertiser_name, format_name\n",
        "        order by bcm_index, sfdc_account_name, sfdc_advertiser_name, format_name\n",
        "        )\n",
        "        select *\n",
        "        , sum(gross_detections) over (partition by sfdc_advertiser_name) as advertiser_total_gross_detections,\n",
        "        sum(gross_detections) over (partition by format_name) as format_total_gross_detections,\n",
        "        sum(gross_detections) over (partition by sfdc_account_name) as account_total_gross_detections,\n",
        "        sum(case when (is_directv_dish is FALSE) then gross_detections else 0 end) over (partition by sfdc_advertiser_name) as advertiser_total_gross_detections_no_directv_dish,\n",
        "        sum(case when (is_directv_dish is FALSE) then gross_detections else 0 end) over (partition by format_name) as format_total_gross_detections_no_directv_dish,\n",
        "        sum(case when (is_directv_dish is FALSE) then gross_detections else 0 end) over (partition by sfdc_account_name) as account_total_gross_detections_no_directv_dish\n",
        "        from totals\n",
        "        where bcm_index = bcm_value\n",
        "        )\n",
        "        ;\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    query_job = bigquery_client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    query_job.result()\n",
        "\n",
        "# Function to generate a signed URL for GCS object\n",
        "def generate_signed_url(bucket_name, blob_name, expiration=30):\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    url = blob.generate_signed_url(\n",
        "        expiration=datetime.utcnow() + timedelta(days=expiration),\n",
        "        method=\"GET\"\n",
        "    )\n",
        "    return url\n",
        "\n",
        "def upsert_invoice_record(invoice_id, file_url, bcm_index, sfdc_account_id, sfdc_advertiser_id, billing_type):\n",
        "    table_id = f\"{project_id}.{dataset_id}.{billing_invoice_files_table_id}\"\n",
        "\n",
        "    # Define the SQL query for the upsert (MERGE)\n",
        "    query = f\"\"\"\n",
        "    MERGE `{table_id}` AS target\n",
        "    USING (\n",
        "        SELECT\n",
        "            @invoice_id AS invoice_id,\n",
        "            @file_url AS excel_path,\n",
        "            @bcm_index AS bcm_index,\n",
        "            @sfdc_account_id AS sfdc_account_id,\n",
        "            @sfdc_advertiser_id AS sfdc_advertiser_id,\n",
        "            @billing_type AS billing_type\n",
        "    ) AS source\n",
        "    ON target.invoice_id = source.invoice_id\n",
        "    WHEN MATCHED THEN\n",
        "        UPDATE SET\n",
        "            excel_path = source.excel_path,\n",
        "            bcm_index = source.bcm_index,\n",
        "            sfdc_account_id = source.sfdc_account_id,\n",
        "            sfdc_advertiser_id = source.sfdc_advertiser_id,\n",
        "            billing_type = source.billing_type\n",
        "    WHEN NOT MATCHED THEN\n",
        "        INSERT (invoice_id, excel_path, bcm_index, sfdc_account_id, sfdc_advertiser_id, billing_type)\n",
        "        VALUES (source.invoice_id, source.excel_path, source.bcm_index, source.sfdc_account_id, source.sfdc_advertiser_id, source.billing_type)\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the query parameters\n",
        "    job_config = bigquery.QueryJobConfig(\n",
        "        query_parameters=[\n",
        "            bigquery.ScalarQueryParameter(\"invoice_id\", \"STRING\", str(invoice_id)),\n",
        "            bigquery.ScalarQueryParameter(\"file_url\", \"STRING\", file_url),\n",
        "            bigquery.ScalarQueryParameter(\"bcm_index\", \"STRING\", bcm_index),\n",
        "            bigquery.ScalarQueryParameter(\"sfdc_account_id\", \"STRING\", sfdc_account_id),\n",
        "            bigquery.ScalarQueryParameter(\"sfdc_advertiser_id\", \"STRING\", sfdc_advertiser_id),\n",
        "            bigquery.ScalarQueryParameter(\"billing_type\", \"STRING\", billing_type),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Run the query\n",
        "    query_job = bigquery_client.query(query, job_config=job_config)\n",
        "\n",
        "    # Wait for the query to complete\n",
        "    query_job.result()\n",
        "\n",
        "    print(f\"Successfully upserted invoice {invoice_id} with URL {file_url}\")\n",
        "\n",
        "def upload_dataframe_to_bigquery(df, table_id, project_id):\n",
        "    # client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Step 1: Check if the table exists\n",
        "    try:\n",
        "        table = bigquery_client.get_table(table_id)\n",
        "        table_exists = True\n",
        "        print(f\"Table {table_id} exists.\")\n",
        "    except NotFound:\n",
        "        table_exists = False\n",
        "        print(f\"Table {table_id} does not exist. It will be created.\")\n",
        "\n",
        "    if not table_exists:\n",
        "        # Define the schema based on the DataFrame's dtypes\n",
        "        schema = []\n",
        "        for column in df.columns:\n",
        "            df[column] = df[column].astype(str)\n",
        "            field_type = 'STRING'\n",
        "            schema.append(bigquery.SchemaField(column, field_type))\n",
        "\n",
        "        # Create the table\n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        table = bigquery_client.create_table(table)\n",
        "        print(f\"Created table {table_id} with schema.\")\n",
        "\n",
        "    # Step 2: Delete existing rows with matching invoice IDs\n",
        "    invoice_ids = df['invoice_id'].dropna().unique().tolist()\n",
        "    if invoice_ids:\n",
        "        # Prepare invoice IDs for SQL query\n",
        "        invoice_ids_str = ', '.join([f\"'{str(i)}'\" for i in invoice_ids])\n",
        "\n",
        "        delete_query = f\"\"\"\n",
        "        DELETE FROM `{table_id}`\n",
        "        WHERE invoice_id IN ({invoice_ids_str})\n",
        "        \"\"\"\n",
        "        query_job = bigquery_client.query(delete_query)\n",
        "        query_job.result()\n",
        "        print(f\"Deleted existing rows with invoice_id in {invoice_ids}.\")\n",
        "    else:\n",
        "        print(\"No invoice_id values found in DataFrame to delete.\")\n",
        "\n",
        "    # Step 3: Insert new data into the table\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND\n",
        "    )\n",
        "\n",
        "    load_job = bigquery_client.load_table_from_dataframe(\n",
        "        df, table_id, job_config=job_config\n",
        "    )\n",
        "    load_job.result()\n",
        "    print(f\"Inserted {len(df)} rows into {table_id}.\")\n",
        "\n",
        "def generate_encoder_excel(bcm_index):\n",
        "    sfdc_opp_summary_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "    sfdc_opp_detail_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "\n",
        "    # set key vars\n",
        "    invoice_target = \"encoders\"\n",
        "    invoice_date = datetime.now()\n",
        "    formatted_invoice_date = invoice_date.strftime(\"%-m/%-d/%Y\")\n",
        "    summary_group_sort_by = ['invoice_id', 'billing_month', 'encoder_device_location', 'encoder_device_name']\n",
        "    summary_columns = ['encoder_device_location', 'encoder_device_name', 'li_label', 'li_quantity', 'li_price', 'li_total']\n",
        "    detail_group_sort_by = ['invoice_id', 'billing_month', 'encoder_device_location', 'encoder_device_name', 'encoding_id']\n",
        "    detail_columns = ['encoder_device_location', 'encoder_device_name', 'encoding_id', 'isci', 'encoded_for', 'encoded_timestamp']\n",
        "\n",
        "    date_columns = ['bcw_start_date_display', 'bcw_end_date_display']\n",
        "    date_time_columns = ['encoded_timestamp']\n",
        "    currency_columns = ['li_price', 'li_total', 'invoice_total']\n",
        "\n",
        "    # Define custom column headers\n",
        "    custom_summary_headers = {\n",
        "        'encoder_device_location': 'Encoder Location',\n",
        "        'encoder_device_name': 'Encoder Name',\n",
        "        'li_label': 'Item',\n",
        "        'li_quantity': 'Quantity',\n",
        "        'li_price': 'Rate',\n",
        "        'li_total': 'Total',\n",
        "        'invoice_total': 'Invoice Total'\n",
        "    }\n",
        "\n",
        "    custom_detail_headers = {\n",
        "        'encoder_device_location': 'Encoder Location',\n",
        "        'encoder_device_name': 'Encoder Name',\n",
        "        'encoding_id': 'Encoding ID',\n",
        "        'isci': 'ISCI',\n",
        "        'encoded_for': 'Encoded For',\n",
        "        'encoded_timestamp': 'Encoded Timestamp UTC'\n",
        "    }\n",
        "\n",
        "\n",
        "    # Extract year and month from bcm_index\n",
        "    year = int(bcm_index)\n",
        "    # Multiply fractional part by 100 and round to get the month\n",
        "    month = int(round((bcm_index - year) * 100))\n",
        "\n",
        "    # Ensure month is between 1 and 12\n",
        "    if not 1 <= month <= 12:\n",
        "        raise ValueError(\"Invalid month extracted from bcm_index.\")\n",
        "\n",
        "    # Convert to 'YYYY-MM' format\n",
        "    bcm_index_str = f\"{year}-{month:02d}\"\n",
        "\n",
        "    # Calculate the first encoding date\n",
        "    first_encoding_date = f\"{bcm_index_str}-01\"\n",
        "    first_month_date = f\"{bcm_index_str}-01\"\n",
        "    last_encoding_date = billing_end_date\n",
        "\n",
        "    # Get the number of days in the month\n",
        "    num_days = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    # Calculate the last encoding date\n",
        "    last_encoding_date = f\"{bcm_index_str}-{num_days:02d}\"\n",
        "\n",
        "\n",
        "    # Convert last_encoding_date to a date object\n",
        "    last_encoding_date_obj = datetime.strptime(last_encoding_date, '%Y-%m-%d').date()\n",
        "\n",
        "    # Get today's date (using the current date from the initial prompt)\n",
        "\n",
        "    if today > last_encoding_date_obj:\n",
        "        print(\"Today's date is after the last day of the bcm_index month.\")\n",
        "\n",
        "\n",
        "        # Query the summary and detail views\n",
        "        processed_query = f\"\"\"\n",
        "        SELECT * FROM `{project_id}.{dataset_id}.avs_billing_encodings_prepped`\n",
        "        \"\"\"\n",
        "        # summary_query = \"\"\"\n",
        "        # SELECT * FROM `adhoc-billing.avs_billing_process._encoder_billing__line_items_summary`\n",
        "        # \"\"\"\n",
        "        # detail_query = \"\"\"\n",
        "        # SELECT * FROM `adhoc-billing.avs_billing_process._encoder_billing__line_items_detail`\n",
        "        # \"\"\"\n",
        "\n",
        "        # Load data into DataFrames\n",
        "        processed_df = bigquery_client.query(processed_query).to_dataframe()\n",
        "        processed_df['encoded_timestamp'] = pd.to_datetime(processed_df['encoded_timestamp']).dt.tz_localize(None)\n",
        "        processed_df = processed_df.sort_values(by=detail_group_sort_by)\n",
        "        # summary_df = bigquery_client.query(summary_query).to_dataframe()\n",
        "        # detail_df = bigquery_client.query(detail_query).to_dataframe()\n",
        "\n",
        "\n",
        "        # summary_df = pd.DataFrame(columns=summary_columns)\n",
        "        # detail_df = pd.DataFrame(columns=detail_columns)\n",
        "\n",
        "        # Assuming summary_columns is a list of column names\n",
        "        mask = processed_df['sfdc_account_id'] == '0013h00001Vc5SCAAZ'\n",
        "        er_summary_df = processed_df[mask].drop_duplicates(subset=['invoice_id', 'encoder_id']).copy()\n",
        "        non_er_summary_df = processed_df[~mask].drop_duplicates(subset=['invoice_id', 'li_label']).dropna(subset=['invoice_total']).copy()\n",
        "        summary_df = pd.concat([er_summary_df, non_er_summary_df])\n",
        "        summary_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # summary_df.columns.tolist()\n",
        "        summary_df[['invoice_id', 'invoice_total_raw', 'min_value']].drop_duplicates()\n",
        "        summary_df[['sfdc_account_name','encoder_type','encoder_device_name','encoder_device_location']].drop_duplicates()\n",
        "        summary_df['broadcast_month_name'] = pd.to_datetime(summary_df['billing_month'], format='%Y-%m').dt.strftime('%B-%Y')\n",
        "        summary_df['op_name'] = summary_df.apply(\n",
        "            lambda row: f\"{row['sfdc_account_name']} - {row['broadcast_month_name']} ({billing_start_date} - {billing_end_date})\",\n",
        "            axis=1\n",
        "        )\n",
        "        summary_df['ownerId'] = opp_owner_id\n",
        "        summary_df['closeDate'] = today.strftime('%Y-%m-%d')\n",
        "        summary_df['poNumber'] = ''\n",
        "        summary_df['stageName'] = opp_stage_name\n",
        "        summary_df['orderdate'] = today.strftime('%Y-%m-%d')\n",
        "        summary_df['billing_start_date'] = billing_start_date\n",
        "        summary_df['billing_end_date'] = billing_end_date\n",
        "        summary_df['type'] = opp_type\n",
        "        summary_df['BVSInvoiceTerms'] = summary_df['description']\n",
        "        summary_df['SOSalesRep'] = ''\n",
        "        summary_df['PriceBook2Id'] = opp_pricebook_id\n",
        "        summary_df['RecordTypeId'] = opp_record_type_id\n",
        "        summary_df['invoice_short_url'] = ''\n",
        "        summary_df['sfdc_opportunity_id'] = ''\n",
        "        summary_df['sfdc_advertiser_id'] = ''\n",
        "        summary_df['poNumber'] = ''\n",
        "        summary_df['reporting_cust_name'] = summary_df['sfdc_account_name']\n",
        "        summary_df['reporting_adv_name'] = ''\n",
        "        summary_df['LineDesc'] = summary_df.apply(\n",
        "            lambda row: f\"{row['encoder_type']} : {row['encoder_device_name']} : {row['encoder_device_location']} - {row['li_label']}\",\n",
        "            axis=1\n",
        "        )\n",
        "        # summary_df['li_label']\n",
        "        summary_df['Product2Id'] = summary_df['sfdc_product_id']\n",
        "        summary_df['ProductCode'] = summary_df['sfdc_product_code']\n",
        "        summary_df['quantity'] = summary_df['li_quantity']\n",
        "        summary_df['price'] = summary_df['li_price']\n",
        "\n",
        "        # Assuming detail_columns is a list of column names\n",
        "\n",
        "        er_detail_df = processed_df[mask].drop_duplicates(subset=['invoice_id', 'encoder_id', 'encoding_id', 'isci']).copy()\n",
        "        non_er_detail_df = processed_df[~mask].drop_duplicates(subset=['invoice_id', 'li_label', 'encoding_id', 'isci']).dropna(subset=['invoice_total']).copy()\n",
        "        detail_df = pd.concat([er_detail_df, non_er_detail_df])\n",
        "        detail_df.reset_index(drop=True, inplace=True)\n",
        "        invoice_ids = summary_df['invoice_id'].unique()\n",
        "\n",
        "        # sfdc_opp_detail_columns = ['invoice_id',  'sfdc_opportunity_id', 'broadcast_month_name', 'LineDesc', 'Product2Id', 'ProductCode', 'quantity', 'price', 'reporting_cust_name', 'reporting_adv_name']\n",
        "\n",
        "        # Process each invoice_id\n",
        "        for invoice_id in invoice_ids:\n",
        "            # Filter summary and detail for the current invoice_id\n",
        "            summary_group = summary_df[summary_df['invoice_id'] == invoice_id]\n",
        "            detail_group = detail_df[detail_df['invoice_id'] == invoice_id]\n",
        "\n",
        "            # Sort summary and detail as needed\n",
        "            summary_group = summary_group.sort_values(by=summary_group_sort_by)\n",
        "            if summary_group['min_value'].iloc[0]:\n",
        "                if summary_group['min_max_applied_at'].iloc[0] == 'account':\n",
        "                    if summary_group['invoice_total_raw'].iloc[0] < summary_group['min_value'].iloc[0]:\n",
        "                        summary_group_min_add_df = pd.DataFrame(columns=summary_columns)\n",
        "                        summary_group_min_add_df = summary_group.iloc[0:1].copy()\n",
        "                        summary_group_min_add_df['li_label'] = 'Monthly Minimum Adjustment'\n",
        "                        summary_group_min_add_df['li_quantity'] = 1\n",
        "                        summary_group_min_add_df['li_price'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                        summary_group_min_add_df['li_total'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                        summary_group_min_add_df['LineDesc'] = 'Monthly Minimum Adjustment'\n",
        "                        summary_group_min_add_df['quantity'] = 1\n",
        "                        summary_group_min_add_df['price'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                        summary_group_2 = pd.concat(\n",
        "                            [summary_group, summary_group_min_add_df],\n",
        "                            ignore_index=True\n",
        "                        )\n",
        "                    else:\n",
        "                        summary_group_2 = summary_group.copy()\n",
        "                else:\n",
        "                    summary_group_2 = summary_group.copy()\n",
        "            else:\n",
        "                if summary_group['max_value'].iloc[0]:\n",
        "                    if summary_group['min_max_applied_at'].iloc[0] == 'account':\n",
        "                        if summary_group['invoice_total_raw'].iloc[0] > summary_group['max_value'].iloc[0]:\n",
        "                            summary_group_min_add_df = pd.DataFrame(columns=summary_columns)\n",
        "                            summary_group_min_add_df = summary_group.iloc[0:1].copy()\n",
        "                            summary_group_min_add_df['li_label'] = 'Monthly Maximum Adjustment'\n",
        "                            summary_group_min_add_df['li_quantity'] = 1\n",
        "                            summary_group_min_add_df['li_price'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                            summary_group_min_add_df['li_total'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                            summary_group_min_add_df['LineDesc'] = 'Monthly Maximum Adjustment'\n",
        "                            summary_group_min_add_df['quantity'] = 1\n",
        "                            summary_group_min_add_df['price'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['invoice_total_raw'].iloc[0]\n",
        "                            summary_group_2 = pd.concat(\n",
        "                                [summary_group, summary_group_min_add_df],\n",
        "                                ignore_index=True\n",
        "                            )\n",
        "                        else:\n",
        "                            summary_group_2 = summary_group.copy()\n",
        "                    else:\n",
        "                        summary_group_2 = summary_group.copy()\n",
        "                else:\n",
        "                    summary_group_2 = summary_group.copy()\n",
        "\n",
        "\n",
        "            summary_group_2.reset_index(drop=True, inplace=True)\n",
        "            summary_group_2.columns.to_list()\n",
        "            # summary_group.drop(columns=['invoice_total_raw'], inplace=True)\n",
        "            detail_group = detail_group.sort_values(by=detail_group_sort_by)\n",
        "\n",
        "            # Extract necessary fields for the filename and metadata\n",
        "            billing_month = summary_group_2['billing_month'].iloc[0]\n",
        "            currency = summary_group_2['currency'].iloc[0].upper()  # Convert currency to uppercase for consistency\n",
        "            sfdc_account_name = summary_group_2['sfdc_account_name'].iloc[0]\n",
        "            sfdc_account_id = summary_group_2['sfdc_account_id'].iloc[0]\n",
        "            sfdc_advertiser_id = ''\n",
        "            billing_type = summary_group_2['billing_type'].iloc[0]\n",
        "            sfdc_account_name_safe = re.sub(r'[^\\w\\-.]', '_', sfdc_account_name)\n",
        "            sfdc_account_name_safe = re.sub(r'[|(){}]', '_', sfdc_account_name_safe)\n",
        "            account_csm = summary_group_2['account_csm'].iloc[0]\n",
        "            description = summary_group_2['description'].iloc[0]\n",
        "\n",
        "            # Create an Excel filename\n",
        "            excel_file_name = f'invoice_{billing_month}_{sfdc_account_name_safe}_{invoice_id}.xlsx'\n",
        "\n",
        "            # Add encoder details to sfdc opp df\n",
        "            sfdc_opp_prep_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "            sfdc_opp_prep_df = summary_group_2[sfdc_opp_summary_columns].drop_duplicates(subset=['invoice_id']).copy()\n",
        "            sfdc_opp_summary_df = pd.concat(\n",
        "                [sfdc_opp_summary_df, sfdc_opp_prep_df],\n",
        "                ignore_index=True\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "            # Create a Pandas Excel writer object\n",
        "            with pd.ExcelWriter(excel_file_name, engine='xlsxwriter') as writer:\n",
        "                # Define workbook and format objects\n",
        "                workbook = writer.book\n",
        "                bold_format = workbook.add_format({'bold': True})\n",
        "                bold_right_format = workbook.add_format({'bold': True, 'align': 'right'})\n",
        "                currency_format = workbook.add_format({'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'num_format': 'C$#,##0.00'})\n",
        "                currency_bold_format = workbook.add_format({'bold': True, 'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'bold': True, 'num_format': 'C$#,##0.00'})\n",
        "                date_format = workbook.add_format({'num_format': 'mm/dd/yyyy'})  # Define date format\n",
        "                date_time_format = workbook.add_format({'num_format': 'mm/dd/yyyy HH:mm:ss'})\n",
        "\n",
        "\n",
        "                # Write the first 4 lines with invoice_id, sfdc_account_name, invoice date, and billing month\n",
        "                summary_sheet = workbook.add_worksheet('Summary')\n",
        "                detail_sheet = workbook.add_worksheet('Details')\n",
        "\n",
        "                summary_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "                detail_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "\n",
        "\n",
        "                headers = [\n",
        "                    ('Invoice ID:', invoice_id),\n",
        "                    ('Invoice date:', formatted_invoice_date),\n",
        "                    ('Account:', sfdc_account_name),\n",
        "                    ('Billing Month:', billing_month),\n",
        "                    ('Description:', description)\n",
        "                ]\n",
        "\n",
        "                for i, (label, value) in enumerate(headers, start=3):\n",
        "                    summary_sheet.write(f'A{i}', label, bold_format)\n",
        "                    summary_sheet.write(f'B{i}', value)\n",
        "                    detail_sheet.write(f'A{i}', label, bold_format)\n",
        "                    detail_sheet.write(f'B{i}', value)\n",
        "\n",
        "                # Write the custom headers in row 8\n",
        "                for col_num, column in enumerate(summary_columns):\n",
        "                    custom_header = custom_summary_headers.get(column, column)\n",
        "                    summary_sheet.write(8, col_num, custom_header, bold_format)\n",
        "\n",
        "                for col_num, column in enumerate(detail_columns):\n",
        "                    custom_header = custom_detail_headers.get(column, column)\n",
        "                    detail_sheet.write(8, col_num, custom_header, bold_format)\n",
        "\n",
        "                # Start writing data from row 9 (resetting the row number for each new sheet)\n",
        "                summary_start_row = 9\n",
        "                detail_start_row = 9\n",
        "\n",
        "                # Write summary data\n",
        "                for row_num, row_data in enumerate(summary_group_2.itertuples(index=False), start=summary_start_row):\n",
        "                    for col_num, column in enumerate(summary_columns):\n",
        "                        value = getattr(row_data, column)\n",
        "                        cell_format = (\n",
        "                            currency_format if column in currency_columns else\n",
        "                            date_format if column in date_columns else\n",
        "                            date_time_format if column in date_time_columns else None\n",
        "                        )\n",
        "                        summary_sheet.write(row_num, col_num, value, cell_format)\n",
        "                    sfdc_opp_detail_prep_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "                    sfdc_opp_detail_prep_df = summary_group_2[sfdc_opp_detail_columns].drop_duplicates(subset=['invoice_id','LineDesc']).copy()\n",
        "                    sfdc_opp_detail_df = pd.concat(\n",
        "                        [sfdc_opp_detail_df, sfdc_opp_detail_prep_df],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "\n",
        "                # Write Grand Total\n",
        "                grand_total_label_row = summary_start_row + len(summary_group_2)\n",
        "                summary_sheet.write(grand_total_label_row, len(summary_columns) - 4, 'Grand Total:', bold_right_format)\n",
        "                summary_sheet.write(grand_total_label_row, len(summary_columns) - 3, summary_group_2['invoice_quantity'].iloc[0], bold_right_format)\n",
        "                summary_sheet.write(grand_total_label_row, len(summary_columns) - 1, summary_group_2['invoice_total'].iloc[0], currency_bold_format)\n",
        "\n",
        "                # Write detail data\n",
        "                for row_num, row_data in enumerate(detail_group.itertuples(index=False), start=detail_start_row):\n",
        "                    for col_num, column in enumerate(detail_columns):\n",
        "                        value = getattr(row_data, column)\n",
        "                        cell_format = (\n",
        "                            currency_format if column in currency_columns else\n",
        "                            date_format if column in date_columns else\n",
        "                            date_time_format if column in date_time_columns else None\n",
        "                        )\n",
        "                        detail_sheet.write(row_num, col_num, value, cell_format)\n",
        "\n",
        "                # Auto-fit columns to content starting from A2\n",
        "                for col_num, column in enumerate(summary_columns):\n",
        "                    max_length = max(\n",
        "                        summary_group[column].astype(str).map(len).max(),\n",
        "                        len(custom_summary_headers.get(column, column))\n",
        "                    ) + 2\n",
        "                    summary_sheet.set_column(col_num, col_num, max_length)\n",
        "\n",
        "                for col_num, column in enumerate(detail_columns):\n",
        "                    max_length = max(\n",
        "                        detail_group[column].astype(str).map(len).max(),\n",
        "                        len(custom_detail_headers.get(column, column))\n",
        "                    ) + 2\n",
        "                    detail_sheet.set_column(col_num, col_num, max_length)\n",
        "\n",
        "\n",
        "            # Upload the file to Google Cloud Storage\n",
        "            blob_path = f'invoices/{billing_month}/{invoice_target}/{excel_file_name}'\n",
        "            blob_path2 = f'invoices/{billing_month}/for_csms/{account_csm}/{invoice_target}/{excel_file_name}'\n",
        "            blob = bucket.blob(blob_path)\n",
        "            blob.upload_from_filename(excel_file_name)\n",
        "            blob = bucket.blob(blob_path2)\n",
        "            blob.upload_from_filename(excel_file_name)\n",
        "\n",
        "            # Generate signed URL for public access\n",
        "            file_url = generate_signed_url(bucket_name, blob_path, expiration=30)\n",
        "\n",
        "            # Upsert the record into BigQuery\n",
        "            upsert_invoice_record(invoice_id, file_url, bcm_index, sfdc_account_id, sfdc_advertiser_id, billing_type)\n",
        "\n",
        "            # Optionally, delete the local file after upload\n",
        "            os.remove(excel_file_name)\n",
        "    else:\n",
        "        print(\"Today's date is before the last day of the bcm_index month.\")\n",
        "\n",
        "    print(\"Encoding billing Excel files generated, uploaded to GCS, and upserted into BigQuery successfully.\")\n",
        "    sfdc_opp_summary_df['invoice_id'] = sfdc_opp_summary_df['invoice_id'].astype(str)\n",
        "\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_summary_df, sfdc_opp_summary_table_id, project_id)\n",
        "    sfdc_opp_detail_df_unique = sfdc_opp_detail_df.drop_duplicates().copy()\n",
        "    sfdc_opp_detail_df_unique['invoice_id'] = sfdc_opp_detail_df_unique['invoice_id'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['quantity'] = sfdc_opp_detail_df_unique['quantity'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['price'] = sfdc_opp_detail_df_unique['price'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['li_total'] = sfdc_opp_detail_df_unique['li_total'].astype(str)\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_detail_df_unique, sfdc_opp_detail_table_id, project_id)\n",
        "\n",
        "def generate_weekly_excel(bcm_index):\n",
        "    # Create DataFrames\n",
        "    sfdc_opp_summary_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "    sfdc_opp_detail_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "\n",
        "    # set key vars\n",
        "    invoice_target = \"weekly\"\n",
        "    invoice_date = datetime.now()\n",
        "    formatted_invoice_date = invoice_date.strftime(\"%-m/%-d/%Y\")\n",
        "    summary_group_sort_by = ['invoice_id','bcw_index', 'category']\n",
        "    summary_columns = ['bcw_index', 'bcw_start_date_display', 'bcw_end_date_display', 'category', 'category_quantity', 'rate',  'category_total']\n",
        "    detail_group_sort_by = ['invoice_id','bcw_index', 'category', 'affiliate', 'callsign']\n",
        "    detail_exclusion_group_sort_by = ['invoice_id','bcw_index', 'category',  'category_orig', 'affiliate', 'callsign']\n",
        "    detail_columns = ['bcw_index', 'bcw_start_date_display', 'bcw_end_date_display', 'category', 'affiliate', 'callsign',  'rate', 'encoding_id', 'isci','spot_estimate', 'cable_estimate']\n",
        "    detail_exclusion_columns = ['bcw_index', 'bcw_start_date_display', 'bcw_end_date_display', 'exclusion_type', 'exclusion_task_id', 'category', 'category_orig', 'affiliate', 'callsign',  'rate', 'encoding_id', 'isci','spot_estimate', 'cable_estimate']\n",
        "\n",
        "\n",
        "    date_columns = ['bcw_start_date_display', 'bcw_end_date_display']\n",
        "    currency_columns = ['rate', 'li_total', 'invoice_total', 'category_total']\n",
        "\n",
        "    weekly_exclusions_df = pd.DataFrame()\n",
        "    # Define custom column headers\n",
        "    custom_summary_headers = {\n",
        "        'bcw_index': 'Broadcast Week Index',\n",
        "        'bcw_start_date_display': 'Start Date',\n",
        "        'bcw_end_date_display': 'End Date',\n",
        "        'category': 'Category',\n",
        "        'category_quantity': 'Quantity',\n",
        "        'rate': 'Rate',\n",
        "        'category_total': 'Total'\n",
        "    }\n",
        "\n",
        "    custom_detail_headers = {\n",
        "        'bcw_index': 'Broadcast Week Index',\n",
        "        'bcw_start_date_display': 'Start Date',\n",
        "        'bcw_end_date_display': 'End Date',\n",
        "        'category': 'Category',\n",
        "        'affiliate': 'Affiliate',\n",
        "        'callsign': 'Callsign',\n",
        "        'rate': 'Rate',\n",
        "        'encoding_id': 'Encoding ID',\n",
        "        'isci': 'ISCI',\n",
        "        'spot_estimate': 'Spot Estimate',\n",
        "        'cable_estimate': 'Cable Estimate'\n",
        "    }\n",
        "\n",
        "    custom_detail_headers_exclusions = {\n",
        "        'bcw_index': 'Broadcast Week Index',\n",
        "        'bcw_start_date_display': 'Start Date',\n",
        "        'bcw_end_date_display': 'End Date',\n",
        "        'exclusion_type': 'Exclusion Type',\n",
        "        'exclusion_task_id': 'Exclusion Task ID',\n",
        "        'category': 'Category',\n",
        "        'category_orig': 'Original Category',\n",
        "        'affiliate': 'Affiliate',\n",
        "        'callsign': 'Callsign',\n",
        "        'rate': 'Rate',\n",
        "        'encoding_id': 'Encoding ID',\n",
        "        'isci': 'ISCI',\n",
        "        'spot_estimate': 'Spot Estimate',\n",
        "        'cable_estimate': 'Cable Estimate'\n",
        "    }\n",
        "\n",
        "\n",
        "    # Query the summary and detail views\n",
        "    processed_query = f\"\"\"\n",
        "    SELECT * FROM `{project_id}.{dataset_id}._weekly_processed_inclusions`\n",
        "    \"\"\"\n",
        "\n",
        "    # summary_query = \"\"\"\n",
        "    # SELECT * FROM `adhoc-billing.avs_billing_process._weekly_billing_invoice_items_summary`\n",
        "    # \"\"\"\n",
        "    # detail_query = \"\"\"\n",
        "    # SELECT * FROM `adhoc-billing.avs_billing_process._weekly_billing_invoice_items_detail`\n",
        "    # \"\"\"\n",
        "\n",
        "    # Load data into DataFrames\n",
        "    processed_df = bigquery_client.query(processed_query).to_dataframe()\n",
        "    processed_df = processed_df.sort_values(by=detail_group_sort_by)\n",
        "    processed_df['excluded'] = processed_df['excluded'].astype(bool)\n",
        "    excluded_mask = processed_df['excluded'] == True\n",
        "    weekly_exclusions_df = processed_df[excluded_mask].copy()\n",
        "    weekly_exclusions_df['exclusion_type'] = weekly_exclusions_df['exclusion_details'].apply(lambda x: x['exclusion_type'])\n",
        "    weekly_exclusions_df['exclusion_task_id'] = weekly_exclusions_df['exclusion_details'].apply(lambda x: x['exclusion_task_id'])\n",
        "    weekly_exclusions_df.reset_index(drop=True, inplace=True)\n",
        "    weekly_include_df = processed_df[~excluded_mask].copy()\n",
        "    weekly_include_df.reset_index(drop=True, inplace=True)\n",
        "    # if len(weekly_exclusions_df) > 0 :\n",
        "    #     has_exclusions = True\n",
        "    # else:\n",
        "    #     has_exclusions = False\n",
        "    # weekly_exclude_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # weekly_include_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # processed_df.dtypes\n",
        "    # array_columns = [col for col in processed_df.columns if isinstance(processed_df[col].iloc[0], np.ndarray)]\n",
        "    # print(\"Columns containing numpy arrays:\", array_columns)\n",
        "    # for col in ['encodings_array', 'isci_array', 'invoice_iscis', 'invoice_encodings']:\n",
        "    #     # Apply a function to each column: convert the numpy array to a list of unique values\n",
        "    #     processed_df[col] = processed_df[col].apply(lambda x: list(np.unique(x)))\n",
        "\n",
        "\n",
        "    # list_columns = [col for col in processed_df.columns if isinstance(processed_df[col].iloc[0], list)]\n",
        "    # print(\"Columns containing lists:\", list_columns)\n",
        "    # processed_df['']\n",
        "    # summary_df_orig = bigquery_client.query(summary_query).to_dataframe()\n",
        "    # summary_df.columns.to_list()\n",
        "    # detail_df = bigquery_client.query(detail_query).to_dataframe()\n",
        "    # detail_df.columns.to_list()\n",
        "    # Get unique invoice_ids\n",
        "    invoice_ids = weekly_include_df['invoice_id'].unique()\n",
        "    # summary_df = pd.DataFrame(columns=summary_columns)\n",
        "    # detail_df = pd.DataFrame(columns=detail_columns)\n",
        "    # summary_df = pd.DataFrame(columns=summary_columns)\n",
        "\n",
        "    # Assuming summary_columns is a list of column names\n",
        "    summary_df = weekly_include_df.drop_duplicates(subset=['invoice_id', 'bcw_index', 'category']).copy()\n",
        "    summary_df.reset_index(drop=True, inplace=True)\n",
        "    # summary_df.columns.tolist()\n",
        "    # summary_df['billing_month']\n",
        "    summary_df['broadcast_month_name'] = pd.to_datetime(summary_df['billing_month'], format='%Y-%m').dt.strftime('%B-%Y')\n",
        "    summary_df['op_name'] = summary_df.apply(\n",
        "        lambda row: f\"{row['sfdc_account_name']} - {row['invoice_title']} ({row['bcm_start_date_display']} - {row['bcm_end_date_display']})\",\n",
        "        axis=1\n",
        "    )\n",
        "    summary_df['ownerId'] = opp_owner_id\n",
        "    summary_df['closeDate'] = today.strftime('%Y-%m-%d')\n",
        "    summary_df['poNumber'] = ''\n",
        "    summary_df['stageName'] = opp_stage_name\n",
        "    summary_df['orderdate'] = today.strftime('%Y-%m-%d')\n",
        "    summary_df['billing_start_date'] = billing_start_date\n",
        "    summary_df['billing_end_date'] = billing_end_date\n",
        "    summary_df['type'] = opp_type\n",
        "    summary_df['BVSInvoiceTerms'] = summary_df['description']\n",
        "    summary_df['SOSalesRep'] = ''\n",
        "    summary_df['PriceBook2Id'] = opp_pricebook_id\n",
        "    summary_df['RecordTypeId'] = opp_record_type_id\n",
        "    summary_df['invoice_short_url'] = ''\n",
        "    summary_df['sfdc_opportunity_id'] = ''\n",
        "    # summary_df['sfdc_advertiser_id'] =\n",
        "    summary_df['poNumber'] = ''\n",
        "    summary_df['reporting_cust_name'] = summary_df['sfdc_account_name']\n",
        "    summary_df['reporting_adv_name'] = summary_df['sfdc_advertiser_id']\n",
        "    summary_df['LineDesc'] = summary_df.apply(\n",
        "        lambda row: f\"{row['bcw_index']} : {row['bcw_start_date_display']} - {row['bcw_end_date_display']} : {row['category']}\",\n",
        "        axis=1\n",
        "    )\n",
        "    # summary_df['li_label']\n",
        "    summary_df['Product2Id'] = summary_df['sfdc_product_id']\n",
        "    summary_df['ProductCode'] = summary_df['sfdc_product_code']\n",
        "    summary_df['quantity'] = summary_df['category_quantity']\n",
        "    summary_df['price'] = summary_df['rate']\n",
        "\n",
        "    # summary_df[['category','quantity', 'rate', 'category_quantity', 'category_total']]\n",
        "\n",
        "    detail_df = weekly_include_df.drop_duplicates(subset=['invoice_id', 'bcw_index', 'category', 'affiliate', 'callsign', 'isci']).copy()\n",
        "    detail_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if len(weekly_exclusions_df) > 0:\n",
        "        detail_exclusions_df = weekly_exclusions_df.drop_duplicates(subset=['invoice_id', 'bcw_index', 'category', 'category_orig', 'affiliate', 'callsign', 'isci', 'spot_estimate', 'cable_estimate']).copy()\n",
        "        detail_exclusions_df.reset_index(drop=True, inplace=True)\n",
        "        has_weekly_exclusions = True\n",
        "    else:\n",
        "        detail_exclusions_df = pd.DataFrame()\n",
        "        has_weekly_exclusions = False\n",
        "\n",
        "\n",
        "    # Process each invoice_id\n",
        "    for invoice_id in invoice_ids:\n",
        "        # Filter summary and detail for the current invoice_id\n",
        "        summary_group = summary_df[summary_df['invoice_id'] == invoice_id]\n",
        "        detail_group = detail_df[detail_df['invoice_id'] == invoice_id]\n",
        "        # invoice_index = summary_group['invoice_index'].iloc[0]\n",
        "        if has_weekly_exclusions:\n",
        "            detail_exclusions_group = (detail_exclusions_df[detail_exclusions_df['invoice_id'] == invoice_id]).copy()\n",
        "            # print((detail_exclusions_group))\n",
        "\n",
        "            if len(detail_exclusions_group) > 0:\n",
        "                detail_exclusions_group.reset_index(drop=True, inplace=True)\n",
        "                detail_exclusions_group = detail_exclusions_group.sort_values(by=detail_exclusion_group_sort_by)\n",
        "                detail_exclusions_group['excluded'] = detail_exclusions_group['excluded'].astype(bool)\n",
        "                has_invoice_exclusions = True\n",
        "            else:\n",
        "                has_invoice_exclusions = False\n",
        "        else:\n",
        "            has_invoice_exclusions = False\n",
        "\n",
        "        # Sort summary and detail as needed\n",
        "        summary_group = summary_group.sort_values(by=summary_group_sort_by)\n",
        "        detail_group = detail_group.sort_values(by=detail_group_sort_by)\n",
        "\n",
        "        # Extract necessary fields for the filename\n",
        "        bcm_index = summary_group['bcm_index'].iloc[0]\n",
        "        bcm_name = summary_group['bcm_name'].iloc[0]\n",
        "        bcm_start = summary_group['bcm_start_date_display'].iloc[0]\n",
        "        bcm_end = summary_group['bcm_end_date_display'].iloc[0]\n",
        "        bcm_dates = f'{bcm_start} to {bcm_end}'\n",
        "        billing_month = summary_group['billing_month'].iloc[0]\n",
        "        currency = summary_group['currency'].iloc[0].upper()  # Convert currency to uppercase for consistency\n",
        "        sfdc_account_name = summary_group['sfdc_account_name'].iloc[0]\n",
        "        sfdc_account_id = summary_group['sfdc_account_id'].iloc[0]\n",
        "        sfdc_advertiser_id = ''\n",
        "        billing_type = 'weekly'\n",
        "        sfdc_account_name_safe = re.sub(r'[^\\w\\-.]', '_', sfdc_account_name)\n",
        "        sfdc_account_name_safe = re.sub(r'[|(){}]', '_', sfdc_account_name_safe)\n",
        "        invoice_title = summary_group['invoice_title'].iloc[0]\n",
        "        invoice_title_safe = re.sub(r'[^\\w\\-.]', '_', invoice_title)\n",
        "        invoice_title_safe = re.sub(r'[|(){}]', '_', invoice_title_safe)\n",
        "        account_csm = summary_group['csm_sfdc_user_name'].iloc[0]\n",
        "        description = summary_group['description'].iloc[0]\n",
        "\n",
        "        # Create an Excel filename\n",
        "        excel_file_name = f'invoice_{billing_month}_{sfdc_account_name_safe}_{invoice_title_safe}_{invoice_id}.xlsx'\n",
        "\n",
        "        # Add weekly details to sfdc opp df\n",
        "        sfdc_opp_prep_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "        sfdc_opp_prep_df = summary_group[sfdc_opp_summary_columns].drop_duplicates(subset=['invoice_id']).copy()\n",
        "        sfdc_opp_summary_df = pd.concat(\n",
        "            [sfdc_opp_summary_df, sfdc_opp_prep_df],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "        # Create a Pandas Excel writer object\n",
        "        with pd.ExcelWriter(excel_file_name, engine='xlsxwriter') as writer:\n",
        "            # Define workbook and format objects\n",
        "            workbook = writer.book\n",
        "            bold_format = workbook.add_format({'bold': True})\n",
        "            bold_right_format = workbook.add_format({'bold': True, 'align': 'right'})\n",
        "            currency_format = workbook.add_format({'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'num_format': 'C$#,##0.00'})\n",
        "            currency_bold_format = workbook.add_format({'bold': True, 'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'bold': True, 'num_format': 'C$#,##0.00'})\n",
        "            date_format = workbook.add_format({'num_format': 'mm/dd/yyyy'})  # Define date format\n",
        "\n",
        "            # Write the company name at the top\n",
        "            summary_sheet = workbook.add_worksheet('Summary')\n",
        "            detail_sheet = workbook.add_worksheet('Details')\n",
        "\n",
        "\n",
        "            summary_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "            detail_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                detail_exclusions_sheet = workbook.add_worksheet('Exclusions')\n",
        "                detail_exclusions_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "\n",
        "            # Write the first 4 lines with invoice_id, sfdc_account_name, invoice date, and billing month\n",
        "            headers = [\n",
        "                ('Invoice ID:', invoice_id),\n",
        "                ('Invoice Date:', formatted_invoice_date),\n",
        "                ('Account:', sfdc_account_name),\n",
        "                ('Broadcast Month:', bcm_name),\n",
        "                ('Broadcast Month Dates:', bcm_dates),\n",
        "                (invoice_title, ''),\n",
        "                ('Description:', description)\n",
        "            ]\n",
        "\n",
        "            for i, (label, value) in enumerate(headers, start=3):  # Start from row 3 to leave space for the company name\n",
        "                summary_sheet.write(f'A{i}', label, bold_format)\n",
        "                summary_sheet.write(f'B{i}', value)\n",
        "                detail_sheet.write(f'A{i}', label, bold_format)\n",
        "                detail_sheet.write(f'B{i}', value)\n",
        "                if has_invoice_exclusions:\n",
        "                    detail_exclusions_sheet.write(f'A{i}', label, bold_format)\n",
        "                    detail_exclusions_sheet.write(f'B{i}', value)\n",
        "\n",
        "            # Write the custom headers in row 10\n",
        "            for col_num, column in enumerate(summary_columns):\n",
        "                custom_header = custom_summary_headers.get(column, column)\n",
        "                summary_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "\n",
        "            for col_num, column in enumerate(detail_columns):\n",
        "                custom_header = custom_detail_headers.get(column, column)\n",
        "                detail_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                for col_num, column in enumerate(detail_exclusion_columns):\n",
        "                    custom_header = custom_detail_headers_exclusions.get(column, column)\n",
        "                    detail_exclusions_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "            # Start writing data from row 11\n",
        "            summary_start_row = 11\n",
        "            detail_start_row = 11\n",
        "            category_grand_quantity = 0\n",
        "            category_grand_total = 0\n",
        "            # Write summary data\n",
        "            for row_num, row_data in enumerate(summary_group.itertuples(index=False), start=summary_start_row):\n",
        "\n",
        "                for col_num, column in enumerate(summary_columns):\n",
        "                    value = getattr(row_data, column)\n",
        "                    cell_format = (\n",
        "                        currency_format if column in currency_columns else\n",
        "                        date_format if column in date_columns else None\n",
        "                    )\n",
        "                    summary_sheet.write(row_num, col_num, value, cell_format)\n",
        "                    if column == 'category_quantity':\n",
        "                        category_grand_quantity = category_grand_quantity + value\n",
        "                    if column == 'category_total':\n",
        "                        category_grand_total = category_grand_total + value\n",
        "                sfdc_opp_detail_prep_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "                sfdc_opp_detail_prep_df = summary_group[sfdc_opp_detail_columns].drop_duplicates(subset=['invoice_id','LineDesc']).copy()\n",
        "                sfdc_opp_detail_df = pd.concat(\n",
        "                    [sfdc_opp_detail_df, sfdc_opp_detail_prep_df],\n",
        "                    ignore_index=True\n",
        "                )\n",
        "\n",
        "            # Write Grand Total\n",
        "            grand_total_label_row = summary_start_row + len(summary_group)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 4, 'Grand Total:', bold_right_format)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 3, category_grand_quantity, bold_right_format)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 1, category_grand_total, currency_bold_format)\n",
        "\n",
        "            # Write detail data\n",
        "            for row_num, row_data in enumerate(detail_group.itertuples(index=False), start=detail_start_row):\n",
        "                for col_num, column in enumerate(detail_columns):\n",
        "                    value = getattr(row_data, column)\n",
        "                    cell_format = (\n",
        "                        currency_format if column in currency_columns else\n",
        "                        date_format if column in date_columns else None\n",
        "                    )\n",
        "                    detail_sheet.write(row_num, col_num, value, cell_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                # Write detail data\n",
        "                for row_num, row_data in enumerate(detail_exclusions_group.itertuples(index=False), start=detail_start_row):\n",
        "                    for col_num, column in enumerate(detail_exclusion_columns):\n",
        "                        value = getattr(row_data, column)\n",
        "                        cell_format = (\n",
        "                            currency_format if column in currency_columns else\n",
        "                            date_format if column in date_columns else None\n",
        "                        )\n",
        "                        detail_exclusions_sheet.write(row_num, col_num, value, cell_format)\n",
        "\n",
        "            # Auto-fit columns to content starting from A2\n",
        "            for col_num, column in enumerate(summary_columns):\n",
        "                max_length = max(\n",
        "                    summary_group[column].astype(str).map(len).max(),\n",
        "                    len(custom_summary_headers.get(column, column))\n",
        "                ) + 6\n",
        "                summary_sheet.set_column(col_num, col_num, max_length)\n",
        "\n",
        "            for col_num, column in enumerate(detail_columns):\n",
        "                max_length = max(\n",
        "                    detail_group[column].astype(str).map(len).max(),\n",
        "                    len(custom_detail_headers.get(column, column))\n",
        "                ) + 6\n",
        "                detail_sheet.set_column(col_num, col_num, max_length)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                for col_num, column in enumerate(detail_exclusion_columns):\n",
        "                    max_length = max(\n",
        "                    detail_exclusions_group[column].astype(str).map(len).max(),\n",
        "                    len(custom_detail_headers.get(column, column))\n",
        "                    ) + 6\n",
        "                    detail_exclusions_sheet.set_column(col_num, col_num, max_length)\n",
        "\n",
        "        # Upload the file to Google Cloud Storage\n",
        "        blob_path = f'invoices/{billing_month}/{invoice_target}/{excel_file_name}'\n",
        "        blob = bucket.blob(blob_path)\n",
        "        blob.upload_from_filename(excel_file_name)\n",
        "        blob2_path = f'invoices/{billing_month}/for_csms/{account_csm}/{invoice_target}/{excel_file_name}'\n",
        "        blob = bucket.blob(blob2_path)\n",
        "        blob.upload_from_filename(excel_file_name)\n",
        "\n",
        "        # Generate signed URL for public access\n",
        "        file_url = generate_signed_url(bucket_name, blob_path, expiration=30)\n",
        "\n",
        "        # Upsert the record into BigQuery\n",
        "        upsert_invoice_record(invoice_id, file_url, bcm_index, sfdc_account_id, sfdc_advertiser_id, billing_type)\n",
        "\n",
        "        # Optionally, delete the local file after upload\n",
        "        os.remove(excel_file_name)\n",
        "\n",
        "    print(\"Weekly billing Excel files generated, uploaded to GCS, and upserted into BigQuery successfully.\")\n",
        "\n",
        "\n",
        "    sfdc_opp_summary_df['invoice_id'] = sfdc_opp_summary_df['invoice_id'].astype(str)\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_summary_df, sfdc_opp_summary_table_id, project_id)\n",
        "\n",
        "    sfdc_opp_detail_df_unique = sfdc_opp_detail_df.drop_duplicates().copy()\n",
        "    sfdc_opp_detail_df_unique['invoice_id'] = sfdc_opp_detail_df_unique['invoice_id'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['quantity'] = sfdc_opp_detail_df_unique['quantity'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['price'] = sfdc_opp_detail_df_unique['price'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['li_total'] = sfdc_opp_detail_df_unique['li_total'].astype(str)\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_detail_df_unique, sfdc_opp_detail_table_id, project_id)\n",
        "\n",
        "def generate_monthly_excel(bcm_index):\n",
        "    # set key vars\n",
        "    invoice_target = \"monthly\"\n",
        "\n",
        "    sfdc_opp_summary_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "    sfdc_opp_detail_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "\n",
        "\n",
        "    invoice_date = datetime.now()\n",
        "    formatted_invoice_date = invoice_date.strftime(\"%-m/%-d/%Y\")\n",
        "    summary_group_sort_by = ['bcm_index', 'sfdc_account_name', 'sfdc_advertiser_name']\n",
        "    summary_columns = ['li_label', 'li_quantity', 'li_price', 'li_total']\n",
        "    advertiser_summary_columns = ['sfdc_advertiser_name','li_label', 'li_quantity', 'li_price', 'li_total']\n",
        "    detail_group_sort_by = ['bcm_index', 'sfdc_account_name', 'sfdc_advertiser_name', 'isci']\n",
        "    detail_exclusion_group_sort_by = ['invoice_id','bcm_index', 'sfdc_account_name', 'sfdc_advertiser_name', 'isci']\n",
        "    detail_columns = ['sfdc_advertiser_name', 'encoding_id', 'isci']\n",
        "    detail_exclusion_columns = [ 'sfdc_advertiser_name', 'encoding_id', 'isci', 'exclusion_type', 'exclusion_task_id']\n",
        "\n",
        "\n",
        "    date_columns = ['bcm_start_date_display', 'bcm_end_date_display']\n",
        "    currency_columns = ['li_price', 'li_total', 'inv_total']\n",
        "    quantity_columns = ['li_qty', 'adv_iscis', 'inv_iscis', 'isci_detections', 'inv_detections']\n",
        "\n",
        "    monthly_exclusions_df = pd.DataFrame()\n",
        "    # Define custom column headers\n",
        "    custom_summary_headers = {\n",
        "        'li_label': 'Item',\n",
        "        'li_qty': 'Quantity',\n",
        "        'li_price': 'Price',\n",
        "        'li_total': 'Total',\n",
        "    }\n",
        "\n",
        "    advertiser_custom_summary_headers = {\n",
        "        'sfdc_advertiser_name': '',\n",
        "        'li_label': 'Item',\n",
        "        'li_qty': 'Quantity',\n",
        "        'li_price': 'Price',\n",
        "        'li_total': 'Total',\n",
        "    }\n",
        "\n",
        "    custom_detail_headers = {\n",
        "\n",
        "        'sfdc_advertiser_name': 'Advertiser',\n",
        "        'encoding_id': 'Encoding ID',\n",
        "        'isci': 'ISCI',\n",
        "\n",
        "    }\n",
        "\n",
        "    custom_detail_headers_exclusions = {\n",
        "        'sfdc_advertiser_name': 'Advertiser Name',\n",
        "        'encoding_id': 'Encoding ID',\n",
        "        'isci': 'ISCI',\n",
        "        'exclusion_type': 'Exclusion Type',\n",
        "        'exclusion_task_id': 'Exclusion Task ID',\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "        # Query the summary and detail views\n",
        "\n",
        "    monthly_processed_query = f\"\"\"\n",
        "    SELECT * FROM `{project_id}.{dataset_id}._monthly_processed_inclusions`\n",
        "    \"\"\"\n",
        "    summary_query = f\"\"\"\n",
        "    SELECT * FROM `{project_id}.{dataset_id}._monthly_billing_invoice_items_usage___summary` where bcm_index = {bcm_index}\n",
        "    \"\"\"\n",
        "    # detail_query = f\"\"\"\n",
        "    # SELECT * FROM `adhoc-billing.avs_billing_process._monthly_billing_invoice_items_usage___detail_archive` where bcm_index = {bcm_index}\n",
        "    # \"\"\"\n",
        "    detail_isci_counts_query = f\"\"\"\n",
        "    SELECT * FROM `{project_id}.{dataset_id}._monthly_billing_invoice_items_usage___detail_isci_counts`\n",
        "    \"\"\"\n",
        "    detail_detection_counts_query = f\"\"\"\n",
        "    SELECT * FROM `{project_id}.{dataset_id}._monthly_billing_invoice_items_usage___detail_detection_counts`\n",
        "    \"\"\"\n",
        "    # # Display all columns\n",
        "    # pd.set_option('display.max_columns', None)\n",
        "\n",
        "    # # Set unlimited column width\n",
        "    # pd.set_option('display.max_colwidth', None)\n",
        "    # Load data into DataFrames\n",
        "    monthly_processed_df = bigquery_client.query(monthly_processed_query).to_dataframe()\n",
        "\n",
        "    monthly_processed_df[monthly_processed_df['format_id'] == 15265]\n",
        "    monthly_processed_df = monthly_processed_df.sort_values(by=detail_group_sort_by)\n",
        "    monthly_processed_df['excluded'] = monthly_processed_df['excluded'].astype(bool)\n",
        "    excluded_mask = monthly_processed_df['excluded'] == True\n",
        "    monthly_exclusions_df = monthly_processed_df[excluded_mask].copy()\n",
        "    monthly_exclusions_df['exclusion_type'] = monthly_exclusions_df['exclusion_details'].apply(lambda x: x['exclusion_type'])\n",
        "    monthly_exclusions_df['exclusion_task_id'] = monthly_exclusions_df['exclusion_details'].apply(lambda x: x['exclusion_task_id'])\n",
        "    monthly_exclusions_df.reset_index(drop=True, inplace=True)\n",
        "    monthly_include_df = monthly_processed_df[~excluded_mask].copy()\n",
        "    monthly_include_df.reset_index(drop=True, inplace=True)\n",
        "    monthly_include_df[monthly_include_df['format_id'] == 15265]\n",
        "\n",
        "    # monthly_include_df['li_label'] = monthly_include_df.apply(\n",
        "    #     lambda row: f\"{row['sfdc_advertiser_name']} : {row['li_label']}\" if row['sfdc_account_id'] == '0013h00001Vc5S1AAJ' else f\"{row['li_label']}\",\n",
        "    #     axis=1\n",
        "    # )\n",
        "\n",
        "    monthly_exclusions_df['sfdc_account_name'].drop_duplicates()\n",
        "\n",
        "    # summary_df = bigquery_client.query(summary_query).to_dataframe()\n",
        "    # detail_df = bigquery_client.query(detail_query).to_dataframe()\n",
        "    detail_isci_counts_df = bigquery_client.query(detail_isci_counts_query).to_dataframe()\n",
        "    detail_isci_counts_df[detail_isci_counts_df['format_id'] == 15265]\n",
        "    detail_detection_counts_df = bigquery_client.query(detail_detection_counts_query).to_dataframe()\n",
        "    detail_detection_counts_df[detail_detection_counts_df['format_id'] == 15265]\n",
        "\n",
        "    # Get unique invoice_ids\n",
        "\n",
        "    monthly_include_df['currency'].fillna('USD', inplace=True)\n",
        "    # advertiser_breakout_mask = monthly_include_df['invoice_format'] == 'account-advertiser_invoice'\n",
        "    # standard_monthly_include_df = monthly_include_df[~advertiser_breakout_mask].copy()\n",
        "    # standard_monthly_include_df['currency'].fillna('USD', inplace=True)\n",
        "    # standard_monthly_include_df.reset_index(drop=True, inplace=True)\n",
        "    # advertiser_monthly_include_df = monthly_include_df[advertiser_breakout_mask].copy()\n",
        "    # advertiser_monthly_include_df['currency'].fillna('USD', inplace=True)\n",
        "    # advertiser_monthly_include_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Assuming summary_columns is a list of column names\n",
        "    summary_df = monthly_include_df.drop_duplicates(subset=['invoice_id', 'li_label', 'li_total', 'li_quantity']).dropna(subset=['li_quantity']).copy()\n",
        "    summary_df[summary_df['format_id'] == 15265]\n",
        "    summary_df.reset_index(drop=True, inplace=True)\n",
        "    invoice_ids = summary_df['invoice_id'].unique()\n",
        "    # advertiser_summary_df = monthly_include_df.drop_duplicates(subset=['invoice_id', 'sfdc_advertiser_id', 'li_label', 'li_total', 'li_quantity']).copy()\n",
        "    # advertiser_summary_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    summary_df.columns.to_list()\n",
        "    summary_df['invoice_format'].drop_duplicates()\n",
        "    summary_df['usage_tier_plat_detection_billing_unit'].drop_duplicates()\n",
        "\n",
        "    summary_df.columns.tolist()\n",
        "    summary_df['li_total']\n",
        "    summary_df['broadcast_month_name'] = pd.to_datetime(summary_df['billing_month'], format='%Y-%m').dt.strftime('%B-%Y')\n",
        "    summary_df['op_name'] = summary_df.apply(\n",
        "        lambda row: f\"{row['sfdc_account_name']} - {row['invoice_title']} ({row['bcm_start_date_display']} - {row['bcm_end_date_display']})\",\n",
        "        axis=1\n",
        "    )\n",
        "    summary_df.columns.to_list()\n",
        "    summary_df['ownerId'] = opp_owner_id\n",
        "    summary_df['closeDate'] = today.strftime('%Y-%m-%d')\n",
        "    summary_df['poNumber'] = ''\n",
        "    summary_df['stageName'] = opp_stage_name\n",
        "    summary_df['orderdate'] = today.strftime('%Y-%m-%d')\n",
        "    summary_df['billing_start_date'] = billing_start_date\n",
        "    summary_df['billing_end_date'] = billing_end_date\n",
        "    summary_df['type'] = opp_type\n",
        "    summary_df['BVSInvoiceTerms'] = summary_df['description']\n",
        "    summary_df['SOSalesRep'] = ''\n",
        "    summary_df['PriceBook2Id'] = opp_pricebook_id\n",
        "    summary_df['RecordTypeId'] = opp_record_type_id\n",
        "    summary_df['invoice_short_url'] = ''\n",
        "    summary_df['sfdc_opportunity_id'] = ''\n",
        "    # summary_df['sfdc_advertiser_id'] =\n",
        "    # summary_df['poNumber'] = ''\n",
        "    summary_df['reporting_cust_name'] = summary_df['sfdc_account_name']\n",
        "    summary_df['reporting_adv_name'] = summary_df['sfdc_advertiser_name']\n",
        "\n",
        "    summary_df['LineDesc'] = summary_df['li_label']\n",
        "\n",
        "    # summary_df['li_label']\n",
        "    summary_df['Product2Id'] = summary_df['sfdc_product_id']\n",
        "    summary_df['ProductCode'] = summary_df['sfdc_product_code']\n",
        "    summary_df['quantity'] = summary_df['li_quantity']\n",
        "    summary_df['price'] = summary_df['li_price']\n",
        "\n",
        "\n",
        "    detail_df = monthly_include_df.drop_duplicates(subset=['invoice_id', 'bcm_index', 'sfdc_account_id', 'sfdc_advertiser_id', 'isci']).dropna(subset=['li_quantity']).copy()\n",
        "    detail_df.reset_index(drop=True, inplace=True)\n",
        "    detail_df.columns.to_list()\n",
        "    # detail_df[['invoice_id', 'bcm_index', 'sfdc_advertiser_id', 'isci','detections']].drop_duplicates()\n",
        "\n",
        "    # exclusions_df = monthly_exclusions_df.drop_duplicates(subset=['invoice_id', 'bcm_index', 'sfdc_account_id', 'sfdc_advertiser_id', 'isci']).copy()\n",
        "\n",
        "    if len(monthly_exclusions_df) > 0:\n",
        "        detail_exclusions_df = monthly_exclusions_df.drop_duplicates(subset=['invoice_id', 'bcm_index', 'sfdc_account_id', 'sfdc_advertiser_id', 'isci']).copy()\n",
        "        detail_exclusions_df.reset_index(drop=True, inplace=True)\n",
        "        has_monthly_exclusions = True\n",
        "    else:\n",
        "        detail_exclusions_df = pd.DataFrame()\n",
        "        has_monthly_exclusions = False\n",
        "\n",
        "    # Extract necessary fields for the filename\n",
        "    bcm_index = summary_df['bcm_index'].iloc[0]\n",
        "    bcm_name = summary_df['bcm_name'].iloc[0]\n",
        "    bcm_start = summary_df['bcm_start_date_display'].iloc[0]\n",
        "    bcm_end = summary_df['bcm_end_date_display'].iloc[0]\n",
        "    bcm_dates = f'{bcm_start} to {bcm_end}'\n",
        "    billing_month = summary_df['billing_month'].iloc[0]\n",
        "    summary_group = pd.DataFrame()\n",
        "    advertiser_summary_group = pd.DataFrame()\n",
        "\n",
        "    # Process each invoice_id\n",
        "    for invoice_id in invoice_ids:\n",
        "        # Filter summary and detail for the current invoice_id\n",
        "        summary_group = summary_df[summary_df['invoice_id'] == invoice_id]\n",
        "        if summary_group['min_value'].iloc[0]:\n",
        "            if summary_group['min_applied_at'].iloc[0] == 'account':\n",
        "                if summary_group['inv_total_raw'].iloc[0] < summary_group['min_value'].iloc[0]:\n",
        "                    summary_group_min_add_df = pd.DataFrame(columns=summary_columns)\n",
        "                    summary_group_min_add_df = summary_group.iloc[0:1].copy()\n",
        "                    summary_group_min_add_df['li_label'] = 'Monthly Minimum Adjustment'\n",
        "                    summary_group_min_add_df['li_quantity'] = 1\n",
        "                    summary_group_min_add_df['li_price'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                    summary_group_min_add_df['li_total'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                    summary_group_min_add_df['LineDesc'] = 'Monthly Minimum Adjustment'\n",
        "                    summary_group_min_add_df['quantity'] = 1\n",
        "                    summary_group_min_add_df['price'] = summary_group_min_add_df['min_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                    summary_group_2 = pd.concat(\n",
        "                        [summary_group, summary_group_min_add_df],\n",
        "                        ignore_index=True\n",
        "                    )\n",
        "                else:\n",
        "                    summary_group_2 = summary_group.copy()\n",
        "            else:\n",
        "                summary_group_2 = summary_group.copy()\n",
        "        else:\n",
        "            if summary_group['max_value'].iloc[0]:\n",
        "                if summary_group['max_applied_at'].iloc[0] == 'account':\n",
        "                    if summary_group['inv_total_raw'].iloc[0] > summary_group['max_value'].iloc[0]:\n",
        "                        summary_group_min_add_df = pd.DataFrame(columns=summary_columns)\n",
        "                        summary_group_min_add_df = summary_group.iloc[0:1].copy()\n",
        "                        summary_group_min_add_df['li_label'] = 'Monthly Maximum Adjustment'\n",
        "                        summary_group_min_add_df['li_quantity'] = 1\n",
        "                        summary_group_min_add_df['li_price'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                        summary_group_min_add_df['li_total'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                        summary_group_min_add_df['LineDesc'] = 'Monthly Maximum Adjustment'\n",
        "                        summary_group_min_add_df['quantity'] = 1\n",
        "                        summary_group_min_add_df['price'] = summary_group_min_add_df['max_value'].iloc[0] - summary_group_min_add_df['inv_total_raw'].iloc[0]\n",
        "                        summary_group_2 = pd.concat(\n",
        "                            [summary_group, summary_group_min_add_df],\n",
        "                            ignore_index=True\n",
        "                        )\n",
        "                    else:\n",
        "                        summary_group_2 = summary_group.copy()\n",
        "                else:\n",
        "                    summary_group_2 = summary_group.copy()\n",
        "            else:\n",
        "                summary_group_2 = summary_group.copy()\n",
        "\n",
        "\n",
        "        summary_group_2.reset_index(drop=True, inplace=True)\n",
        "        summary_group = summary_group_2.copy()\n",
        "        # advertiser_summary_group = advertiser_summary_df[advertiser_summary_df['invoice_id'] == invoice_id]\n",
        "        detail_group = detail_df[detail_df['invoice_id'] == invoice_id]\n",
        "        exclusions_group = monthly_exclusions_df[monthly_exclusions_df['invoice_id'] == invoice_id]\n",
        "\n",
        "        new_invoice_total = 0\n",
        "\n",
        "        if len(exclusions_group) > 0:\n",
        "            has_monthly_invoice_exclusions = True\n",
        "        else:\n",
        "            has_monthly_invoice_exclusions = False\n",
        "\n",
        "        detail_isci_counts_group = detail_isci_counts_df[detail_isci_counts_df['invoice_id'] == invoice_id]\n",
        "\n",
        "        detail_detection_counts_group = detail_detection_counts_df[detail_detection_counts_df['invoice_id'] == invoice_id]\n",
        "\n",
        "        # Extract 'inv_iscis' numeric value for the current invoice_id\n",
        "        try:\n",
        "            inv_iscis = detail_isci_counts_group['inv_iscis'].iloc[0]\n",
        "            advertiser_inv_iscis = advertiser_summary_group[['adv_iscis']].sum()\n",
        "        except:\n",
        "            inv_iscis = 0  # Set to 0 if 'inv_iscis' is not found\n",
        "\n",
        "        include_detections = False\n",
        "        # Extract 'inv_detections' numeric value for the current invoice_id\n",
        "\n",
        "        try:\n",
        "            inv_detections = detail_detection_counts_df['inv_detections'].iloc[0]\n",
        "            # inv_detections = summary_group['isci_detections'].sum()\n",
        "            include_detections = True\n",
        "        except:\n",
        "            inv_detections = 0  # Set to 0 if 'inv_detections' is not found\n",
        "            include_detections = False\n",
        "\n",
        "        total_iscis_inserted = False\n",
        "        total_detections_inserted = False\n",
        "\n",
        "        if has_monthly_exclusions:\n",
        "            detail_exclusions_group = (detail_exclusions_df[detail_exclusions_df['invoice_id'] == invoice_id]).copy()\n",
        "            # print((detail_exclusions_group))\n",
        "\n",
        "            if len(detail_exclusions_group) > 0:\n",
        "                detail_exclusions_group.reset_index(drop=True, inplace=True)\n",
        "                detail_exclusions_group = detail_exclusions_group.sort_values(by=detail_exclusion_group_sort_by)\n",
        "                detail_exclusions_group['excluded'] = detail_exclusions_group['excluded'].astype(bool)\n",
        "                has_invoice_exclusions = True\n",
        "            else:\n",
        "                has_invoice_exclusions = False\n",
        "        else:\n",
        "            has_invoice_exclusions = False\n",
        "\n",
        "        # Sort summary and detail as needed\n",
        "        summary_group = summary_group.sort_values(by=summary_group_sort_by)\n",
        "        detail_group = detail_group.sort_values(by=detail_group_sort_by)\n",
        "        currency = summary_group['currency'].iloc[0].upper()  # Convert currency to uppercase for consistency\n",
        "        sfdc_account_name = summary_group['sfdc_account_name'].iloc[0]\n",
        "        sfdc_account_id = summary_group['sfdc_account_id'].iloc[0]\n",
        "        sfdc_advertiser_id = summary_group['sfdc_advertiser_id'].iloc[0]\n",
        "        billing_type = summary_group['billing_type'].iloc[0]\n",
        "        sfdc_account_name_safe = re.sub(r'[^\\w\\-.]', '_', sfdc_account_name)\n",
        "        sfdc_account_name_safe = re.sub(r'[|(){}]', '_', sfdc_account_name_safe)\n",
        "        invoice_title = summary_group['invoice_title'].iloc[0]\n",
        "        invoice_title_safe = re.sub(r'[^\\w\\-.]', '_', invoice_title)\n",
        "        invoice_title_safe = re.sub(r'[|(){}]', '_', invoice_title_safe)\n",
        "        description = summary_group['description'].iloc[0]\n",
        "        account_csm = summary_group['csm_sfdc_user_name'].iloc[0]\n",
        "\n",
        "\n",
        "        # Create an Excel filename\n",
        "        excel_file_name = f'invoice_{billing_month}_{sfdc_account_name_safe}_{invoice_title_safe}_{invoice_id}.xlsx'\n",
        "\n",
        "        # Add weekly details to sfdc opp df\n",
        "        sfdc_opp_prep_df = pd.DataFrame(columns=sfdc_opp_summary_columns)\n",
        "        sfdc_opp_prep_df = summary_group[sfdc_opp_summary_columns].drop_duplicates(subset=['invoice_id']).copy()\n",
        "        sfdc_opp_summary_df = pd.concat(\n",
        "            [sfdc_opp_summary_df, sfdc_opp_prep_df],\n",
        "            ignore_index=True\n",
        "        )\n",
        "\n",
        "        # Create a Pandas Excel writer object\n",
        "        with pd.ExcelWriter(excel_file_name, engine='xlsxwriter') as writer:\n",
        "            # Define workbook and format objects\n",
        "            workbook = writer.book\n",
        "            bold_format = workbook.add_format({'bold': True})\n",
        "            bold_right_format = workbook.add_format({'bold': True, 'align': 'right'})\n",
        "            quantity_format = workbook.add_format({'num_format': '#,##0'})\n",
        "            quantity_bold_format = workbook.add_format({'bold': True, 'num_format': '#,##0'})\n",
        "            currency_format = workbook.add_format({'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'num_format': 'C$#,##0.00'})\n",
        "            currency_bold_format = workbook.add_format({'bold': True, 'num_format': '$#,##0.00'}) if currency == 'USD' else workbook.add_format({'bold': True, 'num_format': 'C$#,##0.00'})\n",
        "            date_format = workbook.add_format({'num_format': 'mm/dd/yyyy'})  # Define date format\n",
        "\n",
        "            # Write the company name at the top\n",
        "            summary_sheet = workbook.add_worksheet('Summary')\n",
        "            detail_sheet = workbook.add_worksheet('Details')\n",
        "\n",
        "            summary_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "            detail_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                detail_exclusions_sheet = workbook.add_worksheet('Exclusions')\n",
        "                detail_exclusions_sheet.write('A1', \"Veil Global Technologies (formerly Advocado)\", bold_format)\n",
        "\n",
        "\n",
        "            # Write the first 4 lines with invoice_id, sfdc_account_name, invoice date, and billing month\n",
        "            headers = [\n",
        "                ('Invoice ID:', invoice_id),\n",
        "                ('Invoice Date:', formatted_invoice_date),\n",
        "                ('Account:', sfdc_account_name),\n",
        "                ('Broadcast Month:', bcm_name),\n",
        "                ('Broadcast Month Dates:', bcm_dates),\n",
        "                (invoice_title, ''),\n",
        "                ('Description:', description)\n",
        "            ]\n",
        "            for i, (label, value) in enumerate(headers, start=3):  # Start from row 3 to leave space for the company name\n",
        "                summary_sheet.write(f'A{i}', label, bold_format)\n",
        "                summary_sheet.write(f'B{i}', value)\n",
        "                detail_sheet.write(f'A{i}', label, bold_format)\n",
        "                detail_sheet.write(f'B{i}', value)\n",
        "                if has_invoice_exclusions:\n",
        "                    detail_exclusions_sheet.write(f'A{i}', label, bold_format)\n",
        "                    detail_exclusions_sheet.write(f'B{i}', value)\n",
        "\n",
        "            # Adjust the column widths for D & E (Total ISCIs and ISCI Detections)\n",
        "            summary_sheet.set_column(0, 2, width=40)  # Adjusting column D & E to 20 characters wide\n",
        "            summary_sheet.set_column(2, 6, width=20)  # Adjusting column D & E to 20 characters wide\n",
        "            detail_sheet.set_column(3, 6, 20, quantity_format)  # Adjusting column D & E to 20 characters wide\n",
        "\n",
        "\n",
        "            # Add the 'inv_iscis' value above the detail columns\n",
        "            # detail_sheet.write('E10', 'Total ISCIs', bold_format)\n",
        "            # detail_sheet.write('E11', inv_iscis, bold_format)  # Writing the extracted 'inv_iscis' value\n",
        "            detail_sheet.write(9, 3, 'Advertiser ISCIs', bold_format)  # New header for \"Advertiser ISCIs\"\n",
        "            detail_sheet.write(9, 4, 'Total ISCIs', bold_format)\n",
        "\n",
        "            if inv_detections > 0:\n",
        "                # Add \"ISCI Detections\" and \"Total Detections\" columns to the right of \"Total ISCIs\"\n",
        "                detail_sheet.write(9, 5, 'ISCI Detections', bold_format)\n",
        "                detail_sheet.write(9, 6, 'Total Detections', bold_format)\n",
        "\n",
        "            # Initialize a variable to track the first occurrence of each advertiser\n",
        "            advertiser_written = {}\n",
        "\n",
        "            # Write the custom headers in row 11\n",
        "            for col_num, column in enumerate(summary_columns):\n",
        "                custom_header = custom_summary_headers.get(column, column)\n",
        "                summary_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "            for col_num, column in enumerate(detail_columns):\n",
        "                custom_header = custom_detail_headers.get(column, column)\n",
        "                detail_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                for col_num, column in enumerate(detail_exclusion_columns):\n",
        "                    custom_header = custom_detail_headers_exclusions.get(column, column)\n",
        "                    detail_exclusions_sheet.write(10, col_num, custom_header, bold_format)\n",
        "\n",
        "\n",
        "            # Start writing data from row 11\n",
        "            summary_start_row = 11\n",
        "            detail_start_row = 11\n",
        "\n",
        "            # Write summary data\n",
        "            for row_num, row_data in enumerate(summary_group.itertuples(index=False), start=summary_start_row):\n",
        "                for col_num, column in enumerate(summary_columns):\n",
        "                    if column == 'li_total':\n",
        "                        new_invoice_total += getattr(row_data, column)\n",
        "                    value = getattr(row_data, column)\n",
        "                    cell_format = (\n",
        "                        currency_format if column in currency_columns else\n",
        "                        quantity_format if column in quantity_columns else\n",
        "                        date_format if column in date_columns else None\n",
        "                    )\n",
        "                    summary_sheet.write(row_num, col_num, value, cell_format)\n",
        "                sfdc_opp_detail_prep_df = pd.DataFrame(columns=sfdc_opp_detail_columns)\n",
        "                sfdc_opp_detail_prep_df = summary_group[sfdc_opp_detail_columns].drop_duplicates(subset=['invoice_id','LineDesc']).copy()\n",
        "                sfdc_opp_detail_df = pd.concat(\n",
        "                    [sfdc_opp_detail_df, sfdc_opp_detail_prep_df],\n",
        "                    ignore_index=True\n",
        "                )\n",
        "\n",
        "            # Write Grand Total\n",
        "            grand_total_label_row = summary_start_row + len(summary_group)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 4, 'Grand Total:', bold_right_format)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 3, summary_group['li_quantity'].sum(), quantity_bold_format)\n",
        "            # summary_sheet.write(grand_total_label_row, len(summary_columns) - 1, summary_group['inv_total'].iloc[0], currency_bold_format)\n",
        "            summary_sheet.write(grand_total_label_row, len(summary_columns) - 1, new_invoice_total, currency_bold_format)\n",
        "\n",
        "            inv_total_detections = 0\n",
        "            # Write detail data\n",
        "            for row_num, row_data in enumerate(detail_group.itertuples(index=False), start=detail_start_row):\n",
        "                advertiser_name = getattr(row_data, 'sfdc_advertiser_name')\n",
        "                isci = getattr(row_data, 'isci')\n",
        "                isci_detections = getattr(row_data, 'count_detections')\n",
        "                inv_total_detections += isci_detections\n",
        "\n",
        "                # Get the advertiser ISCI count (adv_iscis) by matching invoice_id and sfdc_advertiser_name\n",
        "\n",
        "                # advertiser_isci = summary_group[\n",
        "                #     (summary_group['invoice_id'] == invoice_id) &\n",
        "                #     (summary_group['sfdc_advertiser_name'] == advertiser_name)\n",
        "                # ]['count_iscis_by_advertiser'].iloc[0]\n",
        "\n",
        "                # filtered_series = summary_group[\n",
        "                #     (summary_group['invoice_id'] == invoice_id) &\n",
        "                #     (summary_group['sfdc_advertiser_name'] == advertiser_name)\n",
        "                # ]['count_iscis_by_advertiser']\n",
        "\n",
        "                # if not filtered_series.empty:\n",
        "                #     advertiser_isci = filtered_series.iloc[0]\n",
        "                # else:\n",
        "                #     advertiser_isci = 0\n",
        "\n",
        "                # if not summary_group[\n",
        "                #     (advertiser_summary_group['invoice_id'] == invoice_id) &\n",
        "                #     (advertiser_summary_group['sfdc_advertiser_name'] == advertiser_name)\n",
        "                # ].empty else None\n",
        "\n",
        "                # If this is the first row for this advertiser, write the ISCI count in the \"Advertiser ISCIs\" column\n",
        "                if advertiser_name not in advertiser_written:\n",
        "                    filtered_series = detail_group[\n",
        "                        (detail_group['invoice_id'] == invoice_id) &\n",
        "                        (detail_group['sfdc_advertiser_name'] == advertiser_name)\n",
        "                    ]['count_iscis_by_advertiser']\n",
        "                    if not filtered_series.empty:\n",
        "                        advertiser_isci = filtered_series.iloc[0]\n",
        "                    else:\n",
        "                        advertiser_isci = 0\n",
        "\n",
        "                #     advertiser_isci = summary_group[\n",
        "                #     (summary_group['invoice_id'] == invoice_id) &\n",
        "                #     (summary_group['sfdc_advertiser_name'] == advertiser_name)\n",
        "                # ]['count_iscis_by_advertiser'].iloc[0]\n",
        "                    detail_sheet.write(row_num, 3, advertiser_isci, bold_format)\n",
        "                    advertiser_written[advertiser_name] = True\n",
        "                else:\n",
        "                    detail_sheet.write(row_num, 3, '', bold_format)  # Leave empty for subsequent rows\n",
        "\n",
        "                # Write the total ISCIs value in the \"Total ISCIs\" column\n",
        "                if not total_iscis_inserted:\n",
        "                    inv_iscis = summary_group[\n",
        "                    (summary_group['invoice_id'] == invoice_id)\n",
        "                ]['count_iscis_by_invoice'].iloc[0]\n",
        "                    detail_sheet.write(row_num, 4, inv_iscis, bold_format)\n",
        "                    total_iscis_inserted = True\n",
        "\n",
        "                # if len(detail_detection_counts_group) > 0:\n",
        "                #     # Get ISCI detections\n",
        "                #     # isci_detections = len(detail_df[['detections']])\n",
        "                #     detail_detection_counts_group[\n",
        "                #         (detail_detection_counts_group['invoice_id'] == invoice_id) &\n",
        "                #         (detail_detection_counts_group['sfdc_advertiser_name'] == advertiser_name) &\n",
        "                #         (detail_detection_counts_group['isci'] == isci)\n",
        "                #     ]['isci_detections'].iloc[0]\n",
        "                # else:\n",
        "                #     isci_detections = 0\n",
        "                #     # if not detail_df[\n",
        "                #     #     (detail_df['invoice_id'] == invoice_id) &\n",
        "                #     #     (detail_df['sfdc_advertiser_name'] == advertiser_name) &\n",
        "                #     #     (detail_df['isci'] == isci)\n",
        "                #     # ].empty else None\n",
        "\n",
        "                #     # Write ISCI detections\n",
        "                detail_sheet.write(row_num, 5, isci_detections if isci_detections else 0, bold_format)\n",
        "                detail_sheet.write(11, 6, inv_total_detections, bold_format)\n",
        "\n",
        "\n",
        "                for col_num, column in enumerate(detail_columns):\n",
        "                    value = getattr(row_data, column)\n",
        "                    cell_format = (\n",
        "                        currency_format if column in currency_columns else\n",
        "                        quantity_format if column in quantity_columns else\n",
        "                        date_format if column in date_columns else None\n",
        "                    )\n",
        "                    detail_sheet.write(row_num, col_num, value, cell_format)\n",
        "\n",
        "            if has_invoice_exclusions:\n",
        "                # Write detail data\n",
        "                for row_num, row_data in enumerate(detail_exclusions_group.itertuples(index=False), start=detail_start_row):\n",
        "                    for col_num, column in enumerate(detail_exclusion_columns):\n",
        "                        value = getattr(row_data, column)\n",
        "                        cell_format = (\n",
        "                            currency_format if column in currency_columns else\n",
        "                            date_format if column in date_columns else None\n",
        "                        )\n",
        "                        detail_exclusions_sheet.write(row_num, col_num, value, cell_format)\n",
        "\n",
        "            # Auto-fit columns to content starting from A2\n",
        "            for col_num, column in enumerate(summary_columns):\n",
        "                max_length = max(\n",
        "                    summary_group[column].astype(str).map(len).max(),\n",
        "                    len(custom_summary_headers.get(column, column))\n",
        "                ) + 2\n",
        "                summary_sheet.set_column(col_num, col_num, (max_length + 10))\n",
        "\n",
        "            for col_num, column in enumerate(detail_columns):\n",
        "                max_length = max(\n",
        "                    detail_group[column].astype(str).map(len).max(),\n",
        "                    len(custom_detail_headers.get(column, column))\n",
        "                ) + 2\n",
        "                detail_sheet.set_column(col_num, col_num, (max_length + 10))\n",
        "\n",
        "        # Upload the file to Google Cloud Storage\n",
        "        blob_path = f'invoices/{billing_month}/{invoice_target}/{excel_file_name}'\n",
        "        blob = bucket.blob(blob_path)\n",
        "        blob.upload_from_filename(excel_file_name)\n",
        "        blob2_path = f'invoices/{billing_month}/for_csms/{account_csm}/{invoice_target}/{excel_file_name}'\n",
        "        blob = bucket.blob(blob2_path)\n",
        "        blob.upload_from_filename(excel_file_name)\n",
        "\n",
        "        # Generate signed URL for public access\n",
        "        file_url = generate_signed_url(bucket_name, blob_path, expiration=30)\n",
        "\n",
        "        # Upsert the record into BigQuery\n",
        "        upsert_invoice_record(invoice_id, file_url, bcm_index, sfdc_account_id, sfdc_advertiser_id, billing_type)\n",
        "\n",
        "        # Optionally, delete the local file after upload\n",
        "        os.remove(excel_file_name)\n",
        "\n",
        "    print(\"Monthly billing Excel files generated, uploaded to GCS, and upserted into BigQuery successfully.\")\n",
        "\n",
        "\n",
        "    sfdc_opp_summary_df['invoice_id'] = sfdc_opp_summary_df['invoice_id'].astype(str)\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_summary_df, sfdc_opp_summary_table_id, project_id)\n",
        "\n",
        "    sfdc_opp_detail_df_unique = sfdc_opp_detail_df.drop_duplicates().copy()\n",
        "    sfdc_opp_detail_df_unique['invoice_id'] = sfdc_opp_detail_df_unique['invoice_id'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['quantity'] = sfdc_opp_detail_df_unique['quantity'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['price'] = sfdc_opp_detail_df_unique['price'].astype(str)\n",
        "    sfdc_opp_detail_df_unique['li_total'] = sfdc_opp_detail_df_unique['li_total'].astype(str)\n",
        "    upload_dataframe_to_bigquery(sfdc_opp_detail_df_unique, sfdc_opp_detail_table_id, project_id)\n",
        "\n",
        "def initial_task_upload(bcm_index):\n",
        "    summary_group_sort_by = ['Invoice_ID__c']\n",
        "\n",
        "    sync_type = \"CUSTOM\"\n",
        "    sfdc_task_obj = ['Task']\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, sfdc_task_obj)\n",
        "\n",
        "    sfdc_object_name = 'Task'\n",
        "\n",
        "    sfdc_external_id_field = 'Invoice_ID__c'\n",
        "\n",
        "    # sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    # Query the summary and detail views. 0053h000007adzPAAQ\n",
        "    tasks_query = f\"\"\"\n",
        "    SELECT Subject, OwnerId, Status, Broadcast_Month__c, WhatId, Advertiser__c, Invoice_total__c, Draft_Invoice_Preview__c as long_url, short_url, Priority, ActivityDate, Invoice_ID__c , RecordTypeId\n",
        "    FROM `{project_id}.{dataset_id}._billing_tasks_info` where bcm_index = {bcm_index}\n",
        "    order by Invoice_ID__c\n",
        "    \"\"\"\n",
        "\n",
        "    task_df = bigquery_client.query(tasks_query).to_dataframe()\n",
        "\n",
        "    if len(task_df) > 0:\n",
        "\n",
        "        task_df['short_url'] = task_df.apply(\n",
        "            lambda row: get_short_url(row['long_url']) if pd.isna(row['short_url']) or row['short_url'] == '' else row['short_url'],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Define your BigQuery table ID (project_id.dataset_id.table_id)\n",
        "        inv_table_id = f'{project_id}.{dataset_id}._billing_invoice_files'\n",
        "\n",
        "        # Update BigQuery table with new short URLs\n",
        "        update_bq_invoice_table_short_url(task_df, inv_table_id)\n",
        "\n",
        "        needs_short_url_query = f\"\"\"\n",
        "        SELECT * FROM `{project_id}.{dataset_id}._billing_invoice_files` where coalesce(excel_path_short, '') = ''\n",
        "        \"\"\"\n",
        "\n",
        "        needs_short_url_df = bigquery_client.query(needs_short_url_query).to_dataframe()\n",
        "        if len(needs_short_url_df) > 0:\n",
        "            needs_short_url_df['excel_path_short'] = needs_short_url_df.apply(\n",
        "                lambda row: get_short_url(row['excel_path']) if pd.isna(row['excel_path_short']) or row['excel_path_short'] == '' else row['excel_path_short'],\n",
        "                axis=1\n",
        "            )\n",
        "            needs_short_url_df['short_url'] = needs_short_url_df['excel_path_short']\n",
        "            needs_short_url_df['long_url'] = needs_short_url_df['excel_path']\n",
        "            update_bq_invoice_table_short_url(needs_short_url_df, inv_table_id)\n",
        "\n",
        "        gbq_to_sfdc_field_map = {\n",
        "        'Subject': 'Subject',\n",
        "        'OwnerId': 'OwnerId',\n",
        "        'Status': 'Status',\n",
        "        'Broadcast_Month__c': 'Broadcast_Month__c',\n",
        "        'WhatId': 'WhatId',\n",
        "        'Advertiser__c': 'Advertiser__c',\n",
        "        'Invoice_total__c': 'Invoice_total__c',\n",
        "        'short_url': 'Draft_Invoice_Preview__c',\n",
        "        'Priority': 'Priority',\n",
        "        'ActivityDate': 'ActivityDate',\n",
        "        'Invoice_ID__c': 'Invoice_ID__c',\n",
        "        'RecordTypeId': 'RecordTypeId'\n",
        "        }\n",
        "\n",
        "        if 'sfdc_task_id' not in task_df.columns:\n",
        "            task_df['sfdc_task_id'] = pd.NaT\n",
        "        filtered_task_df = pd.DataFrame()\n",
        "        filtered_task_df = task_df[task_df['sfdc_task_id'].isna()]\n",
        "        final_task_col = ['Subject', 'OwnerId','Status','Broadcast_Month__c','WhatId','Advertiser__c','Invoice_total__c','short_url','Priority','ActivityDate','Invoice_ID__c', 'RecordTypeId']\n",
        "\n",
        "        final_task_df = filtered_task_df[final_task_col].copy()\n",
        "\n",
        "        # Sync Encoder_Group__c object\n",
        "        if len(final_task_df) > 0:\n",
        "            response = sync_data_ids_tasks(final_task_df, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "\n",
        "            sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, sfdc_task_obj)\n",
        "        else:\n",
        "            print(\"No new tasks to sync.\")\n",
        "\n",
        "        excel_short_path_update_sql = f\"\"\"\n",
        "            UPDATE `{project_id}.{dataset_id}._billing_invoice_files` f\n",
        "            set f.excel_path_short = t.Draft_Invoice_Preview__c\n",
        "            from `{project_id}.{dataset_id}.sfdc_task_obj` t\n",
        "            where safe_cast(t.Invoice_ID__c as float64) = safe_cast(f.invoice_id as float64)\n",
        "            and t.AccountId = sfdc_account_id\n",
        "            AND coalesce(f.excel_path, '') <> ''\n",
        "            AND coalesce(f.excel_path_short, '') = '';\n",
        "        \"\"\"\n",
        "        # Execute the query\n",
        "        query_job = bigquery_client.query(excel_short_path_update_sql)\n",
        "\n",
        "        # Wait for the query to finish\n",
        "        query_job.result()\n",
        "\n",
        "def update_sfdc_opp_url():\n",
        "    update_sfdc_opp_url_query = f\"\"\"\n",
        "        update `{project_id}.{dataset_id}._export_sfdc_opp_summary` op\n",
        "        set op.invoice_short_url = b.excel_path_short\n",
        "        from `{project_id}.{dataset_id}._billing_invoice_files` b\n",
        "        where op.invoice_id = b.invoice_id\n",
        "        AND op.invoice_short_url = ''\n",
        "        AND coalesce(b.excel_path_short, '') <> '';\n",
        "        \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    update_sfdc_opp_url_query_job = bigquery_client.query(update_sfdc_opp_url_query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    update_sfdc_opp_url_query_job.result()\n",
        "\n",
        "def update_tasks_needing_adjustment():\n",
        "    # @title reset the task status of adjusted invoices\n",
        "\n",
        "    sync_type = \"CUSTOM\"\n",
        "    sfdc_task_obj = ['Task']\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, sfdc_task_obj)\n",
        "\n",
        "    needs_adjustment_query = f\"\"\"\n",
        "    SELECT Id as sfdc_task_id FROM `adhoc-billing.avs_billing_process._billing_tasks_not_approved`\n",
        "    \"\"\"\n",
        "    needs_adjustment_df = bigquery_client.query(needs_adjustment_query).to_dataframe()\n",
        "\n",
        "    for index, row in needs_adjustment_df.iterrows():\n",
        "        try:\n",
        "            sf.Task.update(row['sfdc_task_id'], {'Status': 'Review Invoice - adjustments made'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error while updating Task at index {index}: {e}\")\n",
        "\n",
        "def process_approved_invoices():\n",
        "    # @title Sync BVS Data to SFDC functions\n",
        "    # @title process approved invoice tasks to opportunities\n",
        "    update_sfdc_opp_url_query = f\"\"\"\n",
        "        update `adhoc-billing.avs_billing_process._export_sfdc_opp_summary` op\n",
        "        set op.invoice_short_url = b.excel_path_short\n",
        "        from `adhoc-billing.avs_billing_process._billing_invoice_files` b\n",
        "        where op.invoice_id = b.invoice_id\n",
        "        AND op.invoice_short_url = ''\n",
        "        AND coalesce(b.excel_path_short, '') <> '';\n",
        "        \"\"\"\n",
        "\n",
        "    # Execute the query\n",
        "    update_sfdc_opp_url_query_job = bigquery_client.query(update_sfdc_opp_url_query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    update_sfdc_opp_url_query_job.result()\n",
        "\n",
        "    # Function to get PricebookEntryId\n",
        "    def get_pricebook_entry_id(product_id, pricebook_id):\n",
        "        query = f\"\"\"\n",
        "        SELECT Id\n",
        "        FROM PricebookEntry\n",
        "        WHERE Product2Id = '{product_id}'\n",
        "        AND Pricebook2Id = '{pricebook_id}'\n",
        "        AND IsActive = True\n",
        "        LIMIT 1\n",
        "        \"\"\"\n",
        "        result = sf.query(query)\n",
        "        if result['totalSize'] > 0:\n",
        "            return result['records'][0]['Id']\n",
        "        else:\n",
        "            raise Exception(f\"No active PricebookEntry found for Product2Id {product_id} in Pricebook2Id {pricebook_id}\")\n",
        "\n",
        "\n",
        "    sync_type = \"CUSTOM\"\n",
        "    sfdc_task_obj = ['Task']\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, sfdc_task_obj)\n",
        "\n",
        "\n",
        "    approvals_summary_query = f\"\"\"\n",
        "        WITH approvedInvoices AS (\n",
        "        SELECT cast(Invoice_ID__c as string) as invoice_id from `adhoc-billing.avs_billing_process.sfdc_task_obj`\n",
        "        WHERE lower(Status) in ('approve to bill')\n",
        "        AND RecordTypeId = '012UV000000KpzFYAS'\n",
        "        )\n",
        "        select ops.* from approvedInvoices a\n",
        "        join `adhoc-billing.avs_billing_process._export_sfdc_opp_summary` ops\n",
        "        ON cast(a.invoice_id as float64) = cast(ops.invoice_id as float64)\n",
        "        where sfdc_opportunity_id = '';\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    approvals_summary_df = bigquery_client.query(approvals_summary_query).to_dataframe()\n",
        "\n",
        "    approvals_detail_query = f\"\"\"\n",
        "        WITH approvedInvoices AS (\n",
        "        SELECT cast(Invoice_ID__c as string) as invoice_id from `adhoc-billing.avs_billing_process.sfdc_task_obj`\n",
        "        WHERE lower(Status) in ('approve to bill')\n",
        "        AND RecordTypeId = '012UV000000KpzFYAS'\n",
        "        )\n",
        "        select ops.* from approvedInvoices a\n",
        "        join `adhoc-billing.avs_billing_process._export_sfdc_opp_detail` ops\n",
        "        ON cast(a.invoice_id as float64) = cast(ops.invoice_id as float64)\n",
        "        where sfdc_opportunity_id = '';\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    approvals_detail_df = bigquery_client.query(approvals_detail_query).to_dataframe()\n",
        "    approvals_summary_df['invoice_short_url']\n",
        "    for col in (approvals_summary_df.columns):\n",
        "        approvals_summary_df[col] = approvals_summary_df[col].astype(str)\n",
        "\n",
        "    for col in (approvals_detail_df.columns):\n",
        "        approvals_detail_df[col] = approvals_detail_df[col].astype(str)\n",
        "\n",
        "    # Create Opportunities and store their IDs\n",
        "    # opportunity_id_map = {}\n",
        "\n",
        "    # approvals_summary_df.columns.to_list()\n",
        "    # approvals_summary_df['RecordTypeId'].drop_duplicates()\n",
        "    try:\n",
        "        blisspoint_invoice_group = approvals_summary_df[approvals_summary_df['sfdc_account_id'] == '0013h00001Vc5S1AAJ']\n",
        "        blisspoint_invoice_id = blisspoint_invoice_group['invoice_id'].iloc[0]\n",
        "    except:\n",
        "        blisspoint_invoice_id = ''\n",
        "\n",
        "    for index, row in approvals_summary_df.iterrows():\n",
        "\n",
        "        try:\n",
        "            if row['sfdc_account_id'] == '0013h00001Vc5S1AAJ':\n",
        "                response = sf.Opportunity.create({'Billing_Start_Date__c': row['billing_start_date'],'Invoice_Summary_Preview2__c': row['invoice_short_url'], 'Invoice_ID__c':row['invoice_id'], 'Name': row['op_name'], 'OwnerId': opp_owner_id, 'Type': opp_type, 'StageName': opp_stage_name, 'CloseDate': row['closeDate'], 'AccountId': row['sfdc_account_id'], 'BVS_Sales_Rep__c': '', 'BVS_Invoice_Terms__c': row['BVSInvoiceTerms'], 'RecordTypeId': opp_record_type_id, 'Pricebook2Id': opp_pricebook_id, 'Order_Date__c': row['orderdate'], 'PO_Number__c': ''})\n",
        "            elif row['sfdc_advertiser_id'] == '':\n",
        "                response = sf.Opportunity.create({'Billing_Start_Date__c': row['billing_start_date'],'Invoice_Summary_Preview2__c': row['invoice_short_url'], 'Invoice_ID__c':row['invoice_id'], 'Name': row['op_name'], 'OwnerId': opp_owner_id, 'Type': opp_type, 'StageName': opp_stage_name, 'CloseDate': row['closeDate'], 'AccountId': row['sfdc_account_id'], 'BVS_Sales_Rep__c': '', 'BVS_Invoice_Terms__c': row['BVSInvoiceTerms'], 'RecordTypeId': opp_record_type_id, 'Pricebook2Id': opp_pricebook_id, 'Order_Date__c': row['orderdate'], 'PO_Number__c': ''})\n",
        "            else:\n",
        "                response = sf.Opportunity.create({'Billing_Start_Date__c': row['billing_start_date'],'Invoice_Summary_Preview2__c': row['invoice_short_url'], 'Invoice_ID__c':row['invoice_id'], 'Name': row['op_name'], 'OwnerId': opp_owner_id, 'Type': opp_type, 'StageName': opp_stage_name, 'CloseDate': row['closeDate'], 'AccountId': row['sfdc_account_id'], 'BVS_Sales_Rep__c': '', 'BVS_Invoice_Terms__c': row['BVSInvoiceTerms'], 'RecordTypeId': opp_record_type_id, 'Pricebook2Id': opp_pricebook_id, 'Order_Date__c': row['orderdate'], 'PO_Number__c': '', 'Advertiser_Object__c': row['sfdc_advertiser_id']})\n",
        "            print(response)\n",
        "            stage_update_response = sf.Opportunity.update(response['id'], {'StageName': opp_stage_name, 'Invoice_Summary_Preview2__c': row['invoice_short_url']})\n",
        "            print(stage_update_response)\n",
        "            # Save the new Opportunity ID to the dataframe\n",
        "            approvals_summary_df.at[index, 'sfdc_opportunity_id'] = response['id']\n",
        "        except Exception as e:\n",
        "            print(f\"Error while uploading Opportunity at index {index}: {e}\")\n",
        "\n",
        "    print(\"Uploaded data to Salesforce Opportunities and retrieved new IDs\")\n",
        "    print(approvals_summary_df)\n",
        "\n",
        "    sfdc_op_ids_to_invoice = approvals_summary_df[['invoice_id', 'sfdc_opportunity_id']].drop_duplicates().copy()\n",
        "    sfdc_op_ids_to_invoice.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Create a mapping from 'invoice_id' to 'new_value'\n",
        "    mapping = sfdc_op_ids_to_invoice.set_index('invoice_id')['sfdc_opportunity_id']\n",
        "    approvals_detail_df['sfdc_opportunity_id'] = approvals_detail_df['invoice_id'].map(mapping).fillna(approvals_detail_df['sfdc_opportunity_id'])\n",
        "\n",
        "    for index, row in approvals_detail_df.iterrows():\n",
        "        try:\n",
        "            # Create the Opportunity Line Item\n",
        "            if row['invoice_id'] == blisspoint_invoice_id:\n",
        "                response = sf.OpportunityLineItem.create({\n",
        "                    'OpportunityId': row['sfdc_opportunity_id'],\n",
        "                    'Product2Id': row['Product2Id'],\n",
        "                    # 'ProductCode': row['ProductCode'],\n",
        "                    'Description': row['LineDesc'],\n",
        "                    'Quantity': row['quantity'],\n",
        "                    'TotalPrice': row['li_total'],  # Or 'TotalPrice': row['TotalPrice']\n",
        "                    # Include any other optional fields as needed\n",
        "                })\n",
        "                print(f\"Opportunity Line Item created with ID: {response['id']}\")\n",
        "            else:\n",
        "                response = sf.OpportunityLineItem.create({\n",
        "                    'OpportunityId': row['sfdc_opportunity_id'],\n",
        "                    'Product2Id': row['Product2Id'],\n",
        "                    # 'ProductCode': row['ProductCode'],\n",
        "                    'Description': row['LineDesc'],\n",
        "                    'Quantity': row['quantity'],\n",
        "                    'UnitPrice': row['price'],  # Or 'TotalPrice': row['TotalPrice']\n",
        "                    # Include any other optional fields as needed\n",
        "                })\n",
        "                print(f\"Opportunity Line Item created with ID: {response['id']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error while uploading Opportunity Line Item at index {index}: {e}\")\n",
        "\n",
        "    processed_opps_query = f\"\"\"\n",
        "        SELECT Id, Invoice_Id__c as invoice_id from `adhoc-billing.avs_billing_process.sfdc_task_obj`\n",
        "        WHERE lower(Status) in ('approve to bill')\n",
        "        AND RecordTypeId = '012UV000000KpzFYAS'\n",
        "        \"\"\"\n",
        "    processed_opps_df = bigquery_client.query(processed_opps_query).to_dataframe()\n",
        "    processed_opps_df['sfdc_opportunity_id'] = processed_opps_df['invoice_id'].map(mapping).fillna(approvals_detail_df['sfdc_opportunity_id'])\n",
        "\n",
        "    for index, row in processed_opps_df.iterrows():\n",
        "        try:\n",
        "            sf.Task.update(row['Id'], {'Status': 'Opportunity Created'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error while updating Task at index {index}: {e}\")\n",
        "\n",
        "    table_1 = f'{project_id}.{dataset_id}._export_sfdc_opp_detail'\n",
        "    table_2 = f'{project_id}.{dataset_id}._export_sfdc_opp_summary'\n",
        "\n",
        "    update_bq_sfdc_opportunity_id(sfdc_op_ids_to_invoice, table_1)\n",
        "    update_bq_sfdc_opportunity_id(sfdc_op_ids_to_invoice, table_2)\n",
        "\n",
        "    sfdc_objs = [\"Task\",\"Opportunity\",\"OpportunityLineItem\"]\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, sfdc_objs)\n",
        "\n"
      ],
      "metadata": {
        "id": "UtHZ193I8L9f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hx5dVLEU1jbZ"
      },
      "outputs": [],
      "source": [
        "# @title Sync BVS Data to SFDC functions\n",
        "# Define the GBQ query and field mapping\n",
        "\n",
        "def sync_bvs_customers_to_sfdc():\n",
        "\n",
        "    sfdc_object_name = 'BVS_Customer__c'\n",
        "\n",
        "\n",
        "    sfdc_external_id_field = 'customer_id__c'\n",
        "\n",
        "    sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "    print(sfdc_max_updated)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT customer_id, account_id, contract_item, customer_name, contract_number, sales_person_code, deleted, last_audit_id, last_updated\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.customers`\n",
        "        WHERE customer_id in (select customer_id from `{project_id}.{dataset_id}.new_or_updated_customer`)\n",
        "    \"\"\"\n",
        "\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'customer_id': 'customer_id__c',\n",
        "        'account_id': 'account_id__c',\n",
        "        'contract_item': 'Contract_Item__c',\n",
        "        'customer_name': 'Name',\n",
        "        'contract_number': 'Contract_Number__c',\n",
        "        'sales_person_code': 'Sales_Person_Code__c',\n",
        "        'deleted': 'Is_Deleted__c',\n",
        "        'last_audit_id': 'Last_Audit_ID__c',\n",
        "        'last_updated': 'Last_Updated__c'\n",
        "    }\n",
        "\n",
        "    # Sync BVS_Customer__c object\n",
        "    response = sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "    else:\n",
        "        print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_profiles_to_sfdc():\n",
        "    sfdc_object_name = 'BVS_Profile__c'\n",
        "\n",
        "    sfdc_external_id_field = 'BVS_Profile_ID__c'\n",
        "\n",
        "    sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT profile_id, profile_name, deleted, default_asset_code, last_audit_id, last_updated, ARRAY_TO_STRING(\n",
        "                ARRAY(\n",
        "                    SELECT TO_JSON_STRING(attr)\n",
        "                    FROM UNNEST(attributes) AS attr\n",
        "                ), ', '\n",
        "            ) AS attributes\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.profiles`\n",
        "        WHERE profile_id in (select profile_id from `{project_id}.{dataset_id}.new_or_updated_profile`)\n",
        "    \"\"\"\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'profile_id': 'BVS_Profile_ID__c',\n",
        "        'profile_name': 'Name',\n",
        "        'deleted': 'Is_Deleted__c',\n",
        "        'default_asset_code': 'Default_Asset_Code__c',\n",
        "        'last_audit_id': 'Last_Audit_ID__c',\n",
        "        'last_updated': 'Last_Updated__c',\n",
        "        'attributes': 'Attributes__c'\n",
        "    }\n",
        "\n",
        "    # Sync BVS_Customer__c object\n",
        "    response = sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "    else:\n",
        "        print(\"Sync completed.\")\n",
        "\n",
        "    # upadte the bvs_customer_id lookup object in sfdc\n",
        "    print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_formats_to_sfdc():\n",
        "    sfdc_object_name = 'BVS_Format__c'\n",
        "\n",
        "    sfdc_external_id_field = 'BVS_Format_ID__c'\n",
        "\n",
        "    sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "    # Define the GBQ query and field mapping\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT format_id, profile_id, customer_id, format_name, report_breakup, deleted, last_audit_id, last_updated\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.formats`\n",
        "        WHERE format_id in (select format_id from `{project_id}.{dataset_id}.new_or_updated_format`)\n",
        "    \"\"\"\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'format_id': 'BVS_Format_ID__c',\n",
        "        'profile_id': 'Profile_ID__c',\n",
        "        'customer_id': 'Customer_ID__c',\n",
        "        'format_name': 'Name',\n",
        "        'report_breakup': 'report_breakup__c',\n",
        "        'deleted': 'Is_Deleted__c',\n",
        "        'last_audit_id': 'Last_Audit_ID__c',\n",
        "        'last_updated': 'Last_Updated__c'\n",
        "    }\n",
        "\n",
        "    # Sync BVS_Customer__c object\n",
        "    response = sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "\n",
        "    update_bvs_customers_lookup_on_format(sf)\n",
        "\n",
        "    update_bvs_profile_lookup_on_format(sf)\n",
        "\n",
        "    print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_encoder_groups_to_sfdc():\n",
        "    sfdc_object_name = 'Encoder_Group__c'\n",
        "\n",
        "    sfdc_external_id_field = 'Encoder_Group_ID__c'\n",
        "\n",
        "    sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT encoder_group_id, encoder_group_name, deleted, last_audit_id, last_updated\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.encoder_groups`\n",
        "        WHERE encoder_group_id in (select encoder_group_id from `{project_id}.{dataset_id}.new_or_updated_encoder_group`)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'encoder_group_id': 'Encoder_Group_ID__c',\n",
        "        'encoder_group_name': 'Name',\n",
        "        'deleted': 'Is_Deleted__c',\n",
        "        'last_audit_id': 'Last_Audit_ID__c',\n",
        "        'last_updated': 'Last_Updated__c'\n",
        "    }\n",
        "\n",
        "\n",
        "    # Sync Encoder_Group__c object\n",
        "    response = sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "    # upadte the bvs_customer_id lookup object in sfdc\n",
        "    print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_encoder_devices_to_sfdc():\n",
        "    sfdc_object_name = 'Encoder_Device__c'\n",
        "\n",
        "    sfdc_external_id_field = 'bvs_encoder_device_id__c'\n",
        "\n",
        "    # sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT encoder_id, encoder_name, encoder_group_id, deleted, last_audit_id, last_updated\n",
        "        FROM `bigquery-sandbox-393916.prod_avs.encoders`\n",
        "        WHERE encoder_id in (select encoder_id from `{project_id}.{dataset_id}.new_or_updated_encoder`)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'encoder_id': 'bvs_encoder_device_id__c',\n",
        "        'encoder_name': 'BVS_Encoder_Name__c',\n",
        "        'encoder_group_id': 'bvs_encoder_group_id__c',\n",
        "        'deleted': 'Is_Deleted__c',\n",
        "        'last_audit_id': 'Last_Audit_ID__c',\n",
        "        'last_updated': 'Last_Updated__c'\n",
        "    }\n",
        "\n",
        "    # Sync Encoder_Group__c object\n",
        "    response = sync_data_ids(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "\n",
        "    # upadte the bvs_customer_id lookup object in sfdc\n",
        "    print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_encoding_advertiserss_to_sfdc():\n",
        "    sfdc_object_name = 'Encoding_Advertiser__c'\n",
        "\n",
        "    sfdc_external_id_field = 'Name'\n",
        "\n",
        "    # sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "    SELECT sfdc_customer_id, bvs_encoding_advertiser_id, advertiser\n",
        "        FROM `{project_id}.{dataset_id}.encodings_advertisers`\n",
        "\n",
        "        where trim(bvs_encoding_advertiser_id) not in (\n",
        "            select trim(Name) from `{project_id}.{dataset_id}.sfdc_encoding_advertiser__c_obj`\n",
        "        )\n",
        "        GROUP BY sfdc_customer_id, bvs_encoding_advertiser_id, advertiser\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'bvs_encoding_advertiser_id': 'Name',\n",
        "        'sfdc_customer_id': 'BVS_Customer__c',\n",
        "        'advertiser': 'Encoding_Advertiser__c',\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    # Sync Encoding_Advertiser__c object\n",
        "    response = sync_data_with_precalculated_id(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "\n",
        "    # upadte the bvs_customer_id lookup object in sfdc\n",
        "    print(\"Sync completed.\")\n",
        "\n",
        "def sync_bvs_encoding_product_codes_to_sfdc():\n",
        "    sfdc_object_name = 'Encoding_Product_Code__c'\n",
        "\n",
        "    sfdc_external_id_field = 'Name'\n",
        "\n",
        "    # sfdc_max_updated = fetch_max_last_updated(sfdc_object_name)\n",
        "\n",
        "\n",
        "    gbq_query = f\"\"\"\n",
        "        SELECT sfdc_customer_id, bvs_encoding_product_code_id, product_code\n",
        "        FROM `{project_id}.{dataset_id}.encodings_product_codes`\n",
        "        where trim(bvs_encoding_product_code_id) not in\n",
        "        (select trim(Name) from `{project_id}.{dataset_id}.sfdc_encoding_product_code__c_obj`)\n",
        "        GROUP BY sfdc_customer_id, bvs_encoding_product_code_id, product_code\n",
        "    \"\"\"\n",
        "    gbq_to_sfdc_field_map = {\n",
        "        'bvs_encoding_product_code_id': 'Name',\n",
        "        'sfdc_customer_id': 'BVS_Customer__c',\n",
        "        'product_code': 'Encoding_Product_Code__c',\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    # Sync Encoding_Product_Code__c object\n",
        "    response = sync_data_with_precalculated_id(gbq_query, gbq_to_sfdc_field_map, sfdc_object_name, sfdc_external_id_field)\n",
        "    if response:\n",
        "        custom_sync_objs.append(response)\n",
        "    # upadte the bvs_customer_id lookup object in sfdc\n",
        "    print(\"Sync completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQzn4So31nP0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data Prep Process Control\n",
        "\n",
        "if sync_salesforce_with_bvs_before_processing:\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_salesforce_type)\n",
        "    custom_sync_objs = []\n",
        "\n",
        "if sync_bvs_with_sfdc_before_processing:\n",
        "    sync_bvs_customers_to_sfdc()\n",
        "    sync_bvs_profiles_to_sfdc()\n",
        "    sync_bvs_formats_to_sfdc()\n",
        "    sync_bvs_encoder_groups_to_sfdc()\n",
        "    sync_bvs_encoder_devices_to_sfdc()\n",
        "    sync_bvs_encoding_advertiserss_to_sfdc()\n",
        "    sync_bvs_encoding_product_codes_to_sfdc()\n",
        "    print(\"Full sync completed.\")\n",
        "else:\n",
        "    print(\"Sync skipped.\")\n",
        "\n",
        "if sync_bvs_with_sfdc_before_processing:\n",
        "    sync_type = \"FULL\"\n",
        "    sync_salesforce_tables(sync_salesforce_url, sync_salesforce_auth_token, sync_type, custom_sync_objs)\n",
        "\n",
        "if rebuild_encodings:\n",
        "    update_encodings_base()\n",
        "    get_unmatched_advertisers()\n",
        "    get_unmatched_accounts()\n",
        "else:\n",
        "    print(\"Skipping encodings base update\")\n",
        "\n",
        "for bcm_index in bcm_index_list:\n",
        "    print(f\"Processing bcm_index: {bcm_index}\")\n",
        "    if rebuild_detection_archive:\n",
        "        load_detections_into_archive(bcm_index)\n",
        "    else:\n",
        "        print(\"Skipping detection archive update\")\n",
        "\n",
        "bucket_name = storage_bucket_name\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "sfdc_opp_summary_columns = ['invoice_id', 'sfdc_opportunity_id', 'broadcast_month_name', 'sfdc_account_id', 'sfdc_advertiser_id', 'op_name', 'ownerId', 'closeDate', 'poNumber', 'stageName', 'orderdate', 'billing_start_date', 'billing_end_date', 'type', 'BVSInvoiceTerms', 'SOSalesRep', 'PriceBook2Id', 'RecordTypeId', 'invoice_short_url']\n",
        "sfdc_opp_detail_columns = ['invoice_id',  'sfdc_opportunity_id', 'broadcast_month_name', 'LineDesc', 'Product2Id', 'ProductCode', 'quantity', 'price', 'li_total', 'reporting_cust_name', 'reporting_adv_name']\n",
        "\n",
        "sfdc_opp_summary_table_name = '_export_sfdc_opp_summary'  # Replace with your desired table name\n",
        "sfdc_opp_summary_table_id = f'{project_id}.{dataset_id}.{sfdc_opp_summary_table_name}'\n",
        "\n",
        "sfdc_opp_detail_table_name = '_export_sfdc_opp_detail'  # Replace with your desired table name\n",
        "sfdc_opp_detail_table_id = f'{project_id}.{dataset_id}.{sfdc_opp_detail_table_name}'\n",
        "today = datetime.now().date()\n",
        "\n",
        "# Extract year and month from bcm_index\n",
        "year = int(bcm_index)\n",
        "# Multiply fractional part by 100 and round to get the month\n",
        "month = int(round((bcm_index - year) * 100))\n",
        "bcm_index_str = f\"{year}-{month:02d}\"\n",
        "first_month_date = f\"{bcm_index_str}-01\"\n",
        "billing_start_date = first_month_date\n",
        "last_day = calendar.monthrange(year, month)[1]\n",
        "billing_end_date = f\"{bcm_index_str}-{last_day:02d}\"\n",
        "\n",
        "if gen_encoder_excel:\n",
        "    generate_encoder_excel(bcm_index)\n",
        "else:\n",
        "    print(\"Skipping encoder excel generation\")\n",
        "\n",
        "if gen_weekly_excel:\n",
        "    generate_weekly_excel(bcm_index)\n",
        "else:\n",
        "    print(\"Skipping weekly excel generation\")\n",
        "\n",
        "if gen_monthly_excel:\n",
        "    generate_monthly_excel(bcm_index)\n",
        "else:\n",
        "    print(\"Skipping monthly excel generation\")\n",
        "\n",
        "if generate_sfdc_tasks:\n",
        "    initial_task_upload(bcm_index)\n",
        "    update_sfdc_opp_url()\n",
        "else:\n",
        "    print(\"Skipping sfdc task generation\")\n",
        "\n",
        "if process_sfdc_opps_from_invoices:\n",
        "    update_tasks_needing_adjustment()\n",
        "    process_approved_invoices()\n",
        "else:\n",
        "    print(\"Skipping sfdc opp processing\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "li2-3zxr7fj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_unmatched_advertisers()\n"
      ],
      "metadata": {
        "id": "JNod5IKnC6bJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}