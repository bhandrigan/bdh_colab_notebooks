{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functions.core.common_functions as cf\n",
    "import importlib\n",
    "\n",
    "importlib.reload(cf)\n",
    "\n",
    "config_path = \"/home/developer/keys/project-keys/colab-settings.yaml\"\n",
    "# service_account_secret_name = \"SA_ADHOC_BILLING\"\n",
    "\n",
    "# # Initialize client manager\n",
    "# veil_client_manager = cf.GCPClientManager(config_path, service_account_secret_name)\n",
    "\n",
    "# # Get clients\n",
    "# veil_bigquery_client = veil_client_manager.get_bigquery_client()\n",
    "# veil_storage_client = veil_client_manager.get_storage_client()\n",
    "# veil_salesforce_client = veil_client_manager.get_salesforce_client()\n",
    "\n",
    "# print(\"Veil clients initialized successfully!\")\n",
    "\n",
    "# service_account_secret_name = \"SA_N90_CORE_APPS\"\n",
    "\n",
    "# # Initialize client manager\n",
    "# n90_client_manager = cf.GCPClientManager(config_path, service_account_secret_name)\n",
    "\n",
    "# # Get clients\n",
    "# n90_bigquery_client = n90_client_manager.get_bigquery_client()\n",
    "# n90_storage_client = n90_client_manager.get_storage_client()\n",
    "\n",
    "# print(\"N90 clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "import functions.core.core_functions as core_functions\n",
    "import functions.pyarrow_functions as pyarrow_functions\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe.utils import assert_eq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import json\n",
    "import gc\n",
    "import cudf\n",
    "import os\n",
    "import yaml\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "# import uu\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "\n",
    "importlib.reload(core_functions)\n",
    "importlib.reload(pyarrow_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = yaml.safe_load(open('table-schemas.yaml'))\n",
    "\n",
    "detections_python = table_schema['detections_python_schema']\n",
    "detections_python\n",
    "\n",
    "n90_schema_dict = {item['name']: item['type'] for item in detections_python}\n",
    "veil_schema_dict = {item['name']: item['type'] for item in detections_python if 'geo_' not in item['name']}\n",
    "final_detections_cols = []\n",
    "for item in detections_python:\n",
    "    final_detections_cols.append(item['name'])\n",
    "    # print(f\"{item['name']} = {item['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = {}\n",
    "resp = core_functions.initialize_clients(service_account_secret_name='SA_ADHOC_BILLING')\n",
    "resp2 = core_functions.initialize_clients(service_account_secret_name='SA_N90_CORE_APPS')\n",
    "\n",
    "config = resp.get('config')\n",
    "bigquery_client = resp.get('clients').get('bigquery_client')\n",
    "n90_bigquery_client = resp2.get('clients').get('bigquery_client')\n",
    "storage_client = resp.get('clients').get('storage_client')\n",
    "sf_client = resp.get('clients').get('sf_client')\n",
    "veil_billing = resp.get('config').get('veil_billing')\n",
    "veil_vars = resp.get('config').get('veil_billing').get('vars')\n",
    "# print(veil_billing)\n",
    "sfdc_adv_account_cols = veil_billing.get('vars').get('sfdc_adv_account_cols')\n",
    "sfdc_rate_card_cols = veil_billing.get('vars').get('sfdc_rate_card_cols')\n",
    "unknown_dma_overrides = config.get('national_dma_overrides_to_us_national')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_ADHOC_BILLING')\n",
    "broadcast_cal_sql = f\"\"\"\n",
    "    SELECT id as bcw_id, bcw_index, bcm_index, bcw_start_date, bcw_start_datetime_match, bcw_end_date, bcw_end_datetime_match FROM `adhoc-billing.avs_billing_process.lu_broadcast_week`\n",
    "\"\"\"\n",
    "broadcast_cal_df = core_functions.fetch_gbq_data(query=broadcast_cal_sql, bigquery_client=bigquery_client)\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_cal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "geo_zip_sql = f\"\"\"\n",
    "    SELECT *\n",
    "    from `next90-core-applications.next90_analytics.geos` WHERE geo_type = 'zip'\n",
    "    AND geo_country in ('United States', 'Canada')\n",
    "\"\"\"\n",
    "geo_zip_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(query=geo_zip_sql, bigquery_client=n90_bigquery_client))\n",
    "geo_zip_df['geo_latitude'] = geo_zip_df['geo_latitude'].astype('Float64')\n",
    "geo_zip_df['geo_longitude'] = geo_zip_df['geo_longitude'].astype('Float64')\n",
    "\n",
    "\n",
    "geo_dma_sql = f\"\"\"\n",
    "    SELECT *\n",
    "    from `next90-core-applications.next90_analytics.geos` WHERE geo_type = 'dma'\n",
    "    AND geo_country in ('United States', 'Canada')\n",
    "\"\"\"\n",
    "geo_dma_df = core_functions.fix_df_dtypes(core_functions.fetch_gbq_data(query=geo_dma_sql, bigquery_client=n90_bigquery_client))\n",
    "geo_dma_df['geo_latitude'] = geo_dma_df['geo_latitude'].astype('Float64')\n",
    "geo_dma_df['geo_longitude'] = geo_dma_df['geo_longitude'].astype('Float64')\n",
    "\n",
    "\n",
    "\n",
    "# int_cols = ['geo_neustar_id','geo_us_msa_id', 'geo_us_county_fips_id','geo_ca_cma_id']\n",
    "# for col in int_cols:\n",
    "#     geo_zip_df[col] = geo_df[col].fillna(-1).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "# broadcast_cal_sql = f\"\"\"\n",
    "#     SELECT id as bcw_id, bcw_index, bcm_index, bcw_start_date, bcw_end_date FROM `next90-core-applications.n90_data_lake.lu_broadcast_week`\n",
    "# \"\"\"\n",
    "# broadcast_cal_df = core_functions.fetch_gbq_data(query=broadcast_cal_sql, bigquery_client=n90_bigquery_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_dma_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = '2023-01-01'\n",
    "data_end_date = '2024-01-01'\n",
    "\n",
    "process_sql = f\"\"\"\n",
    "    SELECT distinct EXTRACT(YEAR FROM created_time) as year, EXTRACT(MONTH FROM created_time) as month, EXTRACT(DAY FROM created_time) as  day\n",
    " from `next90-core-applications.omniData.activity_sessions`\n",
    "    WHERE created_time >= '{data_start_date}' and created_time < '{data_end_date}'\n",
    "    order by year, month, day\n",
    "\"\"\"\n",
    "\n",
    "process_df = core_functions.fetch_gbq_data(process_sql, n90_bigquery_client)\n",
    "process_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_days = process_df.to_dict(orient='records')\n",
    "process_days\n",
    "for record in process_days:\n",
    "    print(f'year: {record[\"year\"]}, month: {record[\"month\"]}, day: {record[\"day\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery, bigquery_storage\n",
    "\n",
    "def process_activity_sessions_segments(_df, _bcc_df, geo_z_df, geo_d_df, media='DIGITAL'):\n",
    "    print('starting process_activity_sessions_segments')\n",
    "    print('adding broadcast cal details')\n",
    "    print(f'length of df: {len(_df)}')\n",
    "    \n",
    "    # Ensure bcw_monday_match and created_time are datetime\n",
    "    _df['activity_date_time'] = pd.to_datetime(_df['created_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    _df['bcw_monday_match'] = pd.to_datetime(_df['bcw_monday_match'])\n",
    "    _df['click_view_gclid'] = _df['gclid'].astype('string')\n",
    "\n",
    "    # Find the broadcast week boundaries\n",
    "    min_date = _df['bcw_monday_match'].min()\n",
    "    max_date = _df['bcw_monday_match'].max()\n",
    "\n",
    "    # Ensure the Mondays before and after are captured\n",
    "    min_monday = min_date - pd.Timedelta(days=7 if min_date.weekday() == 0 else min_date.weekday())\n",
    "    max_monday = max_date + pd.Timedelta(days=7 - max_date.weekday())\n",
    "\n",
    "    # Generate a full list of Mondays between min and max\n",
    "    monday_match_dates = pd.date_range(start=min_monday, end=max_monday, freq='7D').date.tolist()\n",
    "\n",
    "    # Filter _bcc_df to include only valid broadcast weeks\n",
    "    _bcc_df['bcw_start_date'] = pd.to_datetime(_bcc_df['bcw_start_date'])\n",
    "    ref_df = _bcc_df.loc[\n",
    "        _bcc_df['bcw_start_date'].dt.date.isin(monday_match_dates)\n",
    "    ].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Merge _df and _bcc_df based on date conditions\n",
    "    merged_df = pd.merge(_df, ref_df, how='cross')  # Cartesian join (safer with cross in pandas 1.2+)\n",
    "    print(f\"Rows after Cartesian join: {len(merged_df)}\")\n",
    "    merged_df = merged_df.loc[\n",
    "        (merged_df['activity_date_time'] >= merged_df['bcw_start_datetime_match']) &\n",
    "        (merged_df['activity_date_time'] < merged_df['bcw_end_datetime_match'])\n",
    "    ].reset_index(drop=True)\n",
    "    print(f\"Rows after activity date filter: {len(merged_df)}\")\n",
    "\n",
    "    # Final processing\n",
    "    merged_df['bcw_year'] = merged_df['bcw_index'].astype(str).str[:4].astype('Int64')\n",
    "    merged_df['bcm_index'] = merged_df['bcm_index'].astype('Float64')\n",
    "    merged_df['bcw_index'] = merged_df['bcw_index'].astype('Float64')\n",
    "\n",
    "\n",
    "    _df = merged_df.copy().reset_index(drop=True)\n",
    "    del merged_df\n",
    "    # del merged_df2\n",
    "    gc.collect()\n",
    "    _df['_YEAR'] = _df['created_time'].dt.year.astype('Int64')\n",
    "    _df['_MONTH'] = _df['created_time'].dt.month.astype('Int64')\n",
    "    _df['_WEEK_WITHIN_MONTH'] = _df['week_within_month'].astype('Int64')\n",
    "    _df['_DAY'] = _df['created_time'].dt.day.astype('Int64')\n",
    "    print('adding geos')\n",
    "    activity_sessions_with_geos_df = _df.merge(geo_z_df, how='left', left_on='zip_code', right_on='geo_location')\n",
    "    del _df\n",
    "    gc.collect()\n",
    "    activity_sessions_with_geos_df['neustar_dma_id'] = activity_sessions_with_geos_df['neustar_dma_id'].astype('string')\n",
    "    activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()] = activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()].merge(geo_d_df, how='left', left_on='neustar_dma_id', right_on='geo_location')\n",
    "    activity_sessions_with_geos_df['neustar_dma_id'] = activity_sessions_with_geos_df['neustar_dma_id'].astype('Int64')\n",
    "    # activity_sessions_with_geos_df.sort_values(by=['activity_session_id', 'created_time'], ascending=[True, False] ,inplace=True)\n",
    "    activity_sessions_with_geos_df = activity_sessions_with_geos_df.dropna(subset=['activity_session_id'])\n",
    "    _df = activity_sessions_with_geos_df.copy().reset_index(drop=True)\n",
    "    # .drop_duplicates(subset=['activity_session_id'], keep='first').sort_values(by='created_time').copy().reset_index(drop=True)\n",
    "    del activity_sessions_with_geos_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('adding segments')\n",
    "    _df['segments_date'] = pd.to_datetime(_df['created_time'])\n",
    "    _df['segments_day_of_week'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.day_name().astype('string')\n",
    "    _df['segments_media'] =  media\n",
    "        \n",
    "    _df['segments_month_label'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('M').astype('string')\n",
    "    _df['segments_quarter_label'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('Q').astype('string')\n",
    "    _df['segments_week_label'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('W').astype('string')\n",
    "\n",
    "    # Convert timestamp to periods and then use start_time to get the first day of the period\n",
    "    _df['segments_month'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('M').dt.start_time.dt.date.astype('string')\n",
    "    _df['segments_quarter'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('Q').dt.start_time.dt.date.astype('string')\n",
    "    _df['segments_week'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('W').dt.start_time.dt.date.astype('string')\n",
    "\n",
    "    # Year can remain as a period or also be converted similarly if needed\n",
    "    _df['segments_year'] = pd.to_datetime(_df['segments_date']).dt.tz_localize(None).dt.to_period('Y').astype('string')\n",
    "    _df['segments_broadcast_year'] = _df['bcw_year'].astype('Int64')\n",
    "    _df['segments_broadcast_month_index'] = _df['bcm_index'].astype('Float64')\n",
    "    _df['segments_broadcast_week_index'] = _df['bcw_index'].astype('Float64')\n",
    "    # _df['_YEAR'] = _df['segments_date'].dt.year.astype('Int64')\n",
    "    # _df['_MONTH'] = _df['segments_date'].dt.month.astype('Int64')\n",
    "    # _df['_DAY'] = _df['segments_date'].dt.day.astype('Int64')\n",
    "    _df['segments_date'] = _df['segments_date'].dt.date.astype('string')\n",
    "    _df['session_timestamp'] = _df['created_time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    _df['created_time'] = _df['created_time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    _df['bcw_start_date'] = _df['bcw_start_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    _df['bcw_end_date'] = _df['bcw_end_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('finished process_activity_sessions_segments')\n",
    "    \n",
    "\n",
    "    return _df\n",
    "\n",
    "def process_single_day(record, broadcast_cal_df, geo_zip_df, geo_dma_df, n90_bucket, n90_output_prefix, s4_bucket, s4_output_prefix, partition_cols, n90_storage_options, s4_storage_options):\n",
    "    try:\n",
    "        year = record['year']\n",
    "        month = record['month']\n",
    "        day = record['day']\n",
    "        print(f'Processing year: {year}, month: {month}, day: {day}')\n",
    "\n",
    "        activity_sessions_sql = f\"\"\"\n",
    "            WITH temp_activity_sessions AS (\n",
    "            SELECT DISTINCT s.*, CAST((EXTRACT(DAY FROM created_time) - 1) / 7 + 1 AS INT64) AS week_within_month,\n",
    "            row_number() over (partition by s.id order by s.created_time desc) as row_num\n",
    "            FROM `next90-core-applications.omniData.temp_activity_sessions`  s\n",
    "            WHERE EXTRACT(YEAR FROM created_time) = {year}\n",
    "            AND EXTRACT(MONTH FROM created_time) = {month}\n",
    "            AND EXTRACT(DAY FROM created_time) = {day}\n",
    "            )\n",
    "            SELECT * FROM temp_activity_sessions\n",
    "            where row_num = 1\n",
    "        \"\"\"\n",
    "\n",
    "        # Load data\n",
    "        print('Loading activity sessions')\n",
    "        # try:\n",
    "        #     activity_sessions_df = core_functions.fetch_gbq_data(activity_sessions_sql, n90_bigquery_client)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error loading activity sessions with core_functions, reading directly: {e}\")\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "        activity_sessions_df = bigquery.Client().query(activity_sessions_sql).result().to_dataframe()\n",
    "        # activity_sessions_df = bigquery.Client(credentials=config.get('SA_N90_CORE_APPS')).query(activity_sessions_sql).result().to_dataframe()\n",
    "        \n",
    "        activity_sessions_df.rename(columns={'id': 'activity_session_id'}, inplace=True)\n",
    "        activity_sessions_df['lat'] = activity_sessions_df['lat'].astype('Float64')\n",
    "        activity_sessions_df['lon'] = activity_sessions_df['lon'].astype('Float64')\n",
    "\n",
    "        print('Processing with function process_activity_sessions_segments')\n",
    "        activity_sessions_df = process_activity_sessions_segments(activity_sessions_df, broadcast_cal_df, geo_zip_df, geo_dma_df)\n",
    "\n",
    "        # Add metadata\n",
    "        activity_session_last_updated = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        activity_sessions_df['activity_session_last_updated'] = activity_session_last_updated\n",
    "        activity_session_last_audit_id = core_functions.generate_uuid()\n",
    "        activity_sessions_df['activity_session_last_audit_id'] = activity_session_last_audit_id\n",
    "\n",
    "        # Write to GCS\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.get('SA_N90_CORE_APPS')\n",
    "        core_functions.write_hive_partitioned_parquet(activity_sessions_df, n90_bucket, n90_output_prefix, partition_cols, n90_storage_options)\n",
    "        print(f\"Finished writing to {n90_bucket}/{n90_output_prefix}\")\n",
    "\n",
    "        # Write to S4 (try-catch for error handling)\n",
    "        try:\n",
    "            core_functions.write_hive_partitioned_parquet_s4(activity_sessions_df, s4_bucket, s4_output_prefix, partition_cols, s4_storage_options)\n",
    "            print(f\"Finished writing to {s4_bucket}/{s4_output_prefix}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to {s4_bucket}/{s4_output_prefix}: {e}\")\n",
    "\n",
    "        del activity_sessions_df\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {record}: {e}\")\n",
    "        \n",
    "# Parallel processing\n",
    "def process_all_days_concurrently(process_days, broadcast_cal_df, geo_zip_df, geo_dma_df, n90_bucket, n90_output_prefix, s4_bucket, s4_output_prefix, partition_cols, n90_storage_options, s4_storage_options):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:  # Adjust workers as needed\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_single_day, \n",
    "                record, broadcast_cal_df, geo_zip_df, geo_dma_df, \n",
    "                n90_bucket, n90_output_prefix, s4_bucket, s4_output_prefix, \n",
    "                partition_cols, n90_storage_options, s4_storage_options\n",
    "            ) \n",
    "            for record in process_days\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred in a worker: {e}\")\n",
    "\n",
    "# Example usage\n",
    "print('Beginning to write')\n",
    "partition_cols = ['_YEAR', '_MONTH', '_DAY']\n",
    "\n",
    "\n",
    "n90_storage_options = None\n",
    "n90_storage_options = config.get('N90_GCS_STORAGE_OPTIONS')\n",
    "\n",
    "s4_storage_options = None\n",
    "s4_storage_options = config.get('S4_STORAGE_OPTIONS')\n",
    "\n",
    "# process_df['profile__attributes']\n",
    "\n",
    "s4_bucket = 'n90-data-lake-stl'\n",
    "s4_output_prefix = 'activity/activity_sessions'\n",
    "\n",
    "\n",
    "n90_bucket = None\n",
    "n90_bucket = 'n90-data-lake'\n",
    "\n",
    "n90_output_prefix = None\n",
    "n90_output_prefix = 'activity/activity_sessions'\n",
    "\n",
    "\n",
    "process_all_days_concurrently(\n",
    "    process_days, broadcast_cal_df, geo_zip_df, geo_dma_df, \n",
    "    n90_bucket, n90_output_prefix, s4_bucket, s4_output_prefix, \n",
    "    partition_cols, n90_storage_options, s4_storage_options\n",
    ")\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[full_df['segments_date'].isin(['2024-11-24','2024-11-25','2024-11-26']), ['segments_date', 'activity_session_timestamp', '_YEAR', '_MONTH','_WEEK_WITHIN_MONTH', 'bcw_start_datetime_match', 'bcw_end_datetime_match', 'bcw_index']]\n",
    "# full_df['bcw_index'].unique()\n",
    "# activity_sessions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_monday_midnight_bcw_start_dates(min_date, max_date):\n",
    "    # Ensure inputs are datetime objects\n",
    "    min_date = datetime.strptime(min_date, \"%Y-%m-%d\")\n",
    "    max_date = datetime.strptime(max_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Start from the Monday of the week containing min_date\n",
    "    current_date = min_date - timedelta(days=min_date.weekday())\n",
    "    \n",
    "    # Generate list of Mondays as strings\n",
    "    monday_midnights = []\n",
    "    while current_date <= max_date:\n",
    "        monday_midnights.append(current_date.strftime(\"%Y-%m-%d\"))  # Convert to string\n",
    "        current_date += timedelta(weeks=1)  # Increment by 1 week\n",
    "    \n",
    "    return monday_midnights\n",
    "\n",
    "def generate_bcw_df(min_date, max_date, broadcast_cal_df):\n",
    "    # Ensure inputs are datetime objects\n",
    "    min_date = datetime.strptime(min_date, \"%Y-%m-%d\")\n",
    "    max_date = datetime.strptime(max_date, \"%Y-%m-%d\")\n",
    "    bcw_df = None\n",
    "    \n",
    "    # Start from the Monday of the week containing min_date\n",
    "    current_date = min_date - timedelta(days=min_date.weekday())\n",
    "    \n",
    "    # Generate list of Mondays as strings\n",
    "    monday_midnights = []\n",
    "    while current_date <= max_date:\n",
    "        monday_midnights.append(current_date.strftime(\"%Y-%m-%d\"))  # Convert to string\n",
    "        current_date += timedelta(weeks=1)  # Increment by 1 week\n",
    "    for date in monday_midnights:\n",
    "        print(date)\n",
    "        bcw_df = pd.concat([bcw_df, broadcast_cal_df.loc[broadcast_cal_df['bcw_start_date'] == date]])\n",
    "    \n",
    "    return bcw_df\n",
    "\n",
    "# test_df = activity_sessions_df[100000:300000]\n",
    "activity_sessions_df['key'] = 1\n",
    "min_date = (activity_sessions_df['created_time'].dt.date.astype('string')).min()\n",
    "max_date = (activity_sessions_df['created_time'].dt.date.astype('string')).max()\n",
    "bcw2_df = generate_bcw_df(min_date, max_date, broadcast_cal_df)\n",
    "bcw2_df['key'] = 1\n",
    "# _bcc_df['key'] = 1\n",
    "activity_sessions_df['activity_date_time'] = pd.to_datetime(activity_sessions_df['created_time'])\n",
    "bcw2_df['bcw_start_date'] = pd.to_datetime(bcw2_df['bcw_start_date'])\n",
    "bcw2_df['bcw_end_date'] = pd.to_datetime(bcw2_df['bcw_end_date'])\n",
    "# ref_df = None\n",
    "ref_df = bcw2_df.loc[(bcw2_df['bcw_start_date'] >= activity_sessions_df['created_time'].min()) & (bcw2_df['bcw_end_date'] <= activity_sessions_df['created_time'].max())]\n",
    "bcw2_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = None\n",
    "merged_df = pd.merge(activity_sessions_df, bcw2_df, on='key').drop(['key', 'created_time'], axis=1)\n",
    "merged_df['bc_year_index'] = merged_df['bcm_index'].astype(str).str[:4].astype('Int64')\n",
    "merged_df['bcm_index'] = merged_df['bcm_index'].astype('Float64')\n",
    "merged_df['bcw_index'] = merged_df['bcw_index'].astype('Float64')\n",
    "\n",
    "merged_df.sort_values(by=['id'], inplace=True)\n",
    "merged_df = merged_df.drop_duplicates(subset=['id'], keep='first')\n",
    "# df = merged_df\n",
    "\n",
    "\n",
    "\n",
    "# monday_dates = generate_monday_midnights(min_date, max_date)\n",
    "\n",
    "# bcw_df = None\n",
    "# for date in monday_dates:\n",
    "#     print(date)  # Outputs Monday dates as strings\n",
    "#     broadcast_cal_df['bcw_start_date'] = pd.to_datetime(broadcast_cal_df['bcw_start_date'])\n",
    "    \n",
    "#     # Filter DataFrame for the current Monday date\n",
    "#     bcw_set = broadcast_cal_df.loc[broadcast_cal_df['bcw_start_date'] == date]\n",
    "#     print(bcw_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_cal_df.loc[broadcast_cal_df['bcw_start_date'] == '2024-10-28']\n",
    "monday_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27331840\n",
    "# 27331844\n",
    "# 27331840\n",
    "activity_sessions_df.sort_values(by=['id', 'created_time'], ascending=[True, False] ,inplace=True)\n",
    "activity_sessions_df = activity_sessions_df.drop_duplicates(subset=['id'], keep='first')\n",
    "activity_sessions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_df['_YEAR'] = activity_sessions_df['created_time'].dt.year.astype('Int64')\n",
    "activity_sessions_df['_MONTH'] = activity_sessions_df['created_time'].dt.month.astype('Int64')\n",
    "activity_sessions_df['_DAY'] = activity_sessions_df['created_time'].dt.day.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_with_geos_df = activity_sessions_df.merge(geo_zip_df, how='left', left_on='zip_code', right_on='geo_location')\n",
    "activity_sessions_with_geos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()]\n",
    "activity_sessions_with_geos_df['neustar_dma_id'] = activity_sessions_with_geos_df['neustar_dma_id'].astype('string')\n",
    "activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()] = activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()].merge(geo_dma_df, how='left', left_on='neustar_dma_id', right_on='geo_location')\n",
    "activity_sessions_with_geos_df['neustar_dma_id'] = activity_sessions_with_geos_df['neustar_dma_id'].astype('Int64')\n",
    "activity_sessions_with_geos_df.loc[activity_sessions_with_geos_df['neustar_country'].isin(['us', 'ca']) & activity_sessions_with_geos_df['geo_country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_with_geos_df['neustar_dma_id'] = activity_sessions_with_geos_df['neustar_dma_id'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_df['created_time_2'] = pd.to_datetime(activity_sessions_df['created_time'])\n",
    "broadcast_cal_df['bcw_start_date_2'] = pd.to_datetime(broadcast_cal_df['bcw_start_date'])\n",
    "broadcast_cal_df['bcw_end_date_2'] = pd.to_datetime(broadcast_cal_df['bcw_end_date'])\n",
    "\n",
    "\n",
    "activity_sessions_df['created_time'].max()\n",
    "activity_sessions_df['_YEAR'] = activity_sessions_df['created_time'].dt.year.astype('string')\n",
    "activity_sessions_df['_MONTH'] = activity_sessions_df['created_time'].dt.month.astype('string')\n",
    "\n",
    "ref_1_ids_df = broadcast_cal_df[broadcast_cal_df['bcw_start_date_2'] <= activity_sessions_df['created_time_2'].min()]\n",
    "ref_2_ids_df = broadcast_cal_df[broadcast_cal_df['bcw_end_date_2'] > activity_sessions_df['created_time_2'].max()]\n",
    "ref_1_ids_df\n",
    "# common_ids = []\n",
    "# for id in ref_1_ids:\n",
    "#     if id in ref_2_ids:\n",
    "#         common_ids.append(id)\n",
    "# common_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_df['created_time'] = activity_sessions_df['created_time'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_sessions_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_cal_df['bcw_start_date'] = pd.to_datetime(broadcast_cal_df['bcw_start_date'])\n",
    "broadcast_cal_df['bcw_end_date'] = pd.to_datetime(broadcast_cal_df['bcw_end_date'])\n",
    "activity_sessions_df['created_time'] = pd.to_datetime(activity_sessions_df['created_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_min_date = pd.to_datetime(activity_sessions_df['created_time']).min()\n",
    "ref_max_date = pd.to_datetime(activity_sessions_df['created_time']).max()\n",
    "ref_max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_1 = broadcast_cal_df[(broadcast_cal_df['bcw_start_date'] <= ref_min_date)]\n",
    "# ref_2 = ref_1.loc[(ref_1['bcw_end_date'] < ref_max_date)].copy()\n",
    "ref_1\n",
    "# broadcast_cal_df.loc[(broadcast_cal_df['bcw_start_date'] >= activity_sessions_df['created_time'].min()) & (broadcast_cal_df['bcw_end_date'] <= activity_sessions_df['created_time'].max())]\n",
    "# # Get scalar values for the date range\n",
    "# min_created_time = activity_sessions_df['created_time'].min()\n",
    "# max_created_time = activity_sessions_df['created_time'].max()\n",
    "\n",
    "# # Filter the DataFrame\n",
    "# filtered_broadcast_cal_df = broadcast_cal_df.loc[\n",
    "#     (broadcast_cal_df['bcw_start_date'] >= min_created_time) &\n",
    "#     (broadcast_cal_df['bcw_end_date'] <= max_created_time)\n",
    "# ]\n",
    "# filtered_broadcast_cal_df\n",
    "# # reff_df\n",
    "# # activity_sessions_df['created_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = broadcast_cal_df\n",
    "for col in ddf.columns:\n",
    "    print(f'col: {col}, dtype: {ddf[col].dtype}, sample: {ddf[col].iloc[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
